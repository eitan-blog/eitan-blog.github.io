<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;example.com&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Muse&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;right&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:true,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:true,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script>
<meta name="description" content="本文为学习笔记，对应视频教程来自尚硅谷大数据Spark教程从入门到精通 SparkSQL 概述SparkSQL 是什么Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块。 SparkSQL 特点 无缝的整合了 SQL 查询和 Spark 编程 统一的数据访问 兼容 Hive 标准数据连接  DataFrame 是什么在 Spark 中，Da">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark（三）：SparkSQL">
<meta property="og:url" content="http://example.com/2022/05/28/Spark%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9ASparkSQL/index.html">
<meta property="og:site_name" content="Eitan&#39;s Blog">
<meta property="og:description" content="本文为学习笔记，对应视频教程来自尚硅谷大数据Spark教程从入门到精通 SparkSQL 概述SparkSQL 是什么Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块。 SparkSQL 特点 无缝的整合了 SQL 查询和 Spark 编程 统一的数据访问 兼容 Hive 标准数据连接  DataFrame 是什么在 Spark 中，Da">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220528122342.png">
<meta property="article:published_time" content="2022-05-28T04:08:39.968Z">
<meta property="article:modified_time" content="2022-05-29T13:22:56.399Z">
<meta property="article:author" content="Eitan">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220528122342.png">


<link rel="canonical" href="http://example.com/2022/05/28/Spark%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9ASparkSQL/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;http:&#x2F;&#x2F;example.com&#x2F;2022&#x2F;05&#x2F;28&#x2F;Spark%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9ASparkSQL&#x2F;&quot;,&quot;path&quot;:&quot;2022&#x2F;05&#x2F;28&#x2F;Spark（三）：SparkSQL&#x2F;&quot;,&quot;title&quot;:&quot;Spark（三）：SparkSQL&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Spark（三）：SparkSQL | Eitan's Blog</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">
    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Eitan's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkSQL-%E6%A6%82%E8%BF%B0"><span class="nav-number">1.</span> <span class="nav-text">SparkSQL 概述</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SparkSQL-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">1.1.</span> <span class="nav-text">SparkSQL 是什么</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SparkSQL-%E7%89%B9%E7%82%B9"><span class="nav-number">1.2.</span> <span class="nav-text">SparkSQL 特点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DataFrame-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">1.3.</span> <span class="nav-text">DataFrame 是什么</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DataSet-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">1.4.</span> <span class="nav-text">DataSet 是什么</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkSQL-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B"><span class="nav-number">2.</span> <span class="nav-text">SparkSQL 核心编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DataFrame"><span class="nav-number">2.1.</span> <span class="nav-text">DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-DataFrame"><span class="nav-number">2.1.1.</span> <span class="nav-text">创建 DataFrame</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SQL-%E8%AF%AD%E6%B3%95"><span class="nav-number">2.1.2.</span> <span class="nav-text">SQL 语法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DSL-%E8%AF%AD%E6%B3%95"><span class="nav-number">2.1.3.</span> <span class="nav-text">DSL 语法</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E8%BD%AC%E6%8D%A2%E4%B8%BA-DataFrame"><span class="nav-number">2.1.4.</span> <span class="nav-text">RDD 转换为 DataFrame</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DataFrame-%E8%BD%AC%E6%8D%A2%E4%B8%BA-RDD"><span class="nav-number">2.1.5.</span> <span class="nav-text">DataFrame 转换为 RDD</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DataSet"><span class="nav-number">2.2.</span> <span class="nav-text">DataSet</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA-DataSet"><span class="nav-number">2.2.1.</span> <span class="nav-text">创建 DataSet</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E8%BD%AC%E6%8D%A2%E4%B8%BA-DataSet"><span class="nav-number">2.2.2.</span> <span class="nav-text">RDD 转换为 DataSet</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DataSet-%E8%BD%AC%E6%8D%A2%E4%B8%BA-RDD"><span class="nav-number">2.2.3.</span> <span class="nav-text">DataSet 转换为 RDD</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DataFrame-%E5%92%8C-%E5%92%8C-DataSet-%E8%BD%AC%E6%8D%A2"><span class="nav-number">2.3.</span> <span class="nav-text">DataFrame 和 和 DataSet 转换</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#DataFrame-%E8%BD%AC%E6%8D%A2%E4%B8%BA-DataSet"><span class="nav-number">2.3.1.</span> <span class="nav-text">DataFrame 转换为 DataSet</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DataSet-%E8%BD%AC%E6%8D%A2%E4%B8%BA-DataFrame"><span class="nav-number">2.3.2.</span> <span class="nav-text">DataSet 转换为 DataFrame</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#IDEA-%E5%BC%80%E5%8F%91-SparkSQL"><span class="nav-number">2.4.</span> <span class="nav-text">IDEA 开发 SparkSQL</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E4%BE%9D%E8%B5%96"><span class="nav-number">2.4.1.</span> <span class="nav-text">添加依赖</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">2.4.2.</span> <span class="nav-text">代码实现</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%A8%E6%88%B7%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0"><span class="nav-number">2.5.</span> <span class="nav-text">用户自定义函数</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#UDF"><span class="nav-number">2.5.1.</span> <span class="nav-text">UDF</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#UDAF"><span class="nav-number">2.5.2.</span> <span class="nav-text">UDAF</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98"><span class="nav-number">2.6.</span> <span class="nav-text">数据的加载和保存</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E7%9A%84%E5%8A%A0%E8%BD%BD%E5%92%8C%E4%BF%9D%E5%AD%98%E6%96%B9%E5%BC%8F"><span class="nav-number">2.6.1.</span> <span class="nav-text">通用的加载和保存方式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Parquet"><span class="nav-number">2.6.2.</span> <span class="nav-text">Parquet</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#JSON"><span class="nav-number">2.6.3.</span> <span class="nav-text">JSON</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CSV"><span class="nav-number">2.6.4.</span> <span class="nav-text">CSV</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#MySQL"><span class="nav-number">2.6.5.</span> <span class="nav-text">MySQL</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E4%BE%9D%E8%B5%96"><span class="nav-number">2.6.5.1.</span> <span class="nav-text">导入依赖</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%AF%BB%E5%86%99%E6%95%B0%E6%8D%AE"><span class="nav-number">2.6.5.2.</span> <span class="nav-text">读写数据</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Hive"><span class="nav-number">2.6.6.</span> <span class="nav-text">Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%86%85%E5%B5%8C-Hive"><span class="nav-number">2.6.6.1.</span> <span class="nav-text">内嵌 Hive</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%A4%96%E9%83%A8%E7%9A%84-HIVE"><span class="nav-number">2.6.6.2.</span> <span class="nav-text">外部的 HIVE</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C-Spark-SQL-CLI"><span class="nav-number">2.6.6.3.</span> <span class="nav-text">运行 Spark SQL CLI</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C-Spark-beeline"><span class="nav-number">2.6.6.4.</span> <span class="nav-text">运行 Spark beeline</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E6%93%8D%E4%BD%9C-Hive"><span class="nav-number">2.6.7.</span> <span class="nav-text">代码操作 Hive</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkSQL-%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98"><span class="nav-number">3.</span> <span class="nav-text">SparkSQL 项目实战</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-number">3.1.</span> <span class="nav-text">数据准备</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E8%AF%B4%E6%98%8E"><span class="nav-number">3.2.</span> <span class="nav-text">需求说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="nav-number">3.3.</span> <span class="nav-text">代码实现</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Eitan"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Eitan</p>
  <div class="site-description" itemprop="description">blog用于记忆，大脑擅长思考</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/eitan-blog" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;eitan-blog" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:eitan_blog@163.com" title="E-Mail → mailto:eitan_blog@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>




        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/yourname" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/28/Spark%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9ASparkSQL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Eitan">
      <meta itemprop="description" content="blog用于记忆，大脑擅长思考">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Eitan's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark（三）：SparkSQL
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-28 12:08:39" itemprop="dateCreated datePublished" datetime="2022-05-28T12:08:39+08:00">2022-05-28</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-05-29 21:22:56" itemprop="dateModified" datetime="2022-05-29T21:22:56+08:00">2022-05-29</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>26k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>23 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本文为学习笔记，对应视频教程来自<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV11A411L7CK">尚硅谷大数据Spark教程从入门到精通</a></p>
<h3 id="SparkSQL-概述"><a href="#SparkSQL-概述" class="headerlink" title="SparkSQL 概述"></a>SparkSQL 概述</h3><h4 id="SparkSQL-是什么"><a href="#SparkSQL-是什么" class="headerlink" title="SparkSQL 是什么"></a>SparkSQL 是什么</h4><p>Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块。</p>
<h4 id="SparkSQL-特点"><a href="#SparkSQL-特点" class="headerlink" title="SparkSQL 特点"></a>SparkSQL 特点</h4><ol>
<li>无缝的整合了 SQL 查询和 Spark 编程</li>
<li>统一的数据访问</li>
<li>兼容 Hive</li>
<li>标准数据连接</li>
</ol>
<h4 id="DataFrame-是什么"><a href="#DataFrame-是什么" class="headerlink" title="DataFrame 是什么"></a>DataFrame 是什么</h4><p>在 Spark 中，DataFrame 是一种以 RDD 为基础的分布式数据集，类似于传统数据库中的二维表格。DataFrame 与 RDD 的主要区别在于，前者带有 schema 元信息，即 DataFrame 所表示的二维表数据集的每一列都带有名称和类型。这使得 Spark SQL 得以洞察更多的结构信息，从而对藏于 DataFrame 背后的数据源以及作用于 DataFrame 之上的变换进行了针对性的优化，最终达到大幅提升运行时效率的目标。反观 RDD，由于无从得知所存数据元素的具体内部结构，Spark Core 只能在 stage 层面进行简单、通用的流水线优化。</p>
<p>DataFrame 是为数据提供了 Schema 的视图。可以把它当做数据库中的一张表来对待DataFrame 也是懒执行的，但性能上比 RDD 要高，主要原因：优化的执行计划，即查询计划通过 Spark catalyst optimiser 进行优化。</p>
<p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220528122342.png" alt="image-20220528122337869"></p>
<span id="more"></span>

<h4 id="DataSet-是什么"><a href="#DataSet-是什么" class="headerlink" title="DataSet 是什么"></a>DataSet 是什么</h4><p>DataSet 是分布式数据集合。DataSet 是 Spark 1.6 中添加的一个新抽象，是 DataFrame 的一个扩展。它提供了 RDD 的优势（强类型，使用强大的 lambda 函数的能力）以及 Spark SQL 优化执行引擎的优点。DataSet 也可以使用功能性的转换（操作 map，flatMap，filter 等等）。</p>
<ul>
<li>DataSet 是 DataFrame API 的一个扩展，是 SparkSQL 最新的数据抽象；</li>
<li>用户友好的 API 风格，既具有类型安全检查也具有 DataFrame 的查询优化特性；</li>
<li>用样例类来对 DataSet 中定义数据的结构信息，样例类中每个属性的名称直接映射到 DataSet 中的字段名称；</li>
<li>DataSet 是强类型的。比如可以有 DataSet[Car]，DataSet[Person]；</li>
<li>DataFrame 是 DataSet 的特列，DataFrame=DataSet[Row] ，所以可以通过 as 方法将 DataFrame 转换为 DataSet。Row 是一个类型，跟 Car、Person 这些的类型一样，所有的表结构信息都用 Row 来表示。获取数据时需要指定顺序</li>
</ul>
<h3 id="SparkSQL-核心编程"><a href="#SparkSQL-核心编程" class="headerlink" title="SparkSQL 核心编程"></a>SparkSQL 核心编程</h3><h4 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h4><h5 id="创建-DataFrame"><a href="#创建-DataFrame" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在 spark 的 bin/input 目录中创建 user.json 文件</span></span><br><span class="line">[eitan@SparkOrigin ~]$ vim /opt/module/spark-local-3.2.1/bin/input/user.json</span><br><span class="line">&#123;&quot;username&quot;:&quot;zhangsan&quot;, &quot;age&quot;:30&#125;</span><br><span class="line">&#123;&quot;username&quot;:&quot;lisi&quot;, &quot;age&quot;:20&#125;</span><br><span class="line">&#123;&quot;username&quot;:&quot;wangwu&quot;, &quot;age&quot;:40&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看 Spark 支持创建文件的数据源格式</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.read.</span></span><br><span class="line">csv   format   jdbc   json   load   option   options   orc   parquet   schema   table   text   textFile</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 读取 json 文件创建 DataFrame</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val df = spark.read.json(<span class="string">&quot;/opt/module/spark-local-3.2.1/bin/input/user.json&quot;</span>)</span></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, username: string]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看数据</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.show</span></span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| 30|zhangsan|</span><br><span class="line">| 20|    lisi|</span><br><span class="line">| 40|  wangwu|</span><br><span class="line">+---+--------+</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 对于 DataFrame 创建一个全局表</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.createOrReplaceGlobalTempView(<span class="string">&quot;people&quot;</span>)</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过 SQL 语句实现查询全表</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.newSession.sql(<span class="string">&quot;SELECT * FROM global_temp.people&quot;</span>).show</span></span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| 30|zhangsan|</span><br><span class="line">| 20|    lisi|</span><br><span class="line">| 40|  wangwu|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>

<h5 id="SQL-语法"><a href="#SQL-语法" class="headerlink" title="SQL 语法"></a>SQL 语法</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 读取 JSON 文件创建 DataFrame</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> var df = spark.read.json(<span class="string">&quot;bin/input/user.json&quot;</span>)</span></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, username: string] </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 对 DataFrame 创建一个临时表</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.createOrReplaceTempView(<span class="string">&quot;people&quot;</span>)</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过 SQL 语句实现查询全表</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> var sqlDF = spark.sql(<span class="string">&quot;SELECT * FROM people&quot;</span>)</span></span><br><span class="line">sqlDF: org.apache.spark.sql.DataFrame = [age: bigint, username: string]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 结果展示</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> sqlDF.show</span></span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| 30|zhangsan|</span><br><span class="line">| 20|    lisi|</span><br><span class="line">| 40|  wangwu|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>

<h5 id="DSL-语法"><a href="#DSL-语法" class="headerlink" title="DSL 语法"></a>DSL 语法</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 查看 DataFrame 的 Schema 信息</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.printSchema</span></span><br><span class="line">root</span><br><span class="line"> |-- age: long (nullable = true)</span><br><span class="line"> |-- username: string (nullable = true)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 只查看<span class="string">&quot;username&quot;</span>列数据</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.select(<span class="string">&quot;username&quot;</span>).show</span></span><br><span class="line">+--------+</span><br><span class="line">|username|</span><br><span class="line">+--------+</span><br><span class="line">|zhangsan|</span><br><span class="line">|    lisi|</span><br><span class="line">|  wangwu|</span><br><span class="line">+--------+</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看<span class="string">&quot;username&quot;</span>列数据以及<span class="string">&quot;age+1&quot;</span>数据</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 涉及到运算的时候, 每列都必须使用$, 或者采用引号表达式：单引号+字段名</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.select($<span class="string">&quot;username&quot;</span>, $<span class="string">&quot;age&quot;</span>+1).show</span></span><br><span class="line">+--------+---------+</span><br><span class="line">|username|(age + 1)|</span><br><span class="line">+--------+---------+</span><br><span class="line">|zhangsan|       31|</span><br><span class="line">|    lisi|       21|</span><br><span class="line">|  wangwu|       41|</span><br><span class="line">+--------+---------+</span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.select(<span class="string">&#x27;username, &#x27;</span>age+1).show</span></span><br><span class="line">+--------+---------+</span><br><span class="line">|username|(age + 1)|</span><br><span class="line">+--------+---------+</span><br><span class="line">|zhangsan|       31|</span><br><span class="line">|    lisi|       21|</span><br><span class="line">|  wangwu|       41|</span><br><span class="line">+--------+---------+</span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.select(<span class="string">&#x27;username, &#x27;</span>age+1 as <span class="string">&quot;newage&quot;</span>).show</span></span><br><span class="line">+--------+------+</span><br><span class="line">|username|newage|</span><br><span class="line">+--------+------+</span><br><span class="line">|zhangsan|    31|</span><br><span class="line">|    lisi|    21|</span><br><span class="line">|  wangwu|    41|</span><br><span class="line">+--------+------+</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看<span class="string">&quot;age&quot;</span>大于<span class="string">&quot;30&quot;</span>的数据</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.filter(<span class="string">&#x27;age &gt; 30).show</span></span></span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| 40|  wangwu|</span><br><span class="line">+---+--------+</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="string"> 按照&quot;age&quot;分组，查看数据条数</span></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"><span class="string"> df.groupBy(&quot;age&quot;).count.show</span></span></span><br><span class="line">+---+-----+</span><br><span class="line">|age|count|</span><br><span class="line">+---+-----+</span><br><span class="line">| 30|    1|</span><br><span class="line">| 20|    1|</span><br><span class="line">| 40|    1|</span><br><span class="line">+---+-----+</span><br></pre></td></tr></table></figure>

<h5 id="RDD-转换为-DataFrame"><a href="#RDD-转换为-DataFrame" class="headerlink" title="RDD 转换为 DataFrame"></a>RDD 转换为 DataFrame</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 实际开发中，一般通过样例类将 RDD 转换为 DataFrame</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> <span class="keyword">case</span> class User(name:String, age:Int)</span></span><br><span class="line">defined class User</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> RDD 转换为 DataFrame</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val df = sc.makeRDD(List((<span class="string">&quot;zhangsan&quot;</span>,30), (<span class="string">&quot;lisi&quot;</span>,40))).map(t =&gt; User(t._1, t._2)).toDF</span></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 展示数据</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>).show</span></span><br><span class="line">+--------+---+</span><br><span class="line">|    name|age|</span><br><span class="line">+--------+---+</span><br><span class="line">|zhangsan| 30|</span><br><span class="line">|    lisi| 40|</span><br><span class="line">+--------+---+</span><br></pre></td></tr></table></figure>

<h5 id="DataFrame-转换为-RDD"><a href="#DataFrame-转换为-RDD" class="headerlink" title="DataFrame 转换为 RDD"></a>DataFrame 转换为 RDD</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> DataFrame 转换为 RDD</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 此时得到的 RDD 存储类型为 Row</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val rdd = df.rdd</span></span><br><span class="line">rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[60] at rdd at &lt;console&gt;:24</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val array = rdd.collect</span></span><br><span class="line">array: Array[org.apache.spark.sql.Row] = Array([zhangsan,30], [lisi,40])</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> array(0)</span></span><br><span class="line">res24: org.apache.spark.sql.Row = [zhangsan,30]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> array(0)(0)</span></span><br><span class="line">res25: Any = zhangsan</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> array(0).getAs[String](<span class="string">&quot;name&quot;</span>)</span></span><br><span class="line">res27: String = zhangsan</span><br></pre></td></tr></table></figure>

<h4 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h4><h5 id="创建-DataSet"><a href="#创建-DataSet" class="headerlink" title="创建 DataSet"></a>创建 DataSet</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使用样例类序列创建 DataSet</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> <span class="keyword">case</span> class Person(name: String, age: Long)</span></span><br><span class="line">defined class Person</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val caseClassDS = Seq(Person(<span class="string">&quot;zhangsan&quot;</span>,2)).toDS()</span></span><br><span class="line">caseClassDS: org.apache.spark.sql.Dataset[Person] = [name: string, age: bigint]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> caseClassDS.show</span></span><br><span class="line">+--------+---+</span><br><span class="line">|    name|age|</span><br><span class="line">+--------+---+</span><br><span class="line">|zhangsan|  2|</span><br><span class="line">+--------+---+</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用基本类型的序列创建 DataSet</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val ds = Seq(1,2,3,4,5).toDS</span></span><br><span class="line">ds: org.apache.spark.sql.Dataset[Int] = [value: int]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> ds.show</span></span><br><span class="line">+-----+</span><br><span class="line">|value|</span><br><span class="line">+-----+</span><br><span class="line">|    1|</span><br><span class="line">|    2|</span><br><span class="line">|    3|</span><br><span class="line">|    4|</span><br><span class="line">|    5|</span><br><span class="line">+-----+</span><br></pre></td></tr></table></figure>

<h5 id="RDD-转换为-DataSet"><a href="#RDD-转换为-DataSet" class="headerlink" title="RDD 转换为 DataSet"></a>RDD 转换为 DataSet</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> <span class="keyword">case</span> class User(name:String, age:Int)</span></span><br><span class="line">defined class User</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val ds = sc.makeRDD(List((<span class="string">&quot;zhangsan&quot;</span>,30), (<span class="string">&quot;lisi&quot;</span>,49))).map(t=&gt;User(t._1, t._2)).toDS</span></span><br><span class="line">ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> ds.show</span></span><br><span class="line">+--------+---+</span><br><span class="line">|    name|age|</span><br><span class="line">+--------+---+</span><br><span class="line">|zhangsan| 30|</span><br><span class="line">|    lisi| 49|</span><br><span class="line">+--------+---+</span><br></pre></td></tr></table></figure>

<h5 id="DataSet-转换为-RDD"><a href="#DataSet-转换为-RDD" class="headerlink" title="DataSet 转换为 RDD"></a>DataSet 转换为 RDD</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> <span class="keyword">case</span> class User(name:String, age:Int)</span></span><br><span class="line">defined class User</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val ds = sc.makeRDD(List((<span class="string">&quot;zhangsan&quot;</span>,30), (<span class="string">&quot;lisi&quot;</span>,49))).map(t=&gt;User(t._1, t._2)).toDS</span></span><br><span class="line">ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val rdd = ds.rdd</span></span><br><span class="line">rdd: org.apache.spark.rdd.RDD[User] = MapPartitionsRDD[70] at rdd at &lt;console&gt;:23</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> rdd.collect</span></span><br><span class="line">res32: Array[User] = Array(User(zhangsan,30), User(lisi,49))</span><br></pre></td></tr></table></figure>

<h4 id="DataFrame-和-和-DataSet-转换"><a href="#DataFrame-和-和-DataSet-转换" class="headerlink" title="DataFrame 和 和 DataSet 转换"></a>DataFrame 和 和 DataSet 转换</h4><h5 id="DataFrame-转换为-DataSet"><a href="#DataFrame-转换为-DataSet" class="headerlink" title="DataFrame 转换为 DataSet"></a>DataFrame 转换为 DataSet</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> <span class="keyword">case</span> class User(name:String, age:Int)</span></span><br><span class="line">defined class User</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val df = sc.makeRDD(List((<span class="string">&quot;zhangsan&quot;</span>,30), (<span class="string">&quot;lisi&quot;</span>,49))).toDF(<span class="string">&quot;name&quot;</span>,<span class="string">&quot;age&quot;</span>)</span></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val ds = df.as[User]</span></span><br><span class="line">ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]</span><br></pre></td></tr></table></figure>

<h5 id="DataSet-转换为-DataFrame"><a href="#DataSet-转换为-DataFrame" class="headerlink" title="DataSet 转换为 DataFrame"></a>DataSet 转换为 DataFrame</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val ds = df.as[User]</span></span><br><span class="line">ds: org.apache.spark.sql.Dataset[User] = [name: string, age: int]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val df = ds.toDF</span></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, age: int]</span><br></pre></td></tr></table></figure>

<h4 id="IDEA-开发-SparkSQL"><a href="#IDEA-开发-SparkSQL" class="headerlink" title="IDEA 开发 SparkSQL"></a>IDEA 开发 SparkSQL</h4><h5 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_SparkSQL_Basic</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建 SparkSQL 运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// TODO 执行逻辑操作</span></span><br><span class="line">    <span class="comment">// DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DataFrame =&gt; SQL</span></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;SELECT * FROM user&quot;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&quot;SELECT username, age FROM user&quot;</span>).show()</span><br><span class="line">    spark.sql(<span class="string">&quot;SELECT avg(age) FROM user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DataFrame =&gt; DSL</span></span><br><span class="line">    df.select(<span class="string">&quot;username&quot;</span>, <span class="string">&quot;age&quot;</span>).show()</span><br><span class="line">    <span class="comment">// 在使用 DataFrame 时，如果涉及到转换操作，需要引入转换规则</span></span><br><span class="line">    df.select($<span class="string">&quot;age&quot;</span> + <span class="number">1</span>).show</span><br><span class="line">    df.select(<span class="symbol">&#x27;age</span> + <span class="number">1</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DataSet</span></span><br><span class="line">    <span class="comment">// DataFrame 是特定类型的 DataSet</span></span><br><span class="line">    <span class="keyword">val</span> seq = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Int</span>] = seq.toDS()</span><br><span class="line">    ds.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// RDD &lt;=&gt; DataFrame</span></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)] = spark.sparkContext.makeRDD(<span class="type">List</span>((<span class="number">1</span>, <span class="string">&quot;zhangsan&quot;</span>, <span class="number">30</span>), (<span class="number">2</span>, <span class="string">&quot;lisi&quot;</span>, <span class="number">40</span>)))</span><br><span class="line">    <span class="keyword">val</span> dataFrame: <span class="type">DataFrame</span> = rdd.toDF(<span class="string">&quot;id&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> rowRdd: <span class="type">RDD</span>[<span class="type">Row</span>] = dataFrame.rdd</span><br><span class="line"></span><br><span class="line">    <span class="comment">// DataFrame &lt;=&gt; DataSet</span></span><br><span class="line">    <span class="keyword">val</span> dataSet: <span class="type">Dataset</span>[<span class="type">User</span>] = dataFrame.as[<span class="type">User</span>]</span><br><span class="line">    <span class="keyword">val</span> frame: <span class="type">DataFrame</span> = dataSet.toDF()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// RDD &lt;=&gt; DataSet</span></span><br><span class="line">    <span class="keyword">val</span> set: <span class="type">Dataset</span>[<span class="type">User</span>] = rdd.map &#123;</span><br><span class="line">      <span class="keyword">case</span> (id, name, age) =&gt; &#123;</span><br><span class="line">        <span class="type">User</span>(id, name, age)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.toDS()</span><br><span class="line">    <span class="keyword">val</span> userRdd: <span class="type">RDD</span>[<span class="type">User</span>] = set.rdd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">id: <span class="type">Int</span>, name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="用户自定义函数"><a href="#用户自定义函数" class="headerlink" title="用户自定义函数"></a>用户自定义函数</h4><h5 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02_SparkSQL_UDF</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建 SparkSQL 运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.udf.register(<span class="string">&quot;prefixName&quot;</span>, (name: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">      <span class="string">&quot;Name: &quot;</span> + name</span><br><span class="line">    &#125;)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;SELECT age, prefixName(username) FROM user&quot;</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h5><p>弱类型，不推荐使用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_SparkSQL_UDAF_New</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建 SparkSQL 运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line">    df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.udf.register(<span class="string">&quot;ageAvg&quot;</span>, functions.udaf(<span class="keyword">new</span> <span class="type">MyAvgUDAF</span>))</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;SELECT ageAvg(age) FROM user&quot;</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 自定义聚合函数类：计算年龄的平均值</span></span><br><span class="line"><span class="comment">   * 1.继承 org.apache.spark.sql.expressions.Aggregator，定义泛型</span></span><br><span class="line"><span class="comment">   * IN：输入的数据类型   Long</span></span><br><span class="line"><span class="comment">   * BUF：缓冲区数据类型 Buff</span></span><br><span class="line"><span class="comment">   * OUT：输出的数据类型  Long</span></span><br><span class="line"><span class="comment">   * 2.重写方法</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params">var total: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyAvgUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Long</span>, <span class="type">Buff</span>, <span class="type">Long</span>] </span>&#123;</span><br><span class="line">    <span class="comment">// z &amp; zero：初始值或零值</span></span><br><span class="line">    <span class="comment">// 缓冲区初始化</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = &#123;</span><br><span class="line">      <span class="type">Buff</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据我们输入的数据跟新缓冲区数据</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">Buff</span>, in: <span class="type">Long</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">      buff.total += in</span><br><span class="line">      buff.count += <span class="number">1</span></span><br><span class="line">      buff</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 合并缓冲区</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buff1: <span class="type">Buff</span>, buff2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">      buff1.total += buff2.total</span><br><span class="line">      buff1.count += buff2.count</span><br><span class="line">      buff1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算结果</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">Buff</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">      buff.total / buff.count</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 缓冲区的编码操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出的编码操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Long</span>] = <span class="type">Encoders</span>.scalaLong</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>强类型，推荐使用</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark05_SparkSQL_UDAF_Old</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建 SparkSQL 运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 早期版本中，spark 不能在 sql 中使用强类型 UDAF 操作</span></span><br><span class="line">    <span class="comment">// SQL &amp; DSL</span></span><br><span class="line">    <span class="comment">// 早期的 UDAF 强类型聚合函数使用 DSL 语法操作</span></span><br><span class="line">    <span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User</span>] = df.as[<span class="type">User</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将 UDAF 函数转换为查询的列对象</span></span><br><span class="line">    <span class="keyword">val</span> udafCol: <span class="type">TypedColumn</span>[<span class="type">User</span>, <span class="type">Long</span>] = <span class="keyword">new</span> <span class="type">MyAvgUDAF</span>().toColumn</span><br><span class="line"></span><br><span class="line">    ds.select(udafCol).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">username: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 自定义聚合函数类：计算年龄的平均值</span></span><br><span class="line"><span class="comment">   * 1.继承 org.apache.spark.sql.expressions.Aggregator，定义泛型</span></span><br><span class="line"><span class="comment">   * IN：输入的数据类型   User</span></span><br><span class="line"><span class="comment">   * BUF：缓冲区数据类型 Buff</span></span><br><span class="line"><span class="comment">   * OUT：输出的数据类型  Long</span></span><br><span class="line"><span class="comment">   * 2.重写方法</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params">var total: <span class="type">Long</span>, var count: <span class="type">Long</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyAvgUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">User</span>, <span class="type">Buff</span>, <span class="type">Long</span>] </span>&#123;</span><br><span class="line">    <span class="comment">// z &amp; zero：初始值或零值</span></span><br><span class="line">    <span class="comment">// 缓冲区初始化</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = &#123;</span><br><span class="line">      <span class="type">Buff</span>(<span class="number">0</span>L, <span class="number">0</span>L)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据我们输入的数据跟新缓冲区数据</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">Buff</span>, in: <span class="type">User</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">      buff.total += in.age</span><br><span class="line">      buff.count += <span class="number">1</span></span><br><span class="line">      buff</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 合并缓冲区</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buff1: <span class="type">Buff</span>, buff2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">      buff1.total += buff2.total</span><br><span class="line">      buff1.count += buff2.count</span><br><span class="line">      buff1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算结果</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">Buff</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">      buff.total / buff.count</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 缓冲区的编码操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 输出的编码操作</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Long</span>] = <span class="type">Encoders</span>.scalaLong</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="数据的加载和保存"><a href="#数据的加载和保存" class="headerlink" title="数据的加载和保存"></a>数据的加载和保存</h4><h5 id="通用的加载和保存方式"><a href="#通用的加载和保存方式" class="headerlink" title="通用的加载和保存方式"></a>通用的加载和保存方式</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> spark.read.load 是加载数据的通用方法</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.read.format(<span class="string">&quot;…&quot;</span>)[.option(<span class="string">&quot;…&quot;</span>)].load(<span class="string">&quot;…&quot;</span>)</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 案例</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val df = spark.read.format(<span class="string">&quot;json&quot;</span>).load(<span class="string">&quot;bin/input/user.json&quot;</span>)</span></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [age: bigint, username: string]            </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.show</span></span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| 30|zhangsan|</span><br><span class="line">| 20|    lisi|</span><br><span class="line">| 40|  wangwu|</span><br><span class="line">+---+--------+</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">&quot;SELECT * FROM json.`bin/input/user.json`&quot;</span>).show</span></span><br><span class="line">22/05/29 10:06:43 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException</span><br><span class="line">22/05/29 10:06:43 WARN ObjectStore: Failed to get database json, returning NoSuchObjectException</span><br><span class="line">+---+--------+</span><br><span class="line">|age|username|</span><br><span class="line">+---+--------+</span><br><span class="line">| 30|zhangsan|</span><br><span class="line">| 20|    lisi|</span><br><span class="line">| 40|  wangwu|</span><br><span class="line">+---+--------+</span><br></pre></td></tr></table></figure>

<blockquote>
<ol>
<li>format(“…”)：指定加载的数据类型，包括”csv”、”jdbc”、”json”、”orc”、”parquet” 和 “textFile”;</li>
<li>load(“…”)：在”csv”、”jdbc”、”json”、”orc”、”parquet”和”textFile”格式下需要传入加载数据的路径;</li>
<li>option(“…”)：在”jdbc”格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable</li>
</ol>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> df.write.save 是保存数据的通用方法</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash">df.write.format(<span class="string">&quot;…&quot;</span>)[.option(<span class="string">&quot;…&quot;</span>)].save(<span class="string">&quot;…&quot;</span>)</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 案例</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.write.format(<span class="string">&quot;json&quot;</span>).save(<span class="string">&quot;bin/output&quot;</span>)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<ol>
<li>format(“…”)：指定保存的数据类型，包括”csv”、”jdbc”、”json”、”orc”、”parquet”和 “textFile”;</li>
<li>save (“…”)：在”csv”、”orc”、”parquet”和”textFile”格式下需要传入保存数据的路径;</li>
<li>option(“…”)：在”jdbc”格式下需要传入 JDBC 相应参数，url、user、password 和 dbtable。保存操作可以使用 SaveMode, 用来指明如何处理数据，使用 mode()方法来设置。有一点很重要: 这些 SaveMode 都是没有加锁的, 也不是原子操作。</li>
</ol>
</blockquote>
<table>
<thead>
<tr>
<th>Scala/Java</th>
<th>Any Language</th>
<th>Meaning</th>
</tr>
</thead>
<tbody><tr>
<td>SaveMode.ErrorIfExists(default)</td>
<td>“error”(default)</td>
<td>如果文件已经存在则抛出异常</td>
</tr>
<tr>
<td>SaveMode.Append</td>
<td>“append”</td>
<td>如果文件已经存在则追加</td>
</tr>
<tr>
<td>SaveMode.Overwrite</td>
<td>“overwrite”</td>
<td>如果文件已经存在则覆盖</td>
</tr>
<tr>
<td>SaveMode.Ignore</td>
<td>“ignore”</td>
<td>如果文件已经存在则忽略</td>
</tr>
</tbody></table>
<h5 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h5><p>Spark SQL 的默认数据源为 Parquet 格式。Parquet 是一种能够有效存储嵌套数据的列式存储格式。<br>数据源为 Parquet 文件时，Spark SQL 可以方便的执行所有的操作，不需要使用 format。修改配置项 spark.sql.sources.default，可修改默认数据源格式。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 加载数据</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val df = spark.read.load(<span class="string">&quot;examples/src/main/resources/users.parquet&quot;</span>)</span></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.show</span></span><br><span class="line">+------+--------------+----------------+                                        </span><br><span class="line">|  name|favorite_color|favorite_numbers|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line">|Alyssa|          null|  [3, 9, 15, 20]|</span><br><span class="line">|   Ben|           red|              []|</span><br><span class="line">+------+--------------+----------------+</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 保存数据</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.write.mode(<span class="string">&quot;append&quot;</span>).save(<span class="string">&quot;bin/output&quot;</span>)</span></span><br></pre></td></tr></table></figure>

<h5 id="JSON"><a href="#JSON" class="headerlink" title="JSON"></a>JSON</h5><p>Spark SQL 能够自动推测 JSON 数据集的结构，并将它加载为一个 Dataset[Row]. 可以通过 SparkSession.read.json()去加载 JSON 文件。注意：Spark 读取的 JSON 文件不是传统的 JSON 文件，每一行都应该是一个 JSON 串。格式如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Michael&quot;</span>&#125;</span><br><span class="line">&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Andy&quot;</span>， <span class="string">&quot;age&quot;</span>:<span class="number">30</span>&#125;</span><br><span class="line">[&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>， <span class="string">&quot;age&quot;</span>:<span class="number">19</span>&#125;,&#123;<span class="attr">&quot;name&quot;</span>:<span class="string">&quot;Justin&quot;</span>， <span class="string">&quot;age&quot;</span>:<span class="number">19</span>&#125;]</span><br></pre></td></tr></table></figure>

<h5 id="CSV"><a href="#CSV" class="headerlink" title="CSV"></a>CSV</h5><p>Spark SQL 可以配置 CSV 文件的列表信息，读取 CSV 文件,CSV 文件的第一行设置为数据列。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 数据源</span></span><br><span class="line">name;age;job</span><br><span class="line">Jorge;30;Developer</span><br><span class="line">Bob;32;Developer</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 读取 CSV 文件</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> val df = spark.read.format(<span class="string">&quot;csv&quot;</span>).option(<span class="string">&quot;sep&quot;</span>, <span class="string">&quot;;&quot;</span>).option(<span class="string">&quot;inferSchema&quot;</span>,<span class="string">&quot;true&quot;</span>).option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>).load(<span class="string">&quot;examples/src/main/resources/people.csv&quot;</span>)</span></span><br><span class="line">df: org.apache.spark.sql.DataFrame = [name: string, age: int ... 1 more field]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> df.show</span></span><br><span class="line">+-----+---+---------+</span><br><span class="line">| name|age|      job|</span><br><span class="line">+-----+---+---------+</span><br><span class="line">|Jorge| 30|Developer|</span><br><span class="line">|  Bob| 32|Developer|</span><br><span class="line">+-----+---+---------+</span><br></pre></td></tr></table></figure>

<h5 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h5><p>Spark SQL 可以通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame，通过对 DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。如果使用 spark-shell 操作，可在启动 shell 时指定相关的数据库驱动路径或者将相关的数据库驱动放到 spark 的类路径下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell --jars mysql-connector-java-5.1.27-bin.jar</span><br></pre></td></tr></table></figure>

<p>我们这里只演示在 Idea 中通过 JDBC 对 Mysql 进行操作：</p>
<h6 id="导入依赖"><a href="#导入依赖" class="headerlink" title="导入依赖"></a>导入依赖</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.47<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h6 id="读写数据"><a href="#读写数据" class="headerlink" title="读写数据"></a>读写数据</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark06_SparkSQL_JDBC</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建 SparkSQL 运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 读取 Mysql 数据</span></span><br><span class="line">    <span class="keyword">val</span> df: <span class="type">DataFrame</span> = spark.read.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://localhost:3306/spark-sql&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;user&quot;</span>)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 展示数据</span></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 保存数据</span></span><br><span class="line">    df.write.format(<span class="string">&quot;jdbc&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;url&quot;</span>, <span class="string">&quot;jdbc:mysql://localhost:3306/spark-sql&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;driver&quot;</span>, <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;user&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;password&quot;</span>, <span class="string">&quot;root&quot;</span>)</span><br><span class="line">      .option(<span class="string">&quot;dbtable&quot;</span>, <span class="string">&quot;user_new&quot;</span>)</span><br><span class="line">      .mode(<span class="type">SaveMode</span>.<span class="type">Append</span>)</span><br><span class="line">      .save</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h5><p>Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL 编译时可以包含 Hive 支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。</p>
<h6 id="内嵌-Hive"><a href="#内嵌-Hive" class="headerlink" title="内嵌 Hive"></a>内嵌 Hive</h6><p>如果使用 Spark 内嵌的 Hive，则什么都不用做，直接使用即可。</p>
<p>Hive 的元数据存储在 derby 中, 默认仓库地址:$SPARK_HOME/spark-warehouse</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 创建表</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span></span><br><span class="line">+---------+---------+-----------+</span><br><span class="line">|namespace|tableName|isTemporary|</span><br><span class="line">+---------+---------+-----------+</span><br><span class="line">+---------+---------+-----------+</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">&quot;CREATE TABLE user(id int, name String, age int) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27; &#x27;&quot;</span>)</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">&quot;show tables&quot;</span>).show</span></span><br><span class="line">+---------+---------+-----------+                                               </span><br><span class="line">|namespace|tableName|isTemporary|</span><br><span class="line">+---------+---------+-----------+</span><br><span class="line">|  default|     user|      false|</span><br><span class="line">+---------+---------+-----------+</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加数据</span></span><br><span class="line">[eitan@SparkOrigin ~]$ vim /opt/module/spark-local-3.2.1/data/user.txt</span><br><span class="line">1 zhangsan 20</span><br><span class="line">2 lisi 30</span><br><span class="line">3 wangwu 40</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;/opt/module/spark-local-3.2.1/data/user.txt/&#x27; INTO TABLE user&quot;</span>)</span></span><br><span class="line">res13: org.apache.spark.sql.DataFrame = []</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">&quot;SELECT * FROM user&quot;</span>).show</span></span><br><span class="line">+---+--------+---+</span><br><span class="line">| id|    name|age|</span><br><span class="line">+---+--------+---+</span><br><span class="line">|  1|zhangsan| 20|</span><br><span class="line">|  2|    lisi| 30|</span><br><span class="line">|  3|  wangwu| 40|</span><br><span class="line">+---+--------+---+</span><br></pre></td></tr></table></figure>

<h6 id="外部的-HIVE"><a href="#外部的-HIVE" class="headerlink" title="外部的 HIVE"></a>外部的 HIVE</h6><p>这里选用的 Hive 部署方式可用查看 <a target="_blank" rel="noopener" href="https://eitan-blog.github.io/2022/05/08/Hadoop%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">Hadoop（一）：集群搭建</a> 和 <a target="_blank" rel="noopener" href="https://eitan-blog.github.io/2022/05/11/Hadoop%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AHive/">Hadoop（三）：Hive</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.启动 Hive 环境</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 msql</span></span><br><span class="line">[eitan@hadoop102 ~]$ systemctl start mysqld</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 hdfs 集群</span></span><br><span class="line">[eitan@hadoop102 ~]$ /opt/module/hadoop-3.3.2/sbin/start-dfs.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 metastore 服务</span></span><br><span class="line">[eitan@hadoop102 ~]$ nohup /opt/module/apache-hive-3.1.3/bin/hive --service metastore &gt; /home/eitan/log/metastore.out 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 hiveserver2 服务</span></span><br><span class="line">[eitan@hadoop103 ~]$ nohup /opt/module/apache-hive-3.1.3/bin/hiveserver2 &gt; /home/eitan/log/hiveserver2.out 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 <span class="variable">$SPARK_HOME</span>/conf 目录下</span></span><br><span class="line">[eitan@hadoop103 ~]$ scp /opt/module/apache-hive-3.1.3/conf/hive-site.xml eitan@192.168.203.150:/opt/module/spark-local-3.2.1/conf、</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.把 Mysql 的驱动 copy 到 jars/ 目录下</span></span><br><span class="line">[eitan@hadoop103 ~]$ scp /opt/module/apache-hive-3.1.3/lib/mysql-connector-java-8.0.29.jar eitan@192.168.203.150:/opt/module/spark-local-3.2.1/jars</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下，我访问的到</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 5.重启 spark-shell</span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> spark.sql(<span class="string">&quot;show databases&quot;</span>).show</span></span><br><span class="line">+---------+</span><br><span class="line">|namespace|</span><br><span class="line">+---------+</span><br><span class="line">|  default|</span><br><span class="line">|   itcast|</span><br><span class="line">+---------+</span><br></pre></td></tr></table></figure>

<h6 id="运行-Spark-SQL-CLI"><a href="#运行-Spark-SQL-CLI" class="headerlink" title="运行 Spark SQL CLI"></a>运行 Spark SQL CLI</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[eitan@SparkOrigin ~]$ /opt/module/spark-local-3.2.1/bin/spark-sql</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">spark-sql&gt;</span><span class="bash"> show databases;</span></span><br><span class="line">default</span><br><span class="line">itcast</span><br><span class="line">Time taken: 0.046 seconds, Fetched 2 row(s)</span><br></pre></td></tr></table></figure>

<h6 id="运行-Spark-beeline"><a href="#运行-Spark-beeline" class="headerlink" title="运行 Spark beeline"></a>运行 Spark beeline</h6><p>Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容 HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。Spark Thrift Server 的目的也只是取代 HiveServer2，因此它依旧可以和 Hive Metastore 进行交互，获取到 hive 的元数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.Spark 要接管 Hive 需要把 hive-site.xml 拷贝到 <span class="variable">$SPARK_HOME</span>/conf 目录下</span></span><br><span class="line">[eitan@hadoop103 ~]$ scp /opt/module/apache-hive-3.1.3/conf/hive-site.xml eitan@192.168.203.150:/opt/module/spark-local-3.2.1/conf、</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.把 Mysql 的驱动 copy 到 jars/ 目录下</span></span><br><span class="line">[eitan@hadoop103 ~]$ scp /opt/module/apache-hive-3.1.3/lib/mysql-connector-java-8.0.29.jar eitan@192.168.203.150:/opt/module/spark-local-3.2.1/jars</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.如果访问不到 hdfs，则需要把 core-site.xml 和 hdfs-site.xml 拷贝到 conf/目录下，我访问的到</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.启动 Thrift Server</span></span><br><span class="line">[eitan@SparkOrigin ~]$ /opt/module/spark-local-3.2.1/sbin/start-thriftserver.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 5.使用 beeline 连接 Thrift Server</span></span><br><span class="line">[eitan@SparkOrigin ~]$ /opt/module/spark-local-3.2.1/bin/beeline -u jdbc:hive2://192.168.203.103:10000 -n eitan</span><br><span class="line">log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).</span><br><span class="line">log4j:WARN Please initialize the log4j system properly.</span><br><span class="line">log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</span><br><span class="line">Connecting to jdbc:hive2://192.168.203.103:10000</span><br><span class="line">Connected to: Apache Hive (version 3.1.3)</span><br><span class="line">Driver: Hive JDBC (version 2.3.9)</span><br><span class="line">Transaction isolation: TRANSACTION_REPEATABLE_READ</span><br><span class="line">Beeline version 2.3.9 by Apache Hive</span><br><span class="line">0: jdbc:hive2://192.168.203.103:10000&gt; show databases;</span><br><span class="line">+----------------+</span><br><span class="line">| database_name  |</span><br><span class="line">+----------------+</span><br><span class="line">| default        |</span><br><span class="line">| itcast         |</span><br><span class="line">+----------------+</span><br></pre></td></tr></table></figure>

<h5 id="代码操作-Hive"><a href="#代码操作-Hive" class="headerlink" title="代码操作 Hive"></a>代码操作 Hive</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 导入依赖 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark07_SparkSQL_Hive</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建 SparkSQL 运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().enableHiveSupport().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;show databases&quot;</span>).show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="SparkSQL-项目实战"><a href="#SparkSQL-项目实战" class="headerlink" title="SparkSQL 项目实战"></a>SparkSQL 项目实战</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark08_SparkSQL_PrepareData</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 解决权限问题</span></span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;eitan&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建 SparkSQL 运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().enableHiveSupport().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;USE atguigu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 准备数据</span></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |CREATE TABLE `user_visit_action`</span></span><br><span class="line"><span class="string">        |(</span></span><br><span class="line"><span class="string">        |    `date`               string,</span></span><br><span class="line"><span class="string">        |    `user_id`            bigint,</span></span><br><span class="line"><span class="string">        |    `session_id`         string,</span></span><br><span class="line"><span class="string">        |    `page_id`            bigint,</span></span><br><span class="line"><span class="string">        |    `action_time`        string,</span></span><br><span class="line"><span class="string">        |    `search_keyword`     string,</span></span><br><span class="line"><span class="string">        |    `click_category_id`  bigint,</span></span><br><span class="line"><span class="string">        |    `click_product_id`   bigint,</span></span><br><span class="line"><span class="string">        |    `order_category_ids` string,</span></span><br><span class="line"><span class="string">        |    `order_product_ids`  string,</span></span><br><span class="line"><span class="string">        |    `pay_category_ids`   string,</span></span><br><span class="line"><span class="string">        |    `pay_product_ids`    string,</span></span><br><span class="line"><span class="string">        |    ` city_id `          bigint</span></span><br><span class="line"><span class="string">        |) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">    spark.sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;data/user_visit_action.txt&#x27; INTO TABLE user_visit_action&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |CREATE TABLE `product_info`</span></span><br><span class="line"><span class="string">        |(</span></span><br><span class="line"><span class="string">        |    `product_id`   bigint,</span></span><br><span class="line"><span class="string">        |    `product_name` string,</span></span><br><span class="line"><span class="string">        |    `extend_info`  string</span></span><br><span class="line"><span class="string">        |) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">    spark.sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;data/product_info.txt&#x27; INTO TABLE product_info&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |CREATE TABLE `city_info`</span></span><br><span class="line"><span class="string">        |(</span></span><br><span class="line"><span class="string">        |    `city_id`   bigint,</span></span><br><span class="line"><span class="string">        |    `city_name` string,</span></span><br><span class="line"><span class="string">        |    `area`      string</span></span><br><span class="line"><span class="string">        |) ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27;</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin)</span><br><span class="line">    spark.sql(<span class="string">&quot;LOAD DATA LOCAL INPATH &#x27;data/city_info.txt&#x27; INTO TABLE city_info&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;SELECT * FROM city_info&quot;</span>).show</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="需求说明"><a href="#需求说明" class="headerlink" title="需求说明"></a>需求说明</h4><p>这里的热门商品是从点击量的维度来看的，计算各个区域前三大热门商品，并备注上每个商品在主要城市中的分布比例，超过两个城市用其他显示。</p>
<p>例如：</p>
<table>
<thead>
<tr>
<th>地区</th>
<th>商品名称</th>
<th>点击次数</th>
<th>城市备注</th>
</tr>
</thead>
<tbody><tr>
<td>华北</td>
<td>商品 A</td>
<td>100000</td>
<td>北京 21.2%，天津 13.2%，其他 65.6%</td>
</tr>
<tr>
<td>华北</td>
<td>商品 P</td>
<td>80200</td>
<td>北京 63.0%，太原 10%，其他 27.0%</td>
</tr>
<tr>
<td>华北</td>
<td>商品 M</td>
<td>40000</td>
<td>北京 63.0%，太原 10%，其他 27.0%</td>
</tr>
<tr>
<td>东北</td>
<td>商品 J</td>
<td>92000</td>
<td>大连 28%，辽宁 17.0%，其他 55.0%</td>
</tr>
</tbody></table>
<h4 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark09_SparkSQL_Search</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 解决权限问题</span></span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">&quot;HADOOP_USER_NAME&quot;</span>, <span class="string">&quot;eitan&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建 SparkSQL 运行环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;sparkSQL&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().enableHiveSupport().config(sparkConf).getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;USE atguigu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查询基本数据</span></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |SELECT b.product_name,</span></span><br><span class="line"><span class="string">        |       c.city_name,</span></span><br><span class="line"><span class="string">        |       c.area</span></span><br><span class="line"><span class="string">        |FROM user_visit_action a</span></span><br><span class="line"><span class="string">        |         JOIN product_info b ON a.click_product_id = b.product_id</span></span><br><span class="line"><span class="string">        |         JOIN city_info c ON a.city_id = c.city_id</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin).createOrReplaceTempView(<span class="string">&quot;t1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 根据区域，进行商品的聚合</span></span><br><span class="line"></span><br><span class="line">    spark.udf.register(<span class="string">&quot;cityRemark&quot;</span>, functions.udaf(<span class="keyword">new</span> <span class="type">CityRemarkUDAF</span>))</span><br><span class="line"></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |SELECT area,</span></span><br><span class="line"><span class="string">        |       product_name,</span></span><br><span class="line"><span class="string">        |       count(*) AS clickCnt,</span></span><br><span class="line"><span class="string">        |       cityRemark(city_name) AS cityRemark</span></span><br><span class="line"><span class="string">        |FROM t1</span></span><br><span class="line"><span class="string">        |GROUP BY area, product_name</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin).createOrReplaceTempView(<span class="string">&quot;t2&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分区内对点击数量排行</span></span><br><span class="line">    spark.sql(</span><br><span class="line">      <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        |SELECT *,</span></span><br><span class="line"><span class="string">        |       rank() OVER (PARTITION BY area ORDER BY clickCnt DESC) AS rank</span></span><br><span class="line"><span class="string">        |FROM t2</span></span><br><span class="line"><span class="string">        |&quot;&quot;&quot;</span>.stripMargin).createOrReplaceTempView(<span class="string">&quot;t3&quot;</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">&quot;SELECT * FROM t3 WHERE rank &lt;= 3&quot;</span>).show(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Buff</span>(<span class="params">var count: <span class="type">Long</span>, map: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]</span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">CityRemarkUDAF</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">String</span>, <span class="type">Buff</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Buff</span> = &#123;</span><br><span class="line">      <span class="type">Buff</span>(<span class="number">0</span>L, mutable.<span class="type">Map</span>())</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(buff: <span class="type">Buff</span>, in: <span class="type">String</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">      buff.count += <span class="number">1</span></span><br><span class="line">      <span class="keyword">val</span> newCnt: <span class="type">Long</span> = buff.map.getOrElse(in, <span class="number">0</span>L) + <span class="number">1</span></span><br><span class="line">      buff.map.update(in, newCnt)</span><br><span class="line">      buff</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buff1: <span class="type">Buff</span>, buff2: <span class="type">Buff</span>): <span class="type">Buff</span> = &#123;</span><br><span class="line">      buff2.map.foreach &#123;</span><br><span class="line">        <span class="keyword">case</span> (cityName, cnt) =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> newCnt: <span class="type">Long</span> = buff1.map.getOrElse(cityName, <span class="number">0</span>L) + cnt</span><br><span class="line">          buff1.map.update(cityName, newCnt)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      buff1.count += buff2.count</span><br><span class="line">      buff1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将统计结果生成字符串信息</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(buff: <span class="type">Buff</span>): <span class="type">String</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> remarkList = <span class="type">ListBuffer</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> cityCntList: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = buff.map.toList.sortBy(_._2)(<span class="type">Ordering</span>.<span class="type">Long</span>.reverse)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> total: <span class="type">Long</span> = buff.count</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> list: <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = cityCntList.take(<span class="number">2</span>)</span><br><span class="line">      <span class="keyword">var</span> rsum = <span class="number">0</span>L;</span><br><span class="line">      list.foreach &#123;</span><br><span class="line">        <span class="keyword">case</span> (city, cnt) =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> ratio: <span class="type">Long</span> = cnt * <span class="number">100</span> / total</span><br><span class="line">          remarkList.append(<span class="string">s&quot;<span class="subst">$&#123;city&#125;</span> <span class="subst">$&#123;ratio&#125;</span>%&quot;</span>)</span><br><span class="line">          rsum += ratio</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (cityCntList.size &gt; <span class="number">2</span>) &#123;</span><br><span class="line">        remarkList.append(<span class="string">s&quot;其他 <span class="subst">$&#123;100 - rsum&#125;</span>%&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      remarkList.mkString(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Buff</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">String</span>] = <span class="type">Encoders</span>.<span class="type">STRING</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Eitan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/2022/05/28/Spark%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9ASparkSQL/" title="Spark（三）：SparkSQL">http://example.com/2022/05/28/Spark（三）：SparkSQL/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"><i class="fa fa-tag"></i> spark</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/05/22/Spark%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9ASparkCore/" rel="prev" title="Spark（二）：SparkCore">
                  <i class="fa fa-chevron-left"></i> Spark（二）：SparkCore
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/05/29/Spark%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9ASparkStreaming/" rel="next" title="Spark（四）：SparkStreaming">
                  Spark（四）：SparkStreaming <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Eitan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  





</body>
</html>
