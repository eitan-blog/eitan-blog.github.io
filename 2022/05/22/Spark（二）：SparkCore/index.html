<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;example.com&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Muse&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;right&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:true,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:true,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script>
<meta name="description" content="本文为学习笔记，对应视频教程来自尚硅谷大数据Spark教程从入门到精通 Spark运行架构运行架构">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark（二）：SparkCore">
<meta property="og:url" content="http://example.com/2022/05/22/Spark%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9ASparkCore/index.html">
<meta property="og:site_name" content="Eitan&#39;s Blog">
<meta property="og:description" content="本文为学习笔记，对应视频教程来自尚硅谷大数据Spark教程从入门到精通 Spark运行架构运行架构">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522095955.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522105535.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522162734.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522163018.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522163530.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522164119.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220526155540.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220526165853.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220527005735.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220527103550.png">
<meta property="article:published_time" content="2022-05-22T03:00:33.803Z">
<meta property="article:modified_time" content="2022-05-27T03:32:54.498Z">
<meta property="article:author" content="Eitan">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522095955.png">


<link rel="canonical" href="http://example.com/2022/05/22/Spark%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9ASparkCore/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;http:&#x2F;&#x2F;example.com&#x2F;2022&#x2F;05&#x2F;22&#x2F;Spark%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9ASparkCore&#x2F;&quot;,&quot;path&quot;:&quot;2022&#x2F;05&#x2F;22&#x2F;Spark（二）：SparkCore&#x2F;&quot;,&quot;title&quot;:&quot;Spark（二）：SparkCore&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Spark（二）：SparkCore | Eitan's Blog</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">
    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Eitan's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84"><span class="nav-number">1.</span> <span class="nav-text">Spark运行架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84"><span class="nav-number">1.1.</span> <span class="nav-text">运行架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-number">1.2.</span> <span class="nav-text">核心组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Driver"><span class="nav-number">1.2.1.</span> <span class="nav-text">Driver</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Executor"><span class="nav-number">1.2.2.</span> <span class="nav-text">Executor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ApplicationMaster"><span class="nav-number">1.2.3.</span> <span class="nav-text">ApplicationMaster</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="nav-number">1.3.</span> <span class="nav-text">核心概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Executor-%E4%B8%8E-%E4%B8%8E-Core"><span class="nav-number">1.3.1.</span> <span class="nav-text">Executor 与 与 Core</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B9%B6%E8%A1%8C%E5%BA%A6%EF%BC%88Parallelism-%EF%BC%89"><span class="nav-number">1.3.2.</span> <span class="nav-text">并行度（Parallelism ）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B"><span class="nav-number">1.4.</span> <span class="nav-text">提交流程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Yarn-Client-%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.1.</span> <span class="nav-text">Yarn Client 模式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Yarn-Cluster-%E6%A8%A1%E5%BC%8F"><span class="nav-number">1.4.2.</span> <span class="nav-text">Yarn Cluster 模式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B"><span class="nav-number">2.</span> <span class="nav-text">Spark 核心编程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD"><span class="nav-number">3.</span> <span class="nav-text">RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-%E6%A6%82%E8%BF%B0"><span class="nav-number">3.1.</span> <span class="nav-text">RDD 概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B8%E5%BF%83%E5%B1%9E%E6%80%A7"><span class="nav-number">3.2.</span> <span class="nav-text">核心属性</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E5%88%97%E8%A1%A8"><span class="nav-number">3.2.1.</span> <span class="nav-text">分区列表</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E8%AE%A1%E7%AE%97%E5%87%BD%E6%95%B0"><span class="nav-number">3.2.2.</span> <span class="nav-text">分区计算函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="nav-number">3.2.3.</span> <span class="nav-text">RDD 之间的依赖关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E5%99%A8%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="nav-number">3.2.4.</span> <span class="nav-text">分区器（可选）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%A6%96%E9%80%89%E4%BD%8D%E7%BD%AE%EF%BC%88%E5%8F%AF%E9%80%89%EF%BC%89"><span class="nav-number">3.2.5.</span> <span class="nav-text">首选位置（可选）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD-%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="nav-number">3.3.</span> <span class="nav-text">RDD 的执行流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B"><span class="nav-number">3.4.</span> <span class="nav-text">基础编程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E7%9A%84%E5%88%9B%E5%BB%BA"><span class="nav-number">3.4.1.</span> <span class="nav-text">RDD 的创建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E5%B9%B6%E8%A1%8C%E5%BA%A6%E4%B8%8E%E5%88%86%E5%8C%BA"><span class="nav-number">3.4.2.</span> <span class="nav-text">RDD 并行度与分区</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="nav-number">3.4.3.</span> <span class="nav-text">RDD 转换算子</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Value-%E7%B1%BB%E5%9E%8B"><span class="nav-number">3.4.3.1.</span> <span class="nav-text">Value 类型</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#map"><span class="nav-number">3.4.3.1.1.</span> <span class="nav-text">map</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#mapPartitions"><span class="nav-number">3.4.3.1.2.</span> <span class="nav-text">mapPartitions</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#mapPartitionsWithIndex"><span class="nav-number">3.4.3.1.3.</span> <span class="nav-text">mapPartitionsWithIndex</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#flatMap"><span class="nav-number">3.4.3.1.4.</span> <span class="nav-text">flatMap</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#glom"><span class="nav-number">3.4.3.1.5.</span> <span class="nav-text">glom</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#groupBy"><span class="nav-number">3.4.3.1.6.</span> <span class="nav-text">groupBy</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#filter"><span class="nav-number">3.4.3.1.7.</span> <span class="nav-text">filter</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#sample"><span class="nav-number">3.4.3.1.8.</span> <span class="nav-text">sample</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#distinct"><span class="nav-number">3.4.3.1.9.</span> <span class="nav-text">distinct</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#coalesce"><span class="nav-number">3.4.3.1.10.</span> <span class="nav-text">coalesce</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#repartition"><span class="nav-number">3.4.3.1.11.</span> <span class="nav-text">repartition</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#sortBy"><span class="nav-number">3.4.3.1.12.</span> <span class="nav-text">sortBy</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8F%8C-Value-%E7%B1%BB%E5%9E%8B"><span class="nav-number">3.4.3.2.</span> <span class="nav-text">双 Value 类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Key-Value-%E7%B1%BB%E5%9E%8B"><span class="nav-number">3.4.3.3.</span> <span class="nav-text">Key - Value 类型</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#partitionBy"><span class="nav-number">3.4.3.3.1.</span> <span class="nav-text">partitionBy</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#reduceByKey"><span class="nav-number">3.4.3.3.2.</span> <span class="nav-text">reduceByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#groupByKey"><span class="nav-number">3.4.3.3.3.</span> <span class="nav-text">groupByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#aggregateByKey"><span class="nav-number">3.4.3.3.4.</span> <span class="nav-text">aggregateByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#foldByKey"><span class="nav-number">3.4.3.3.5.</span> <span class="nav-text">foldByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#combineByKey"><span class="nav-number">3.4.3.3.6.</span> <span class="nav-text">combineByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#sortByKey"><span class="nav-number">3.4.3.3.7.</span> <span class="nav-text">sortByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#join"><span class="nav-number">3.4.3.3.8.</span> <span class="nav-text">join</span></a></li></ol></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="nav-number">3.4.3.4.</span> <span class="nav-text">案例实操</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="nav-number">3.4.3.4.1.</span> <span class="nav-text">数据准备</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E6%8F%8F%E8%BF%B0"><span class="nav-number">3.4.3.4.2.</span> <span class="nav-text">需求描述</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.4.3.4.3.</span> <span class="nav-text">功能实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90"><span class="nav-number">3.4.4.</span> <span class="nav-text">RDD 行动算子</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#reduce"><span class="nav-number">3.4.4.0.1.</span> <span class="nav-text">reduce</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#collect"><span class="nav-number">3.4.4.0.2.</span> <span class="nav-text">collect</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#count"><span class="nav-number">3.4.4.0.3.</span> <span class="nav-text">count</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#first"><span class="nav-number">3.4.4.0.4.</span> <span class="nav-text">first</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#take"><span class="nav-number">3.4.4.0.5.</span> <span class="nav-text">take</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#takeOrdered"><span class="nav-number">3.4.4.0.6.</span> <span class="nav-text">takeOrdered</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#aggregate"><span class="nav-number">3.4.4.0.7.</span> <span class="nav-text">aggregate</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#fold"><span class="nav-number">3.4.4.0.8.</span> <span class="nav-text">fold</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#countByKey"><span class="nav-number">3.4.4.0.9.</span> <span class="nav-text">countByKey</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#save-%E7%9B%B8%E5%85%B3%E7%AE%97%E5%AD%90"><span class="nav-number">3.4.4.0.10.</span> <span class="nav-text">save 相关算子</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#foreach"><span class="nav-number">3.4.4.0.11.</span> <span class="nav-text">foreach</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E5%BA%8F%E5%88%97%E5%8C%96"><span class="nav-number">3.4.5.</span> <span class="nav-text">RDD 序列化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%97%AD%E5%8C%85%E6%A3%80%E6%9F%A5"><span class="nav-number">3.4.5.1.</span> <span class="nav-text">闭包检查</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BA%8F%E5%88%97%E5%8C%96%E6%96%B9%E6%B3%95%E5%92%8C%E5%B1%9E%E6%80%A7"><span class="nav-number">3.4.5.2.</span> <span class="nav-text">序列化方法和属性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Kryo-%E5%BA%8F%E5%88%97%E5%8C%96%E6%A1%86%E6%9E%B6"><span class="nav-number">3.4.5.3.</span> <span class="nav-text">Kryo 序列化框架</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB"><span class="nav-number">3.4.6.</span> <span class="nav-text">RDD 依赖关系</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB"><span class="nav-number">3.4.6.1.</span> <span class="nav-text">RDD 血缘关系</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB-1"><span class="nav-number">3.4.6.2.</span> <span class="nav-text">RDD 依赖关系</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E7%AA%84%E4%BE%9D%E8%B5%96"><span class="nav-number">3.4.6.3.</span> <span class="nav-text">RDD 窄依赖</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E5%AE%BD%E4%BE%9D%E8%B5%96"><span class="nav-number">3.4.6.4.</span> <span class="nav-text">RDD 宽依赖</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E9%98%B6%E6%AE%B5%E5%88%92%E5%88%86"><span class="nav-number">3.4.6.5.</span> <span class="nav-text">RDD 阶段划分</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-%E4%BB%BB%E5%8A%A1%E5%88%92%E5%88%86"><span class="nav-number">3.4.6.6.</span> <span class="nav-text">RDD 任务划分</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">3.4.7.</span> <span class="nav-text">RDD 持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-Cache-%E7%BC%93%E5%AD%98"><span class="nav-number">3.4.7.1.</span> <span class="nav-text">RDD Cache 缓存</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#persist-%E6%9B%B4%E6%94%B9%E5%AD%98%E5%82%A8%E7%BA%A7%E5%88%AB"><span class="nav-number">3.4.7.2.</span> <span class="nav-text">persist 更改存储级别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RDD-CheckPoint-%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="nav-number">3.4.7.3.</span> <span class="nav-text">RDD CheckPoint 检查点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%93%E5%AD%98%E5%92%8C%E6%A3%80%E6%9F%A5%E7%82%B9%E5%8C%BA%E5%88%AB"><span class="nav-number">3.4.7.4.</span> <span class="nav-text">缓存和检查点区别</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RDD-%E5%88%86%E5%8C%BA%E5%99%A8"><span class="nav-number">3.4.8.</span> <span class="nav-text">RDD 分区器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96%E4%B8%8E%E4%BF%9D%E5%AD%98"><span class="nav-number">3.4.9.</span> <span class="nav-text">文件读取与保存</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#text-%E6%96%87%E4%BB%B6"><span class="nav-number">3.4.9.1.</span> <span class="nav-text">text 文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#sequence-%E6%96%87%E4%BB%B6"><span class="nav-number">3.4.9.2.</span> <span class="nav-text">sequence 文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#object-%E5%AF%B9%E8%B1%A1"><span class="nav-number">3.4.9.3.</span> <span class="nav-text">object 对象</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">3.5.</span> <span class="nav-text">累加器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86"><span class="nav-number">3.5.1.</span> <span class="nav-text">实现原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B-1"><span class="nav-number">3.5.2.</span> <span class="nav-text">基础编程</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%B3%BB%E7%BB%9F%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">3.5.2.1.</span> <span class="nav-text">系统累加器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">3.5.2.2.</span> <span class="nav-text">自定义累加器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">3.6.</span> <span class="nav-text">广播变量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86-1"><span class="nav-number">3.6.1.</span> <span class="nav-text">实现原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9F%BA%E7%A1%80%E7%BC%96%E7%A8%8B-2"><span class="nav-number">3.6.2.</span> <span class="nav-text">基础编程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark-%E6%A1%88%E4%BE%8B%E5%AE%9E%E6%93%8D"><span class="nav-number">3.7.</span> <span class="nav-text">Spark 案例实操</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87-1"><span class="nav-number">3.7.1.</span> <span class="nav-text">数据准备</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%A6%E7%BB%86%E5%AD%97%E6%AE%B5%E8%AF%B4%E6%98%8E"><span class="nav-number">3.7.2.</span> <span class="nav-text">详细字段说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E4%B8%80-%EF%BC%9ATop10-%E7%83%AD%E9%97%A8%E5%93%81%E7%B1%BB"><span class="nav-number">3.7.3.</span> <span class="nav-text">需求一 ：Top10 热门品类</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E8%AF%B4%E6%98%8E"><span class="nav-number">3.7.3.1.</span> <span class="nav-text">需求说明</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%80"><span class="nav-number">3.7.3.2.</span> <span class="nav-text">实现一</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E4%BA%8C"><span class="nav-number">3.7.3.3.</span> <span class="nav-text">实现二</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E4%B8%89"><span class="nav-number">3.7.3.4.</span> <span class="nav-text">实现三</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0%E5%9B%9B"><span class="nav-number">3.7.3.5.</span> <span class="nav-text">实现四</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E4%BA%8C%EF%BC%9ATop10-%E7%83%AD%E9%97%A8%E5%93%81%E7%B1%BB%E4%B8%AD%E6%AF%8F%E4%B8%AA%E5%93%81%E7%B1%BB%E7%9A%84-%E7%9A%84-Top10-%E6%B4%BB%E8%B7%83-Session-%E7%BB%9F%E8%AE%A1"><span class="nav-number">3.7.4.</span> <span class="nav-text">需求二：Top10 热门品类中每个品类的 的 Top10 活跃 Session 统计</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E8%AF%B4%E6%98%8E-1"><span class="nav-number">3.7.4.1.</span> <span class="nav-text">需求说明</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.7.4.2.</span> <span class="nav-text">实现</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E4%B8%89%EF%BC%9A%E9%A1%B5%E9%9D%A2%E5%8D%95%E8%B7%B3%E8%BD%AC%E6%8D%A2%E7%8E%87%E7%BB%9F%E8%AE%A1"><span class="nav-number">3.7.5.</span> <span class="nav-text">需求三：页面单跳转换率统计</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E8%AF%B4%E6%98%8E-2"><span class="nav-number">3.7.5.1.</span> <span class="nav-text">需求说明</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E5%88%86%E6%9E%90"><span class="nav-number">3.7.5.2.</span> <span class="nav-text">需求分析</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9C%80%E6%B1%82%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.7.5.3.</span> <span class="nav-text">需求实现</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%B7%A5%E7%A8%8B%E5%8C%96%E4%BB%A3%E7%A0%81"><span class="nav-number">3.7.6.</span> <span class="nav-text">工程化代码</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#application"><span class="nav-number">3.7.6.1.</span> <span class="nav-text">application</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#common"><span class="nav-number">3.7.6.2.</span> <span class="nav-text">common</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#controller"><span class="nav-number">3.7.6.3.</span> <span class="nav-text">controller</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#dao"><span class="nav-number">3.7.6.4.</span> <span class="nav-text">dao</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#service"><span class="nav-number">3.7.6.5.</span> <span class="nav-text">service</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#util"><span class="nav-number">3.7.6.6.</span> <span class="nav-text">util</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Eitan"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Eitan</p>
  <div class="site-description" itemprop="description">blog用于记忆，大脑擅长思考</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">44</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/eitan-blog" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;eitan-blog" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:eitan_blog@163.com" title="E-Mail → mailto:eitan_blog@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>




        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/yourname" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/22/Spark%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9ASparkCore/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Eitan">
      <meta itemprop="description" content="blog用于记忆，大脑擅长思考">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Eitan's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark（二）：SparkCore
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-22 11:00:33" itemprop="dateCreated datePublished" datetime="2022-05-22T11:00:33+08:00">2022-05-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-05-27 11:32:54" itemprop="dateModified" datetime="2022-05-27T11:32:54+08:00">2022-05-27</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>63k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>57 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本文为学习笔记，对应视频教程来自<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV11A411L7CK">尚硅谷大数据Spark教程从入门到精通</a></p>
<h2 id="Spark运行架构"><a href="#Spark运行架构" class="headerlink" title="Spark运行架构"></a>Spark运行架构</h2><h3 id="运行架构"><a href="#运行架构" class="headerlink" title="运行架构"></a>运行架构</h3><p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522095955.png" alt="image-20220522095950890"></p>
<span id="more"></span>

<h3 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h3><h4 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h4><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。Driver 在 Spark 作业执行时主要负责：</p>
<ul>
<li>将用户程序转化为作业（job）</li>
<li>在 Executor 之间调度任务(task)</li>
<li>跟踪 Executor 的执行情况</li>
<li>通过 UI 展示查询运行情况</li>
</ul>
<h4 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h4><p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行。Executor 有两个核心功能：</p>
<ul>
<li>负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程</li>
<li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</li>
</ul>
<h4 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h4><p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。说的简单点就是，ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是 ApplicationMaster。</p>
<h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><h4 id="Executor-与-与-Core"><a href="#Executor-与-与-Core" class="headerlink" title="Executor 与 与 Core"></a>Executor 与 与 Core</h4><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。应用程序相关启动参数如下：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>–num-executors</td>
<td>配置 Executor 的数量</td>
</tr>
<tr>
<td>–executor-memory</td>
<td>配置每个 Executor 的内存大小</td>
</tr>
<tr>
<td>–executor-cores</td>
<td>配置每个 Executor 的虚拟 CPU core 数量</td>
</tr>
</tbody></table>
<h4 id="并行度（Parallelism-）"><a href="#并行度（Parallelism-）" class="headerlink" title="并行度（Parallelism ）"></a>并行度（Parallelism ）</h4><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行。这里我们将整个集群并行执行任务的数量称之为并行度。</p>
<h3 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h3><p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522105535.png" alt="image-20220522105532798"></p>
<h4 id="Yarn-Client-模式"><a href="#Yarn-Client-模式" class="headerlink" title="Yarn Client 模式"></a>Yarn Client 模式</h4><p>Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试。</p>
<ul>
<li>Driver 在任务提交的本地机器上运行</li>
<li>Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster</li>
<li>ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存</li>
<li>ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后 ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程</li>
<li>Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数</li>
<li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行</li>
</ul>
<h4 id="Yarn-Cluster-模式"><a href="#Yarn-Cluster-模式" class="headerlink" title="Yarn Cluster 模式"></a>Yarn Cluster 模式</h4><p>Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于实际生产环境。</p>
<ul>
<li>在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动 ApplicationMaster</li>
<li>随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver</li>
<li>Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到 ApplicationMaster 的资源申请后会分配container，然后在合适的 NodeManager 上启动 Executor 进程</li>
<li>Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行 main 函数</li>
<li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行</li>
</ul>
<h2 id="Spark-核心编程"><a href="#Spark-核心编程" class="headerlink" title="Spark 核心编程"></a>Spark 核心编程</h2><p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p>
<ul>
<li>RDD : 弹性分布式数据集</li>
<li>累加器：分布式共享只写变量</li>
<li>广播变量：分布式共享只读变量</li>
</ul>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><h3 id="RDD-概述"><a href="#RDD-概述" class="headerlink" title="RDD 概述"></a>RDD 概述</h3><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p>
<blockquote>
<ul>
<li>弹性</li>
<li>存储的弹性：内存与磁盘的自动切换；</li>
<li>容错的弹性：数据丢失可以自动恢复；</li>
<li>计算的弹性：计算出错重试机制；</li>
<li>分片的弹性：可根据需要重新分片。</li>
<li>分布式：数据存储在大数据集群不同节点上</li>
<li>数据集：RDD 封装了计算逻辑，并不保存数据</li>
<li>数据抽象：RDD 是一个抽象类，需要子类具体实现</li>
<li>不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑</li>
</ul>
</blockquote>
<h3 id="核心属性"><a href="#核心属性" class="headerlink" title="核心属性"></a>核心属性</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Internally, each RDD is characterized by five main properties:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  - A list of partitions</span></span><br><span class="line"><span class="comment"> *  - A function for computing each split</span></span><br><span class="line"><span class="comment"> *  - A list of dependencies on other RDDs</span></span><br><span class="line"><span class="comment"> *  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</span></span><br><span class="line"><span class="comment"> *  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for</span></span><br><span class="line"><span class="comment"> *    an HDFS file)</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>

<h4 id="分区列表"><a href="#分区列表" class="headerlink" title="分区列表"></a>分区列表</h4><p>RDD 数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to return the set of partitions in this RDD. This method will only</span></span><br><span class="line"><span class="comment"> * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * The partitions in this array must satisfy the following property:</span></span><br><span class="line"><span class="comment"> *   `rdd.partitions.zipWithIndex.forall &#123; case (partition, index) =&gt; partition.index == index &#125;`</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>]</span><br></pre></td></tr></table></figure>

<h4 id="分区计算函数"><a href="#分区计算函数" class="headerlink" title="分区计算函数"></a>分区计算函数</h4><p>Spark 在计算时，是使用分区函数对每一个分区进行计算</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to compute a given partition.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>]</span><br></pre></td></tr></table></figure>

<h4 id="RDD-之间的依赖关系"><a href="#RDD-之间的依赖关系" class="headerlink" title="RDD 之间的依赖关系"></a>RDD 之间的依赖关系</h4><p>RDD 是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个 RDD 建立依赖关系</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only</span></span><br><span class="line"><span class="comment"> * be called once, so it is safe to implement a time-consuming computation in it.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getDependencies</span></span>: <span class="type">Seq</span>[<span class="type">Dependency</span>[_]] = deps</span><br></pre></td></tr></table></figure>

<h4 id="分区器（可选）"><a href="#分区器（可选）" class="headerlink" title="分区器（可选）"></a>分区器（可选）</h4><p>当数据为 KV 类型数据时，可以通过设定分区器自定义数据的分区</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Optionally overridden by subclasses to specify how they are partitioned. */</span></span><br><span class="line"><span class="meta">@transient</span> <span class="keyword">val</span> partitioner: <span class="type">Option</span>[<span class="type">Partitioner</span>] = <span class="type">None</span></span><br></pre></td></tr></table></figure>

<h4 id="首选位置（可选）"><a href="#首选位置（可选）" class="headerlink" title="首选位置（可选）"></a>首选位置（可选）</h4><p>计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Optionally overridden by subclasses to specify placement preferences.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getPreferredLocations</span></span>(split: <span class="type">Partition</span>): <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Nil</span></span><br></pre></td></tr></table></figure>

<h3 id="RDD-的执行流程"><a href="#RDD-的执行流程" class="headerlink" title="RDD 的执行流程"></a>RDD 的执行流程</h3><p>启动 Yarn 集群环境</p>
<p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522162734.png" alt="image-20220522162731363"></p>
<p>Spark 通过申请资源创建调度节点和计算节点</p>
<p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522163018.png" alt="image-20220522163016911"></p>
<p>Spark 框架根据需求将计算逻辑根据分区划分成不同的任务</p>
<p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522163530.png" alt="image-20220522163528738"></p>
<p>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算</p>
<p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220522164119.png" alt="image-20220522164117888"></p>
<h3 id="基础编程"><a href="#基础编程" class="headerlink" title="基础编程"></a>基础编程</h3><h4 id="RDD-的创建"><a href="#RDD-的创建" class="headerlink" title="RDD 的创建"></a>RDD 的创建</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02_RDD_File</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// TODO 准备环境</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;RDD&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 创建RDD</span></span><br><span class="line">    <span class="comment">// 本地文件</span></span><br><span class="line">    <span class="keyword">val</span> localRdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/1.txt&quot;</span>)</span><br><span class="line">    <span class="comment">// 分布式文件</span></span><br><span class="line">    <span class="keyword">val</span> hdfsRdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;hdfs://spark152:8020/data/1.txt&quot;</span>)</span><br><span class="line">    <span class="comment">// textFile：以行为单位读取数据</span></span><br><span class="line">    <span class="comment">// wholeTextFiles：以文件为单位读取数据</span></span><br><span class="line">    <span class="keyword">val</span> wholeTextRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = sparkContext.wholeTextFiles(<span class="string">&quot;hdfs://spark152:8020/data&quot;</span>)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;================ localRdd ================&quot;</span>)</span><br><span class="line">    localRdd.collect().foreach(println)</span><br><span class="line">    println(<span class="string">&quot;================ hdfsRdd ================&quot;</span>)</span><br><span class="line">    hdfsRdd.collect().foreach(println)</span><br><span class="line">    println(<span class="string">&quot;================ wholeTextRdd ================&quot;</span>)</span><br><span class="line">    wholeTextRdd.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 关闭环境</span></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">================ localRdd ================</span><br><span class="line">Hello World</span><br><span class="line">Hello Spark</span><br><span class="line">================ hdfsRdd ================</span><br><span class="line">Hello World</span><br><span class="line">Hello Spark</span><br><span class="line">================ wholeTextRdd ================</span><br><span class="line">(hdfs:<span class="regexp">//</span>spark152:<span class="number">8020</span><span class="regexp">/data/</span><span class="number">1</span>.txt,Hello World</span><br><span class="line">Hello Spark)</span><br><span class="line">(hdfs:<span class="regexp">//</span>spark152:<span class="number">8020</span><span class="regexp">/data/</span><span class="number">2</span>.txt,Hello World</span><br><span class="line">Hello Spark)</span><br></pre></td></tr></table></figure>

<h4 id="RDD-并行度与分区"><a href="#RDD-并行度与分区" class="headerlink" title="RDD 并行度与分区"></a>RDD 并行度与分区</h4><p>Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建 RDD 时指定。这里的并行执行的任务数量，并不是指的切分任务的数量。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">// Sequences need to be sliced at the same set of index positions for operations</span></span><br><span class="line">  <span class="comment">// like RDD.zip() to behave as expected</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">positions</span></span>(length: <span class="type">Long</span>, numSlices: <span class="type">Int</span>): <span class="type">Iterator</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = &#123;</span><br><span class="line">    (<span class="number">0</span> until numSlices).iterator.map &#123; i =&gt;</span><br><span class="line">      <span class="keyword">val</span> start = ((i * length) / numSlices).toInt</span><br><span class="line">      <span class="keyword">val</span> end = (((i + <span class="number">1</span>) * length) / numSlices).toInt</span><br><span class="line">      (start, end)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">slice</span></span>(from: <span class="type">Int</span>, until: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">   <span class="keyword">val</span> reprVal = repr</span><br><span class="line">   <span class="keyword">val</span> lo = math.max(from, <span class="number">0</span>)</span><br><span class="line">   <span class="keyword">val</span> hi = math.min(math.max(until, <span class="number">0</span>), reprVal.length)</span><br><span class="line">   <span class="keyword">val</span> size = math.max(hi - lo, <span class="number">0</span>)</span><br><span class="line">   <span class="keyword">val</span> result = java.lang.reflect.<span class="type">Array</span>.newInstance(elementClass, size)</span><br><span class="line">   <span class="keyword">if</span> (size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="type">Array</span>.copy(reprVal, lo, result, <span class="number">0</span>, size)</span><br><span class="line">   &#125;</span><br><span class="line">   result.asInstanceOf[<span class="type">Array</span>[<span class="type">T</span>]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>内存数据的分区规则源码：</p>
<ol>
<li>length = 集合长度，numSlices = 分区个数</li>
<li>该函数将返回大小为 numSlices 的元组</li>
<li>元组的每一项开始位置 start = ((i * length) / numSlices).toInt</li>
<li>元组的每一项结束位置  end = (((i + 1) * length) / numSlices).toInt</li>
<li>将元组的每一项传入 slice(from: Int, until: Int) 实现左闭右开的分区规则</li>
</ol>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> InputSplit[] getSplits(JobConf job, <span class="keyword">int</span> numSplits)</span><br><span class="line">  <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">  StopWatch sw = <span class="keyword">new</span> StopWatch().start();</span><br><span class="line">  FileStatus[] stats = listStatus(job);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Save the number of input files for metrics/loadgen</span></span><br><span class="line">  job.setLong(NUM_INPUT_FILES, stats.length);</span><br><span class="line">  <span class="keyword">long</span> totalSize = <span class="number">0</span>;                           <span class="comment">// compute total size</span></span><br><span class="line">  <span class="keyword">boolean</span> ignoreDirs = !job.getBoolean(INPUT_DIR_RECURSIVE, <span class="keyword">false</span>)</span><br><span class="line">    &amp;&amp; job.getBoolean(INPUT_DIR_NONRECURSIVE_IGNORE_SUBDIRS, <span class="keyword">false</span>);</span><br><span class="line"></span><br><span class="line">  List&lt;FileStatus&gt; files = <span class="keyword">new</span> ArrayList&lt;&gt;(stats.length);</span><br><span class="line">  <span class="keyword">for</span> (FileStatus file: stats) &#123;                <span class="comment">// check we have valid files</span></span><br><span class="line">    <span class="keyword">if</span> (file.isDirectory()) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!ignoreDirs) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">&quot;Not a file: &quot;</span>+ file.getPath());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      files.add(file);</span><br><span class="line">      totalSize += file.getLen();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">long</span> goalSize = totalSize / (numSplits == <span class="number">0</span> ? <span class="number">1</span> : numSplits);</span><br><span class="line">  <span class="keyword">long</span> minSize = Math.max(job.getLong(org.apache.hadoop.mapreduce.lib.input.</span><br><span class="line">    FileInputFormat.SPLIT_MINSIZE, <span class="number">1</span>), minSplitSize);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// generate splits</span></span><br><span class="line">  ArrayList&lt;FileSplit&gt; splits = <span class="keyword">new</span> ArrayList&lt;FileSplit&gt;(numSplits);</span><br><span class="line">  NetworkTopology clusterMap = <span class="keyword">new</span> NetworkTopology();</span><br><span class="line">  <span class="keyword">for</span> (FileStatus file: files) &#123;</span><br><span class="line">    Path path = file.getPath();</span><br><span class="line">    <span class="keyword">long</span> length = file.getLen();</span><br><span class="line">    <span class="keyword">if</span> (length != <span class="number">0</span>) &#123;</span><br><span class="line">      FileSystem fs = path.getFileSystem(job);</span><br><span class="line">      BlockLocation[] blkLocations;</span><br><span class="line">      <span class="keyword">if</span> (file <span class="keyword">instanceof</span> LocatedFileStatus) &#123;</span><br><span class="line">        blkLocations = ((LocatedFileStatus) file).getBlockLocations();</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        blkLocations = fs.getFileBlockLocations(file, <span class="number">0</span>, length);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (isSplitable(fs, path)) &#123;</span><br><span class="line">        <span class="keyword">long</span> blockSize = file.getBlockSize();</span><br><span class="line">        <span class="keyword">long</span> splitSize = computeSplitSize(goalSize, minSize, blockSize);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">long</span> bytesRemaining = length;</span><br><span class="line">        <span class="keyword">while</span> (((<span class="keyword">double</span>) bytesRemaining)/splitSize &gt; SPLIT_SLOP) &#123;</span><br><span class="line">          String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations,</span><br><span class="line">              length-bytesRemaining, splitSize, clusterMap);</span><br><span class="line">          splits.add(makeSplit(path, length-bytesRemaining, splitSize,</span><br><span class="line">              splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">          bytesRemaining -= splitSize;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (bytesRemaining != <span class="number">0</span>) &#123;</span><br><span class="line">          String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations, length</span><br><span class="line">              - bytesRemaining, bytesRemaining, clusterMap);</span><br><span class="line">          splits.add(makeSplit(path, length - bytesRemaining, bytesRemaining,</span><br><span class="line">              splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">          <span class="comment">// Log only if the file is big enough to be splitted</span></span><br><span class="line">          <span class="keyword">if</span> (length &gt; Math.min(file.getBlockSize(), minSize)) &#123;</span><br><span class="line">            LOG.debug(<span class="string">&quot;File is not splittable so no parallelization &quot;</span></span><br><span class="line">                + <span class="string">&quot;is possible: &quot;</span> + file.getPath());</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        String[][] splitHosts = getSplitHostsAndCachedHosts(blkLocations,<span class="number">0</span>,length,clusterMap);</span><br><span class="line">        splits.add(makeSplit(path, <span class="number">0</span>, length, splitHosts[<span class="number">0</span>], splitHosts[<span class="number">1</span>]));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">      <span class="comment">//Create empty hosts array for zero length files</span></span><br><span class="line">      splits.add(makeSplit(path, <span class="number">0</span>, length, <span class="keyword">new</span> String[<span class="number">0</span>]));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  sw.stop();</span><br><span class="line">  <span class="keyword">if</span> (LOG.isDebugEnabled()) &#123;</span><br><span class="line">    LOG.debug(<span class="string">&quot;Total # of splits generated by getSplits: &quot;</span> + splits.size()</span><br><span class="line">        + <span class="string">&quot;, TimeTaken: &quot;</span> + sw.now(TimeUnit.MILLISECONDS));</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> splits.toArray(<span class="keyword">new</span> FileSplit[splits.size()]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>文件数据的分区规则源码：</p>
<ol>
<li>这段代码只能分析出部分实现逻辑，但是并不能完全解释文件数据的分区行为</li>
<li>totalSize = 数据的总长度，goalSize = totalSize / numSplits  为每片分区的目标长度</li>
<li>splitSize = Math.max(minSize, Math.min(goalSize, blockSize)) 实际分片大小</li>
<li>start = length-bytesRemaining，bytesRemaining -= splitSize</li>
</ol>
</blockquote>
<h4 id="RDD-转换算子"><a href="#RDD-转换算子" class="headerlink" title="RDD 转换算子"></a>RDD 转换算子</h4><h5 id="Value-类型"><a href="#Value-类型" class="headerlink" title="Value 类型"></a>Value 类型</h5><h6 id="map"><a href="#map" class="headerlink" title="map"></a>map</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从服务器日志数据 apache.log 中获取用户请求 URL 资源路径</span></span><br><span class="line"><span class="comment"> * 83.149.9.216 - - 17/05/2015:10:05:03 +0000 GET /presentations/logstash-monitorama-2013/images/kibana-search.png</span></span><br><span class="line"><span class="comment"> * 83.149.9.216 - - 17/05/2015:10:05:43 +0000 GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Operator_Transform_Example</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/apache.log&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRdd: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.map(</span><br><span class="line">      line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> array: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">        array(<span class="number">6</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    mapRdd.collect().foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取每个分区的最大值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Operator_Transform_MapPartitions</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>),<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapPartitionsRdd: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.mapPartitions(</span><br><span class="line">      iterator =&gt; <span class="type">List</span>(iterator.max).iterator</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    mapPartitionsRdd.collect().foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>map 和 mapPartitions 的区别：</p>
<ol>
<li><p>数据处理角度</p>
<p>Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作。</p>
</li>
<li><p>功能的角度</p>
<p>Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据</p>
</li>
<li><p>性能的角度</p>
<p>Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高。但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。</p>
</li>
</ol>
</blockquote>
<h6 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 获取第二个数据分区的数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_RDD_Operator_Transform_MapPartitionsWithIndex</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapPartitionWithIndexRdd: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.mapPartitionsWithIndex(</span><br><span class="line">      (index, iterator) =&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (index == <span class="number">1</span>) &#123;</span><br><span class="line">          iterator</span><br><span class="line">        &#125; <span class="keyword">else</span></span><br><span class="line">          <span class="type">Nil</span>.iterator</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    mapPartitionWithIndexRdd.collect().foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 对字符串扁平化处理为单词集合</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark05_RDD_Operator_Transform_FlatMap</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="string">&quot;Hello World&quot;</span>, <span class="string">&quot;Hello Spark&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> flatMapRdd: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    flatMapRdd.collect().foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h6><p>将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 计算所有分区最大值求和（分区内取最大值，分区间最大值求和）</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">object</span> Spark06_RDD_Operator_Transform_Glom &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array<span class="literal">[S<span class="identifier">tring</span>]</span>): Unit = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: SparkConf = <span class="keyword">new</span> <span class="constructor">SparkConf()</span>.set<span class="constructor">Master(<span class="string">&quot;local[*]&quot;</span>)</span>.set<span class="constructor">AppName(<span class="string">&quot;Operator&quot;</span>)</span></span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="constructor">SparkContext(<span class="params">sparkConf</span>)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: RDD<span class="literal">[I<span class="identifier">nt</span>]</span> = sparkContext.make<span class="constructor">RDD(List(1, 2, 3, 4)</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> glomRdd: RDD<span class="literal">[A<span class="identifier">rray</span>[I<span class="identifier">nt</span>]</span>] = rdd.glom<span class="literal">()</span></span><br><span class="line">    <span class="keyword">val</span> mapRdd: RDD<span class="literal">[I<span class="identifier">nt</span>]</span> = glomRdd.map(<span class="module-access"><span class="module"><span class="identifier">_</span>.</span></span>max)</span><br><span class="line"></span><br><span class="line">    println(mapRdd.collect<span class="literal">()</span>.sum)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop<span class="literal">()</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将 List(&quot;Hello&quot;, &quot;hive&quot;, &quot;hbase&quot;, &quot;Hadoop&quot;) 根据单词首写字母进行分组</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark07_RDD_Operator_Transform_GroupBy</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;hbase&quot;</span>, <span class="string">&quot;Hadoop&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> groupRdd: <span class="type">RDD</span>[(<span class="type">Char</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = rdd.groupBy(_.charAt(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    groupRdd.collect().foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>将数据根据指定的规则进行分组，分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle；</p>
<p>一个组的数据被放到一个分区当中，但是并不是说一个分区之中只有一个组。</p>
</blockquote>
<h6 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从服务器日志数据 apache.log 中获取 2015 年 5 月 17 日的数据</span></span><br><span class="line"><span class="comment"> * 83.149.9.216 - - 17/05/2015:10:05:03 +0000 GET /presentations/logstash-monitorama-2013/images/kibana-search.png</span></span><br><span class="line"><span class="comment"> * 83.149.9.216 - - 17/05/2015:10:05:43 +0000 GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark08_RDD_Operator_Transform_Filter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/apache.log&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> filterRdd: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.filter(</span><br><span class="line">      line =&gt; &#123;</span><br><span class="line">        line.split(<span class="string">&quot; &quot;</span>)(<span class="number">3</span>).startsWith(<span class="string">&quot;17/05/2015&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    filterRdd.collect().foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a sampled subset of this RDD.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param withReplacement can elements be sampled multiple times (replaced when sampled out)</span></span><br><span class="line"><span class="comment"> * @param fraction expected size of the sample as a fraction of this RDD&#x27;s size</span></span><br><span class="line"><span class="comment"> *  without replacement: probability that each element is chosen; fraction must be [0, 1]</span></span><br><span class="line"><span class="comment"> *  with replacement: expected number of times each element is chosen; fraction must be greater</span></span><br><span class="line"><span class="comment"> *  than or equal to 0</span></span><br><span class="line"><span class="comment"> * @param seed seed for the random number generator</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note This is NOT guaranteed to provide exactly the fraction of the count</span></span><br><span class="line"><span class="comment"> * of the given [[RDD]].</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(</span><br><span class="line">    withReplacement: <span class="type">Boolean</span>,</span><br><span class="line">    fraction: <span class="type">Double</span>,</span><br><span class="line">    seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong)</span><br></pre></td></tr></table></figure>

<h6 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark09_RDD_Operator_Transform_Distinct</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 去重通过 RDD 的 partitioner 逻辑：map(x =&gt; (x, null)).reduceByKey((x, _) =&gt; x, numPartitions).map(_._1)</span></span><br><span class="line">    rdd.distinct().collect().foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h6><p>根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少分区的个数，减小任务调度成本</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD that is reduced into `numPartitions` partitions.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This results in a narrow dependency, e.g. if you go from 1000 partitions</span></span><br><span class="line"><span class="comment"> * to 100 partitions, there will not be a shuffle, instead each of the 100</span></span><br><span class="line"><span class="comment"> * new partitions will claim 10 of the current partitions. If a larger number</span></span><br><span class="line"><span class="comment"> * of partitions is requested, it will stay at the current number of partitions.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * However, if you&#x27;re doing a drastic coalesce, e.g. to numPartitions = 1,</span></span><br><span class="line"><span class="comment"> * this may result in your computation taking place on fewer nodes than</span></span><br><span class="line"><span class="comment"> * you like (e.g. one node in the case of numPartitions = 1). To avoid this,</span></span><br><span class="line"><span class="comment"> * you can pass shuffle = true. This will add a shuffle step, but means the</span></span><br><span class="line"><span class="comment"> * current upstream partitions will be executed in parallel (per whatever</span></span><br><span class="line"><span class="comment"> * the current partitioning is).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note With shuffle = true, you can actually coalesce to a larger number</span></span><br><span class="line"><span class="comment"> * of partitions. This is useful if you have a small number of partitions,</span></span><br><span class="line"><span class="comment"> * say 100, potentially with a few partitions being abnormally large. Calling</span></span><br><span class="line"><span class="comment"> * coalesce(1000, shuffle = true) will result in 1000 partitions with the</span></span><br><span class="line"><span class="comment"> * data distributed using a hash partitioner. The optional partition coalescer</span></span><br><span class="line"><span class="comment"> * passed in must be serializable.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coalesce</span></span>(numPartitions: <span class="type">Int</span>, shuffle: <span class="type">Boolean</span> = <span class="literal">false</span>,</span><br><span class="line">             partitionCoalescer: <span class="type">Option</span>[<span class="type">PartitionCoalescer</span>] = <span class="type">Option</span>.empty)</span><br><span class="line">            (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>)</span><br></pre></td></tr></table></figure>

<h6 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h6><p>该操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true。无论是将分区数多的 RDD 转换为分区数少的 RDD，还是将分区数少的 RDD 转换为分区数多的 RDD，repartition 操作都可以完成，因为无论如何都会经 shuffle 过程。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return a new RDD that has exactly numPartitions partitions.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Can increase or decrease the level of parallelism in this RDD. Internally, this uses</span></span><br><span class="line"><span class="comment"> * a shuffle to redistribute data.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * If you are decreasing the number of partitions in this RDD, consider using `coalesce`,</span></span><br><span class="line"><span class="comment"> * which can avoid performing a shuffle.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartition</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  coalesce(numPartitions, shuffle = <span class="literal">true</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h6><p>该操作用于排序数据，默认为升序排列，第二个参数可以改变排序方式。排序后新产生的 RDD 的分区数与原 RDD 的分区数一致,中间存在 shuffle 的过。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark10_RDD_Operator_Transform_SortBy</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>((<span class="number">1</span>, <span class="number">9</span>), (<span class="number">2</span>, <span class="number">8</span>), (<span class="number">3</span>, <span class="number">7</span>)))</span><br><span class="line">    </span><br><span class="line">    rdd.sortBy(_._1).collect().foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="双-Value-类型"><a href="#双-Value-类型" class="headerlink" title="双 Value 类型"></a>双 Value 类型</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark11_RDD_Operator_Transform</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// intersection 交集</span></span><br><span class="line">    println(rdd1.intersection(rdd2).collect().mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// union 并集</span></span><br><span class="line">    println(rdd1.union(rdd2).collect().mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// subtract 差集</span></span><br><span class="line">    println(rdd1.subtract(rdd2).collect().mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// zip 拉链：将两个 RDD 中的元素，以键值对的形式进行合并。</span></span><br><span class="line">    println(rdd1.zip(rdd2).collect().mkString(<span class="string">&quot;,&quot;</span>))</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Key-Value-类型"><a href="#Key-Value-类型" class="headerlink" title="Key - Value 类型"></a>Key - Value 类型</h5><h6 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark12_RDD_Operator_Transform_PartitionBy</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>), <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    rdd.map((_, <span class="number">1</span>)).</span><br><span class="line">      partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>)).</span><br><span class="line">      saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将数据按照相同的 Key 对 Value 进行聚合</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark13_RDD_Operator_Transform_ReduceByKey</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">      (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// reduceByKey 中如果 Key 的数据只有一个，是不会参与运算的</span></span><br><span class="line">    rdd.reduceByKey((x, y) =&gt; &#123;</span><br><span class="line">      println(<span class="string">s&quot;x = <span class="subst">$&#123;x&#125;</span>, y = <span class="subst">$&#123;y&#125;</span>&quot;</span>)</span><br><span class="line">      x + y</span><br><span class="line">    &#125;).collect.foreach(println)</span><br><span class="line">    </span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将数据按照相同的 Key 对 Value 进行聚合</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark14_RDD_Operator_Transform_GroupByKey</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">      (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// groupByKey：将数据源中，相同 Key 的数据分在一个组中，形成一个对偶元组</span></span><br><span class="line">    <span class="keyword">val</span> groupRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = rdd.groupByKey()</span><br><span class="line"></span><br><span class="line">    groupRdd.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将数据根据不同的规则进行分区内计算和分区间计算</span></span><br><span class="line"><span class="comment"> * 需求：分区内求最大值，分区间相加</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark15_RDD_Operator_Transform_AggregateByKey</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">      (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>),</span><br><span class="line">      (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">6</span>)</span><br><span class="line">    ), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// aggregateByKey 存在两个柯里化，有两个参数列表</span></span><br><span class="line">    <span class="comment">// 第一个参数列表需要传入一个参数表示初始值</span></span><br><span class="line">    <span class="comment">// 第二个参数列表需要传递两个参数</span></span><br><span class="line">    <span class="comment">//    第一个参数表示分区内计算规则</span></span><br><span class="line">    <span class="comment">//    第二个参数表示分区间计算规则</span></span><br><span class="line">    rdd.aggregateByKey(<span class="type">Int</span>.<span class="type">MinValue</span>)(</span><br><span class="line">      math.max(_, _),</span><br><span class="line">      _ + _</span><br><span class="line">    ).collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 需求：求不同 Key 的 Value 平均值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark15_RDD_Operator_Transform_AggregateByKey_Average</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">      (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>),</span><br><span class="line">      (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">6</span>)</span><br><span class="line">    ), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// (K, V) -&gt; (K, (V, 1)) -&gt; (K, (sum(V), sum(1))) -&gt; (Key, sum(V)/ sum(1))</span></span><br><span class="line">    println(<span class="string">&quot;---------- method one ----------&quot;</span>)</span><br><span class="line">    rdd.map(</span><br><span class="line">      tuple =&gt; &#123;</span><br><span class="line">        (tuple._1, (tuple._2, <span class="number">1</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey((t1, t2) =&gt; &#123;</span><br><span class="line">      (t1._1 + t2._1, t1._2 + t2._2)</span><br><span class="line">    &#125;).map(</span><br><span class="line">      tuple =&gt; &#123;</span><br><span class="line">        (tuple._1, tuple._2._1 / tuple._2._2)</span><br><span class="line">      &#125;</span><br><span class="line">    ).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// (K, V) -&gt; (K, (sum(1), sum(V)) -&gt; (K, sum(V) / sum(1)</span></span><br><span class="line">    println(<span class="string">&quot;---------- method two ----------&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> aggRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = rdd.aggregateByKey((<span class="number">0</span>, <span class="number">0</span>))(</span><br><span class="line">      (t, v) =&gt; &#123;</span><br><span class="line">        (t._1 + <span class="number">1</span>, t._2 + v)</span><br><span class="line">      &#125;,</span><br><span class="line">      (t1, t2) =&gt; &#123;</span><br><span class="line">        (t1._1 + t2._1, t1._2 + t2._2)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line">    aggRdd.map(</span><br><span class="line">      t =&gt; &#123;</span><br><span class="line">        (t._1, t._2._2 / t._2._1)</span><br><span class="line">      &#125;</span><br><span class="line">    ).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark16_RDD_Operator_Transform_FoldByKey</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">      (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>),</span><br><span class="line">      (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">6</span>)</span><br><span class="line">    ), <span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    rdd.foldByKey(<span class="type">Int</span>.<span class="type">MinValue</span>)(_ + _).collect.foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 将数据 List((&quot;a&quot;, 1), (&quot;a&quot;, 2), (&quot;b&quot;, 3), (&quot;b&quot;, 4), (&quot;b&quot;, 5), (&quot;a&quot;, 6))求每个 key 的平均值</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark17_RDD_Operator_Transform_CombineByKey</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">      (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>),</span><br><span class="line">      (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">6</span>)</span><br><span class="line">    ), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> combineRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = rdd.combineByKey(</span><br><span class="line">      (_, <span class="number">1</span>),</span><br><span class="line">      (x: (<span class="type">Int</span>, <span class="type">Int</span>), y) =&gt; &#123;</span><br><span class="line">        (x._1 + y, x._2 + <span class="number">1</span>)</span><br><span class="line">      &#125;,</span><br><span class="line">      (x: (<span class="type">Int</span>, <span class="type">Int</span>), y: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; &#123;</span><br><span class="line">        (x._1 + y._1, x._2 + y._2)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    combineRdd.map(</span><br><span class="line">      x =&gt; &#123;</span><br><span class="line">        (x._1, x._2._1 / x._2._2)</span><br><span class="line">      &#125;</span><br><span class="line">    ).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别？</p>
<ul>
<li>reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同</li>
<li>FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同</li>
<li>AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同</li>
<li>CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同。</li>
</ul>
</blockquote>
<h6 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口(特质)，返回一个按照 key 进行排序</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark18_RDD_Operator_Transform_SortByKey</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">      (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>),</span><br><span class="line">      (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">6</span>)</span><br><span class="line">    ), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;ascending = true&quot;</span>)</span><br><span class="line">    rdd.sortByKey(<span class="literal">true</span>).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;ascending = false&quot;</span>)</span><br><span class="line">    rdd.sortByKey(<span class="literal">false</span>).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="join"><a href="#join" class="headerlink" title="join"></a>join</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * -------- join --------</span></span><br><span class="line"><span class="comment"> * (a,(1,4))</span></span><br><span class="line"><span class="comment"> * (b,(2,5))</span></span><br><span class="line"><span class="comment"> * -------- leftOuterJoin --------</span></span><br><span class="line"><span class="comment"> * (a,(1,Some(4)))</span></span><br><span class="line"><span class="comment"> * (b,(2,Some(5)))</span></span><br><span class="line"><span class="comment"> * (c,(3,None))</span></span><br><span class="line"><span class="comment"> * -------- rightOuterJoin --------</span></span><br><span class="line"><span class="comment"> * (a,(Some(1),4))</span></span><br><span class="line"><span class="comment"> * (b,(Some(2),5))</span></span><br><span class="line"><span class="comment"> * (d,(None,6))</span></span><br><span class="line"><span class="comment"> * (d,(None,7))</span></span><br><span class="line"><span class="comment"> * -------- cogroup --------</span></span><br><span class="line"><span class="comment"> * (a,(CompactBuffer(1),CompactBuffer(4)))</span></span><br><span class="line"><span class="comment"> * (b,(CompactBuffer(2),CompactBuffer(5)))</span></span><br><span class="line"><span class="comment"> * (c,(CompactBuffer(3),CompactBuffer()))</span></span><br><span class="line"><span class="comment"> * (d,(CompactBuffer(),CompactBuffer(6, 7)))</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark19_RDD_Operator_Transform_Join</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>)))</span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">7</span>)))</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;-------- join --------&quot;</span>)</span><br><span class="line">    rdd1.join(rdd2).collect().foreach(println)</span><br><span class="line">    println(<span class="string">&quot;-------- leftOuterJoin --------&quot;</span>)</span><br><span class="line">    rdd1.leftOuterJoin(rdd2).collect().foreach(println)</span><br><span class="line">    println(<span class="string">&quot;-------- rightOuterJoin --------&quot;</span>)</span><br><span class="line">    rdd1.rightOuterJoin(rdd2).collect().foreach(println)</span><br><span class="line">    println(<span class="string">&quot;-------- cogroup --------&quot;</span>)</span><br><span class="line">    rdd1.cogroup(rdd2).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="案例实操"><a href="#案例实操" class="headerlink" title="案例实操"></a>案例实操</h5><h6 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h6><p>agent.log：时间戳，省份，城市，用户，广告，中间字段使用空格分隔</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1516609143867 6 7 64 16</span><br><span class="line">1516609143869 9 4 75 18</span><br><span class="line">1516609143869 1 7 87 12</span><br><span class="line">1516609143869 2 8 92 9</span><br><span class="line">1516609143869 6 7 84 24</span><br><span class="line">1516609143869 1 8 95 5</span><br><span class="line">1516609143869 8 1 90 29</span><br><span class="line">1516609143869 3 3 36 16</span><br></pre></td></tr></table></figure>

<h6 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h6><p>统计出每一个省份 每个广告被点击数量排行的 Top3</p>
<h6 id="功能实现"><a href="#功能实现" class="headerlink" title="功能实现"></a>功能实现</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark20_RDD_Operator_Transform_Case</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data/agent.log&quot;</span>)</span><br><span class="line">    rdd.map(</span><br><span class="line">      line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">        ((data(<span class="number">1</span>), data(<span class="number">4</span>)), <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line">      .map(</span><br><span class="line">        data =&gt; &#123;</span><br><span class="line">          (data._1._1, (data._1._2, data._2))</span><br><span class="line">        &#125;</span><br><span class="line">      ).groupByKey()</span><br><span class="line">      .mapValues(</span><br><span class="line">        _.toList.sortBy(_._2)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).take(<span class="number">3</span>)</span><br><span class="line">      ).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="RDD-行动算子"><a href="#RDD-行动算子" class="headerlink" title="RDD 行动算子"></a>RDD 行动算子</h4><h6 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Operator_Action_Reduce</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO - 行动算子</span></span><br><span class="line">    <span class="comment">// 所谓的行动算子，其实就是触发作业（Job）执行的方法</span></span><br><span class="line">    <span class="comment">// 底层调用的是环境对象的 runJob 方法</span></span><br><span class="line">    <span class="comment">// 底层代码中会创建 ActiveJob，并提交执行</span></span><br><span class="line">    println(rdd.reduce(_ + _))</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 以数组 Array 的形式返回数据集的所有元素</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02_RDD_Operator_Action_Collect</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    rdd.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="count"><a href="#count" class="headerlink" title="count"></a>count</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 返回 RDD 中元素的个数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_RDD_Operator_Action_Count</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    println(rdd.count())</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="first"><a href="#first" class="headerlink" title="first"></a>first</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 返回 RDD 中的第一个元素</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_RDD_Operator_Action_First</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    println(rdd.first())</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="take"><a href="#take" class="headerlink" title="take"></a>take</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 返回一个由 RDD 的前 n 个元素组成的数组</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark05_RDD_Operator_Action_Take</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    rdd.take(<span class="number">3</span>).foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="takeOrdered"><a href="#takeOrdered" class="headerlink" title="takeOrdered"></a>takeOrdered</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 返回该 RDD 排序后的前 n 个元素组成的数组</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark05_RDD_Operator_Action_TakeOrdered</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    rdd.takeOrdered(<span class="number">3</span>)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 分区的数据通过初始值和分区内的数据进行聚合，然后再和初始值进行分区间的数据聚合</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark06_RDD_Operator_Action_Aggregate</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// aggregateByKey：初始值只会参与分区内计算</span></span><br><span class="line">    <span class="comment">// aggregate：初始值会参与分区内计算，也会参与分区间计算</span></span><br><span class="line">    println(rdd.aggregate(<span class="number">0</span>)(_ + _, _ + _))</span><br><span class="line">    println(rdd.aggregate(<span class="number">10</span>)(_ + _, _ + _))</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="fold"><a href="#fold" class="headerlink" title="fold"></a>fold</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 折叠操作，aggregate 的简化版操作</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark06_RDD_Operator_Action_Fold</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), <span class="number">8</span>)</span><br><span class="line">    </span><br><span class="line">    println(rdd.fold(<span class="number">0</span>)(_ + _))</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 统计每种 key 的个数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark08_RDD_Operator_Action_CountByKey</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;---------- countByValue ----------&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd1: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">val</span> intToLong: collection.<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Long</span>] = rdd1.countByValue()</span><br><span class="line">    println(intToLong)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;---------- countByKey ----------&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">      (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">4</span>)</span><br><span class="line">    ))</span><br><span class="line">    <span class="keyword">val</span> stringToLong: collection.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = rdd2.countByKey()</span><br><span class="line">    println(stringToLong)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h6 id="save-相关算子"><a href="#save-相关算子" class="headerlink" title="save 相关算子"></a>save 相关算子</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存成 Text 文件</span></span><br><span class="line">rdd.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"><span class="comment">// 序列化成对象保存到文件</span></span><br><span class="line">rdd.saveAsObjectFile(<span class="string">&quot;output1&quot;</span>)</span><br><span class="line"><span class="comment">// 保存成 Sequencefile 文件</span></span><br><span class="line">rdd.map((_,<span class="number">1</span>)).saveAsSequenceFile(<span class="string">&quot;output2&quot;</span>)</span><br></pre></td></tr></table></figure>

<h6 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h6><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 分布式遍历 RDD 中的每一个元素，调用指定函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark09_RDD_Operator_Action_Foreach</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Operator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Driver 端内存集合的循环遍历方法</span></span><br><span class="line">    rdd.collect().foreach(println)</span><br><span class="line">    println(<span class="string">&quot;================&quot;</span>)</span><br><span class="line">    <span class="comment">// Executor 端内存数据的循环遍历</span></span><br><span class="line">    rdd.foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="RDD-序列化"><a href="#RDD-序列化" class="headerlink" title="RDD 序列化"></a>RDD 序列化</h4><h5 id="闭包检查"><a href="#闭包检查" class="headerlink" title="闭包检查"></a>闭包检查</h5><p>从计算的角度, <strong>算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor 端执行</strong>。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor 端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12 版本后闭包编译方式发生了改变</p>
<h5 id="序列化方法和属性"><a href="#序列化方法和属性" class="headerlink" title="序列化方法和属性"></a>序列化方法和属性</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Serial</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Serial&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.makeRDD(<span class="type">Array</span>(<span class="string">&quot;hello world&quot;</span>, <span class="string">&quot;hello spark&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;atguigu&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> search = <span class="keyword">new</span> <span class="type">Search</span>(<span class="string">&quot;h&quot;</span>)</span><br><span class="line"></span><br><span class="line">    search.getMatch1(rdd).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 类的构造参数其实是类的属性，所以需要闭包检测</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">Search</span>(<span class="params">query: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMatch</span></span>(s: <span class="type">String</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">      s.contains(query)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 函数序列化案例</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatch1</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">      rdd.filter(isMatch)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 属性序列化案例</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getMatch2</span></span>(rdd: <span class="type">RDD</span>[<span class="type">String</span>]): <span class="type">RDD</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">      rdd.filter(x =&gt; x.contains(query))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="Kryo-序列化框架"><a href="#Kryo-序列化框架" class="headerlink" title="Kryo 序列化框架"></a>Kryo 序列化框架</h5><p>参考地址: <a target="_blank" rel="noopener" href="https://github.com/EsotericSoftware/kryo">https://github.com/EsotericSoftware/kryo</a><br>Java 的序列化能够序列化任何的类。但是比较重（字节多），序列化后，对象的提交也比较大。Spark 出于性能的考虑，Spark2.0 开始支持另外一种 Kryo 序列化机制。Kryo 速度是 Serializable 的 10 倍。当 RDD 在 Shuffle 数据的时候，简单数据类型、数组和字符串类型已经在 Spark 内部使用 Kryo 来序列化。<br>注意：即使使用 Kryo 序列化，也要继承 Serializable 接口</p>
<h4 id="RDD-依赖关系"><a href="#RDD-依赖关系" class="headerlink" title="RDD 依赖关系"></a>RDD 依赖关系</h4><h5 id="RDD-血缘关系"><a href="#RDD-血缘关系" class="headerlink" title="RDD 血缘关系"></a>RDD 血缘关系</h5><p>RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列 Lineage（血统）记录下来，以便恢复丢失的分区。RDD 的 Lineage 会记录 RDD 的元数据信息和转换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ---------------- lines ----------------</span></span><br><span class="line"><span class="comment"> * (2) data\1.txt,data\2.txt MapPartitionsRDD[1] at textFile at Spark01_RDD_Dependence.scala:11 []</span></span><br><span class="line"><span class="comment"> *  |  data\1.txt,data\2.txt HadoopRDD[0] at textFile at Spark01_RDD_Dependence.scala:11 []</span></span><br><span class="line"><span class="comment"> * ---------------- words ----------------</span></span><br><span class="line"><span class="comment"> * (2) MapPartitionsRDD[2] at flatMap at Spark01_RDD_Dependence.scala:15 []</span></span><br><span class="line"><span class="comment"> *  |  data\1.txt,data\2.txt MapPartitionsRDD[1] at textFile at Spark01_RDD_Dependence.scala:11 []</span></span><br><span class="line"><span class="comment"> *  |  data\1.txt,data\2.txt HadoopRDD[0] at textFile at Spark01_RDD_Dependence.scala:11 []</span></span><br><span class="line"><span class="comment"> * ---------------- wordToOne ----------------</span></span><br><span class="line"><span class="comment"> * (2) MapPartitionsRDD[3] at map at Spark01_RDD_Dependence.scala:19 []</span></span><br><span class="line"><span class="comment"> *  |  MapPartitionsRDD[2] at flatMap at Spark01_RDD_Dependence.scala:15 []</span></span><br><span class="line"><span class="comment"> *  |  data\1.txt,data\2.txt MapPartitionsRDD[1] at textFile at Spark01_RDD_Dependence.scala:11 []</span></span><br><span class="line"><span class="comment"> *  |  data\1.txt,data\2.txt HadoopRDD[0] at textFile at Spark01_RDD_Dependence.scala:11 []</span></span><br><span class="line"><span class="comment"> * ---------------- wordToSum ----------------</span></span><br><span class="line"><span class="comment"> * (2) ShuffledRDD[4] at reduceByKey at Spark01_RDD_Dependence.scala:23 []</span></span><br><span class="line"><span class="comment"> *  +-(2) MapPartitionsRDD[3] at map at Spark01_RDD_Dependence.scala:19 []</span></span><br><span class="line"><span class="comment"> *     |  MapPartitionsRDD[2] at flatMap at Spark01_RDD_Dependence.scala:15 []</span></span><br><span class="line"><span class="comment"> *     |  data\1.txt,data\2.txt MapPartitionsRDD[1] at textFile at Spark01_RDD_Dependence.scala:11 []</span></span><br><span class="line"><span class="comment"> *     |  data\1.txt,data\2.txt HadoopRDD[0] at textFile at Spark01_RDD_Dependence.scala:11 []</span></span><br><span class="line"><span class="comment"> * (Hello,4)</span></span><br><span class="line"><span class="comment"> * (World,2)</span></span><br><span class="line"><span class="comment"> * (Spark,2)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Dependence</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;Dependence&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data\\1.txt,data\\2.txt&quot;</span>)</span><br><span class="line">    println(<span class="string">&quot;---------------- lines ----------------&quot;</span>)</span><br><span class="line">    println(lines.toDebugString)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> words: <span class="type">RDD</span>[<span class="type">String</span>] = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    println(<span class="string">&quot;---------------- words ----------------&quot;</span>)</span><br><span class="line">    println(words.toDebugString)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordToOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.map((_, <span class="number">1</span>))</span><br><span class="line">    println(<span class="string">&quot;---------------- wordToOne ----------------&quot;</span>)</span><br><span class="line">    println(wordToOne.toDebugString)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordToSum: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKey(_ + _)</span><br><span class="line">    println(<span class="string">&quot;---------------- wordToSum ----------------&quot;</span>)</span><br><span class="line">    println(wordToSum.toDebugString)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> array: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToSum.collect()</span><br><span class="line">    array.foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="RDD-依赖关系-1"><a href="#RDD-依赖关系-1" class="headerlink" title="RDD 依赖关系"></a>RDD 依赖关系</h5><p>这里所谓的依赖关系，其实就是两个相邻 RDD 之间的关系</p>
<h5 id="RDD-窄依赖"><a href="#RDD-窄依赖" class="headerlink" title="RDD 窄依赖"></a>RDD 窄依赖</h5><p>窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OneToOneDependency</span>[<span class="type">T</span>](<span class="params">rdd: <span class="type">RDD</span>[<span class="type">T</span>]</span>) <span class="keyword">extends</span> <span class="title">NarrowDependency</span>[<span class="type">T</span>](<span class="params">rdd</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getParents</span></span>(partitionId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(partitionId)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="RDD-宽依赖"><a href="#RDD-宽依赖" class="headerlink" title="RDD 宽依赖"></a>RDD 宽依赖</h5><p>宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * :: DeveloperApi ::</span></span><br><span class="line"><span class="comment"> * Represents a dependency on the output of a shuffle stage. Note that in the case of shuffle,</span></span><br><span class="line"><span class="comment"> * the RDD is transient since we don&#x27;t need it on the executor side.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param _rdd the parent RDD</span></span><br><span class="line"><span class="comment"> * @param partitioner partitioner used to partition the shuffle output</span></span><br><span class="line"><span class="comment"> * @param serializer [[org.apache.spark.serializer.Serializer Serializer]] to use. If not set</span></span><br><span class="line"><span class="comment"> *                   explicitly then the default serializer, as specified by `spark.serializer`</span></span><br><span class="line"><span class="comment"> *                   config option, will be used.</span></span><br><span class="line"><span class="comment"> * @param keyOrdering key ordering for RDD&#x27;s shuffles</span></span><br><span class="line"><span class="comment"> * @param aggregator map/reduce-side aggregator for RDD&#x27;s shuffle</span></span><br><span class="line"><span class="comment"> * @param mapSideCombine whether to perform partial aggregation (also known as map-side combine)</span></span><br><span class="line"><span class="comment"> * @param shuffleWriterProcessor the processor to control the write behavior in ShuffleMapTask</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">    @transient private val _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="params"><span class="class">    val partitioner: <span class="type">Partitioner</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer,</span></span></span><br><span class="line"><span class="params"><span class="class">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="class">    val mapSideCombine: <span class="type">Boolean</span> = false,</span></span></span><br><span class="line"><span class="params"><span class="class">    val shuffleWriterProcessor: <span class="type">ShuffleWriteProcessor</span> = new <span class="type">ShuffleWriteProcessor</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]] <span class="keyword">with</span> <span class="type">Logging</span> </span><br></pre></td></tr></table></figure>

<h5 id="RDD-阶段划分"><a href="#RDD-阶段划分" class="headerlink" title="RDD 阶段划分"></a>RDD 阶段划分</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Create a ResultStage associated with the provided jobId.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(</span><br><span class="line">    rdd: <span class="type">RDD</span>[_],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    jobId: <span class="type">Int</span>,</span><br><span class="line">    callSite: <span class="type">CallSite</span>): <span class="type">ResultStage</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd)</span><br><span class="line">  <span class="keyword">val</span> resourceProfile = mergeResourceProfilesForStage(resourceProfiles)</span><br><span class="line">  checkBarrierStageWithDynamicAllocation(rdd)</span><br><span class="line">  checkBarrierStageWithNumSlots(rdd, resourceProfile)</span><br><span class="line">  checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)</span><br><span class="line">  <span class="keyword">val</span> parents = getOrCreateParentStages(shuffleDeps, jobId)</span><br><span class="line">  <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">  <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId,</span><br><span class="line">    callSite, resourceProfile.id)</span><br><span class="line">  stageIdToStage(id) = stage</span><br><span class="line">  updateJobIdStageIdMaps(jobId, stage)</span><br><span class="line">  stage</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Get or create the list of parent stages for the given shuffle dependencies. The new</span></span><br><span class="line"><span class="comment"> * Stages will be created with the provided firstJobId.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getOrCreateParentStages</span></span>(shuffleDeps: <span class="type">HashSet</span>[<span class="type">ShuffleDependency</span>[_, _, _]],</span><br><span class="line">    firstJobId: <span class="type">Int</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">  shuffleDeps.map &#123; shuffleDep =&gt;</span><br><span class="line">    getOrCreateShuffleMapStage(shuffleDep, firstJobId)</span><br><span class="line">  &#125;.toList</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<ol>
<li>提交 Job 时，会查找当前 RDD 有无 shuffleDeps</li>
<li>为每个 shuffleDeps 创建对应的 shuffleMapStage</li>
<li>不管有没有 shuffleDeps 都会创建 ResultStage</li>
</ol>
</blockquote>
<h5 id="RDD-任务划分"><a href="#RDD-任务划分" class="headerlink" title="RDD 任务划分"></a>RDD 任务划分</h5><p>RDD 任务切分中间分为：Application、Job、Stage 和 Task</p>
<ul>
<li>Application：初始化一个 SparkContext 即生成一个 Application；</li>
<li>Job：一个 Action 算子就会生成一个 Job；</li>
<li>Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；</li>
<li>Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数。</li>
</ul>
<blockquote>
<p>注意：Application-&gt;Job-&gt;Stage-&gt;Task 每一层都是 1 对 n 的关系。</p>
</blockquote>
<h4 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h4><h5 id="RDD-Cache-缓存"><a href="#RDD-Cache-缓存" class="headerlink" title="RDD Cache 缓存"></a>RDD Cache 缓存</h5><p>RDD 通过 Cache 或者 Persist 方法将前面的计算结果缓存，默认情况下会把数据以缓存在 JVM 的堆内存中。但是并不是这两个方法被调用时立即缓存，而是触发后面的 action 算子时，该 RDD 将会被缓存在计算节点的内存中，并供后面重用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * RDD 不存储数据，如果一个 RDD 需要重复使用，那么需要从头再次执行来获取数据</span></span><br><span class="line"><span class="comment"> * // mapRdd.cache()                mapRdd.cache()</span></span><br><span class="line"><span class="comment"> * ------ map ------                ------ map ------</span></span><br><span class="line"><span class="comment"> * ------ map ------                ------ map ------</span></span><br><span class="line"><span class="comment"> * ------ map ------                ------ map ------</span></span><br><span class="line"><span class="comment"> * ------ map ------                ------ map ------</span></span><br><span class="line"><span class="comment"> * (Hello,2)                        (Hello,2)</span></span><br><span class="line"><span class="comment"> * (Spark,1)                        (Spark,1)</span></span><br><span class="line"><span class="comment"> * (World,1)                        (Spark,1)</span></span><br><span class="line"><span class="comment"> * ====================             ====================</span></span><br><span class="line"><span class="comment"> * ------ map ------                (Hello,2)</span></span><br><span class="line"><span class="comment"> * ------ map ------                (Spark,1)</span></span><br><span class="line"><span class="comment"> * ------ map ------                (Spark,1)</span></span><br><span class="line"><span class="comment"> * ------ map ------</span></span><br><span class="line"><span class="comment"> * (Hello,2)</span></span><br><span class="line"><span class="comment"> * (Spark,1)</span></span><br><span class="line"><span class="comment"> * (World,1)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Cache</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Cache&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.makeRDD(<span class="type">Array</span>(<span class="string">&quot;Hello World&quot;</span>, <span class="string">&quot;Hello Spark&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> flatMapRdd: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = flatMapRdd.map(</span><br><span class="line">      word =&gt; &#123;</span><br><span class="line">        println(<span class="string">&quot;------ map ------&quot;</span>)</span><br><span class="line">        (word, <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    mapRdd.cache()</span><br><span class="line"></span><br><span class="line">    mapRdd.reduceByKey(_ + _).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;====================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    mapRdd.aggregateByKey(<span class="number">0</span>)(_ + _, _ + _).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="persist-更改存储级别"><a href="#persist-更改存储级别" class="headerlink" title="persist 更改存储级别"></a>persist 更改存储级别</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02_RDD_Persist</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Persist&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.makeRDD(<span class="type">Array</span>(<span class="string">&quot;Hello World&quot;</span>, <span class="string">&quot;Hello Spark&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> flatMapRdd: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = flatMapRdd.map(</span><br><span class="line">      word =&gt; &#123;</span><br><span class="line">        println(<span class="string">&quot;------ map ------&quot;</span>)</span><br><span class="line">        (word, <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 可以更改存储级别</span></span><br><span class="line">    mapRdd.persist(<span class="type">StorageLevel</span>.<span class="type">DISK_ONLY</span>)</span><br><span class="line"></span><br><span class="line">    mapRdd.reduceByKey(_ + _).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;====================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    mapRdd.aggregateByKey(<span class="number">0</span>)(_ + _, _ + _).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>级别</th>
<th>使用的空间</th>
<th>CPU时间</th>
<th>是否在内存中</th>
<th>是否在磁盘上</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>MEMORY_ONLY</td>
<td>高</td>
<td>低</td>
<td>是</td>
<td>否</td>
<td></td>
</tr>
<tr>
<td>MEMORY_ONLY_SER</td>
<td>低</td>
<td>高</td>
<td>是</td>
<td>否</td>
<td></td>
</tr>
<tr>
<td>MEMORY_AND_DISK</td>
<td>高</td>
<td>中等</td>
<td>部分</td>
<td>部分</td>
<td>如果数据在内存中放不下，则溢写到磁盘上</td>
</tr>
<tr>
<td>MEMORY_AND_DISK_SER</td>
<td>低</td>
<td>高</td>
<td>部分</td>
<td>部分·</td>
<td>如果数据在内存中放不下，则溢写到磁盘上。在内存中存放序列化后的数据</td>
</tr>
<tr>
<td>DISK_ONLY</td>
<td>低</td>
<td>高</td>
<td>否</td>
<td>是</td>
<td></td>
</tr>
</tbody></table>
<h5 id="RDD-CheckPoint-检查点"><a href="#RDD-CheckPoint-检查点" class="headerlink" title="RDD CheckPoint 检查点"></a>RDD CheckPoint 检查点</h5><p>所谓的检查点其实就是通过将 RDD 中间结果写入磁盘由于血缘依赖过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果检查点之后有节点出现问题，可以从检查点开始重做血缘，减少了开销。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_RDD_CheckPoint</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;CheckPoint&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    sparkContext.setCheckpointDir(<span class="string">&quot;checkpoint&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.makeRDD(<span class="type">Array</span>(<span class="string">&quot;Hello World&quot;</span>, <span class="string">&quot;Hello Spark&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> flatMapRdd: <span class="type">RDD</span>[<span class="type">String</span>] = rdd.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> mapRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = flatMapRdd.map(</span><br><span class="line">      word =&gt; &#123;</span><br><span class="line">        println(<span class="string">&quot;------ map ------&quot;</span>)</span><br><span class="line">        (word, <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// checkpoint 需要落盘，需要指定检查点保存路径</span></span><br><span class="line">    <span class="comment">// 检查点路径保存的文件，当作业执行完毕后不会被删除</span></span><br><span class="line">    mapRdd.checkpoint()</span><br><span class="line"></span><br><span class="line">    mapRdd.reduceByKey(_ + _).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    println(<span class="string">&quot;====================&quot;</span>)</span><br><span class="line"></span><br><span class="line">    mapRdd.aggregateByKey(<span class="number">0</span>)(_ + _, _ + _).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="缓存和检查点区别"><a href="#缓存和检查点区别" class="headerlink" title="缓存和检查点区别"></a>缓存和检查点区别</h5><ol>
<li>Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖；</li>
<li>Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高；</li>
<li>建议对 checkpoint() 的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Cache 缓存中读取数据即可，否则需要再从头计算一次 RDD。</li>
</ol>
<h4 id="RDD-分区器"><a href="#RDD-分区器" class="headerlink" title="RDD 分区器"></a>RDD 分区器</h4><p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。Hash 分区为当前的默认分区。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。</p>
<ul>
<li>只有 Key-Value 类型的 RDD 才有分区器，非 Key-Value 类型的 RDD 分区的值是 None；</li>
<li>每个 RDD 的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的。</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_RDD_Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Partitioner&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = sparkContext.makeRDD(<span class="type">Array</span>(</span><br><span class="line">      (<span class="string">&quot;湖北&quot;</span>, <span class="string">&quot;武汉&quot;</span>), (<span class="string">&quot;福建&quot;</span>, <span class="string">&quot;厦门&quot;</span>), (<span class="string">&quot;上海&quot;</span>, <span class="string">&quot;上海&quot;</span>),</span><br><span class="line">      (<span class="string">&quot;福建&quot;</span>, <span class="string">&quot;福州&quot;</span>), (<span class="string">&quot;福建&quot;</span>, <span class="string">&quot;三明&quot;</span>), (<span class="string">&quot;湖北&quot;</span>, <span class="string">&quot;十堰&quot;</span>)</span><br><span class="line">    ),<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    rdd.partitionBy(<span class="keyword">new</span> <span class="type">MyHashPartitioner</span>).saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 自定义分区器</span></span><br><span class="line"><span class="comment">   * 1. 继承 Partitioner</span></span><br><span class="line"><span class="comment">   * 2. 重写方法</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      key <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="string">&quot;福建&quot;</span> =&gt; <span class="number">0</span></span><br><span class="line">        <span class="keyword">case</span> <span class="string">&quot;湖北&quot;</span> =&gt; <span class="number">1</span></span><br><span class="line">        <span class="keyword">case</span> <span class="string">&quot;上海&quot;</span> =&gt; <span class="number">2</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="文件读取与保存"><a href="#文件读取与保存" class="headerlink" title="文件读取与保存"></a>文件读取与保存</h4><h5 id="text-文件"><a href="#text-文件" class="headerlink" title="text 文件"></a>text 文件</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 读取输入文件</span></span><br><span class="line"><span class="keyword">val</span> inputRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">inputRDD.saveAsTextFile(<span class="string">&quot;output&quot;</span>)</span><br></pre></td></tr></table></figure>

<h5 id="sequence-文件"><a href="#sequence-文件" class="headerlink" title="sequence 文件"></a>sequence 文件</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存数据为 SequenceFile</span></span><br><span class="line">dataRDD.saveAsSequenceFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"><span class="comment">// 读取 SequenceFile 文件</span></span><br><span class="line">sc.sequenceFile[<span class="type">Int</span>,<span class="type">Int</span>](<span class="string">&quot;output&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h5 id="object-对象"><a href="#object-对象" class="headerlink" title="object 对象"></a>object 对象</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存数据</span></span><br><span class="line">dataRDD.saveAsObjectFile(<span class="string">&quot;output&quot;</span>)</span><br><span class="line"><span class="comment">// 读取数据</span></span><br><span class="line">sc.objectFile[<span class="type">Int</span>](<span class="string">&quot;output&quot;</span>).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><h4 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h4><p>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge。</p>
<h4 id="基础编程-1"><a href="#基础编程-1" class="headerlink" title="基础编程"></a>基础编程</h4><h5 id="系统累加器"><a href="#系统累加器" class="headerlink" title="系统累加器"></a>系统累加器</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_Accumulator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Accumulator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取系统累加器</span></span><br><span class="line">    <span class="keyword">val</span> sumAcc: <span class="type">LongAccumulator</span> = sparkContext.longAccumulator(<span class="string">&quot;sum&quot;</span>)</span><br><span class="line"></span><br><span class="line">    rdd.foreach(sumAcc.add(_))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取累加器的值</span></span><br><span class="line">    println(sumAcc.value)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意细节：</p>
<ul>
<li>少加：转换算子中调用累加器，如果没用调用行动算子，那么变不会执行，导致累加器少加</li>
<li>多加：行动算子多次调用，则累加器多次被调用造成多加</li>
<li>一般情况下，累加器放在行动算子中进行操作</li>
</ul>
</blockquote>
<h5 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02_Accumulator_WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Accumulator&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="string">&quot;Hello Spark&quot;</span>, <span class="string">&quot;Hello World&quot;</span>, <span class="string">&quot;Hello Scala&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建累加器</span></span><br><span class="line">    <span class="keyword">val</span> wordCountAccumulator = <span class="keyword">new</span> <span class="type">WordCountAccumulator</span></span><br><span class="line">    <span class="comment">// 注册累加器到 sparkContext</span></span><br><span class="line">    sparkContext.register(wordCountAccumulator, <span class="string">&quot;wordCountAccumulator&quot;</span>)</span><br><span class="line"></span><br><span class="line">    rdd.flatMap(_.split(<span class="string">&quot; &quot;</span>)).foreach(wordCountAccumulator.add(_))</span><br><span class="line"></span><br><span class="line">    println(wordCountAccumulator.value)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">WordCountAccumulator</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>]] </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> wcMap: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>]()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断是否为初始状态</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">      wcMap.isEmpty</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 复制累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>]] = &#123;</span><br><span class="line">      <span class="keyword">val</span> copy = <span class="keyword">new</span> <span class="type">WordCountAccumulator</span>()</span><br><span class="line">      copy.wcMap = <span class="keyword">this</span>.wcMap</span><br><span class="line">      copy</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重置累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      wcMap.clear()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//  获取累加器要计算的值</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> newCount: <span class="type">Int</span> = wcMap.getOrElse(v, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">      wcMap.update(v, newCount)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Driver合并多个累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> map1: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = other.value</span><br><span class="line">      <span class="keyword">val</span> map2: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = <span class="keyword">this</span>.value</span><br><span class="line"></span><br><span class="line">      map1.foreach(</span><br><span class="line">        t =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> newCount: <span class="type">Int</span> = map2.getOrElse(t._1, <span class="number">0</span>) + t._2</span><br><span class="line">          map2.update(t._1, newCount)</span><br><span class="line">        &#125;</span><br><span class="line">      )</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 累加器结果</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = &#123;</span><br><span class="line">      wcMap</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><h4 id="实现原理-1"><a href="#实现原理-1" class="headerlink" title="实现原理"></a>实现原理</h4><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送。</p>
<p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220526155540.png" alt="image-20220526155535133"></p>
<h4 id="基础编程-2"><a href="#基础编程-2" class="headerlink" title="基础编程"></a>基础编程</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_Broadcast</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Broadcast&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sparkContext.makeRDD(<span class="type">List</span>(</span><br><span class="line">      (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>)</span><br><span class="line">    ))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> map: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>] = mutable.<span class="type">Map</span>(</span><br><span class="line">      (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">5</span>)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment">// 声明广播变量</span></span><br><span class="line">    <span class="keyword">val</span> broadcast: <span class="type">Broadcast</span>[mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>]] = sparkContext.broadcast(map)</span><br><span class="line"></span><br><span class="line">    rdd.map &#123;</span><br><span class="line">      <span class="keyword">case</span> (k, v) =&gt; &#123;</span><br><span class="line">        <span class="comment">// 使用广播变量</span></span><br><span class="line">        <span class="keyword">val</span> value: <span class="type">Int</span> = broadcast.value.getOrElse(k, <span class="number">0</span>)</span><br><span class="line">        (k, (v, value))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Spark-案例实操"><a href="#Spark-案例实操" class="headerlink" title="Spark 案例实操"></a>Spark 案例实操</h3><h4 id="数据准备-1"><a href="#数据准备-1" class="headerlink" title="数据准备"></a>数据准备</h4><p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220526165853.png" alt="image-20220526165850309"></p>
<p>上面的数据图是从数据文件中截取的一部分内容，表示为电商网站的用户行为数据，主要包含用户的 4 种行为：<strong>搜索，点击，下单，支付</strong>。数据规则如下：</p>
<ol>
<li>数据文件中每行数据采用下划线分隔数据</li>
<li>每一行数据表示用户的一次行为，这个行为只能是 4 种行为的一种</li>
<li>如果搜索关键字为 null,表示数据不是搜索数据</li>
<li>如果点击的品类 ID 和产品 ID 为-1，表示数据不是点击数据</li>
<li>针对于下单行为，一次可以下单多个商品，所以品类 ID 和产品 ID 可以是多个，id 之间采用逗号分隔，如果本次不是下单行为，则数据采用 null 表示</li>
<li>支付行为和下单行为类似</li>
</ol>
<h4 id="详细字段说明"><a href="#详细字段说明" class="headerlink" title="详细字段说明"></a>详细字段说明</h4><table>
<thead>
<tr>
<th>编码</th>
<th>字段名称</th>
<th>字段类型</th>
<th>字段含义</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>date</td>
<td>String</td>
<td>用户点击行为的日期</td>
</tr>
<tr>
<td>2</td>
<td>user_id</td>
<td>Long</td>
<td>用户的 ID</td>
</tr>
<tr>
<td>3</td>
<td>session_id</td>
<td>String</td>
<td>Session 的 ID</td>
</tr>
<tr>
<td>4</td>
<td>page_id</td>
<td>Long</td>
<td>某个页面的 ID</td>
</tr>
<tr>
<td>5</td>
<td>action_time</td>
<td>String</td>
<td>动作的时间点</td>
</tr>
<tr>
<td>6</td>
<td>search_key</td>
<td>String</td>
<td>用户搜索的关键词</td>
</tr>
<tr>
<td>7</td>
<td>click_category_id</td>
<td>Long</td>
<td>某一个商品品类的 ID</td>
</tr>
<tr>
<td>8</td>
<td>click_product_id</td>
<td>Long</td>
<td>某一个商品的 ID</td>
</tr>
<tr>
<td>9</td>
<td>order_category_ids</td>
<td>String</td>
<td>一次订单中所有品类的 ID 集合</td>
</tr>
<tr>
<td>10</td>
<td>order_product_ids</td>
<td>String</td>
<td>一次订单中所有商品的 ID 集合</td>
</tr>
<tr>
<td>11</td>
<td>pay_category_ids</td>
<td>String</td>
<td>一次支付中所有品类的 ID 集合</td>
</tr>
<tr>
<td>12</td>
<td>pay_product_ids</td>
<td>String</td>
<td>一次支付中所有商品的 ID 集合</td>
</tr>
<tr>
<td>13</td>
<td>city_id</td>
<td>Long</td>
<td>城市 ID</td>
</tr>
</tbody></table>
<h4 id="需求一-：Top10-热门品类"><a href="#需求一-：Top10-热门品类" class="headerlink" title="需求一 ：Top10 热门品类"></a>需求一 ：Top10 热门品类</h4><h5 id="需求说明"><a href="#需求说明" class="headerlink" title="需求说明"></a>需求说明</h5><p>先按照点击数排名，靠前的就排名高；如果点击数相同，再比较下单数；下单数再相同，就比较支付数</p>
<h5 id="实现一"><a href="#实现一" class="headerlink" title="实现一"></a>实现一</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark01_HotCategoryTop10</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Case&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 读取原始日志数据</span></span><br><span class="line">    <span class="keyword">val</span> actionRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data\\user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 统计品类的点击数量：（品类ID，点击数量）</span></span><br><span class="line">    <span class="keyword">val</span> clickActionRdd: <span class="type">RDD</span>[<span class="type">String</span>] = actionRDD.filter(_.split(<span class="string">&quot;_&quot;</span>)(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> clickCountRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = clickActionRdd.map(</span><br><span class="line">      action =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = action.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        (datas(<span class="number">6</span>), <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 统计品类的下单数量：（品类ID，下单数量）</span></span><br><span class="line">    <span class="keyword">val</span> orderActionRdd: <span class="type">RDD</span>[<span class="type">String</span>] = actionRDD.filter(_.split(<span class="string">&quot;_&quot;</span>)(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> orderCountRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = orderActionRdd.flatMap(</span><br><span class="line">      action =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = action.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> cidStr: <span class="type">String</span> = datas(<span class="number">8</span>)</span><br><span class="line">        <span class="keyword">val</span> cids: <span class="type">Array</span>[<span class="type">String</span>] = cidStr.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        cids.map((_, <span class="number">1</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 统计品类的支付数量：（品类ID，支付数量）</span></span><br><span class="line">    <span class="keyword">val</span> payActionRdd: <span class="type">RDD</span>[<span class="type">String</span>] = actionRDD.filter(_.split(<span class="string">&quot;_&quot;</span>)(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> payCountRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = payActionRdd.flatMap(</span><br><span class="line">      action =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = action.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> cidStr: <span class="type">String</span> = datas(<span class="number">10</span>)</span><br><span class="line">        <span class="keyword">val</span> cids: <span class="type">Array</span>[<span class="type">String</span>] = cidStr.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        cids.map((_, <span class="number">1</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 将品类进行排序</span></span><br><span class="line">    <span class="comment">//    元组排序：先比较第一个，再比较第二个，再比较第三，以此类推</span></span><br><span class="line">    <span class="comment">//    （品类ID,（点击数量，下单数量，支付数量））</span></span><br><span class="line">    <span class="keyword">val</span> cogroupRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = clickCountRdd.cogroup(orderCountRdd, payCountRdd)</span><br><span class="line"></span><br><span class="line">    cogroupRdd.mapValues &#123;</span><br><span class="line">      <span class="keyword">case</span> (clickIter, orderIter, payIter) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> clickCount: <span class="type">Int</span> = <span class="keyword">if</span> (clickIter.iterator.hasNext) clickIter.iterator.next() <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">val</span> orderCount: <span class="type">Int</span> = <span class="keyword">if</span> (orderIter.iterator.hasNext) orderIter.iterator.next() <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">val</span> payCount: <span class="type">Int</span> = <span class="keyword">if</span> (payIter.iterator.hasNext) payIter.iterator.next() <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        (clickCount, orderCount, payCount)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="实现二"><a href="#实现二" class="headerlink" title="实现二"></a>实现二</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark02_HotCategoryTop10</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Case&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 读取原始日志数据</span></span><br><span class="line">    <span class="keyword">val</span> actionRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data\\user_visit_action.txt&quot;</span>)</span><br><span class="line">    actionRDD.cache()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 统计品类的点击数量：（品类ID，点击数量）</span></span><br><span class="line">    <span class="keyword">val</span> clickActionRdd: <span class="type">RDD</span>[<span class="type">String</span>] = actionRDD.filter(_.split(<span class="string">&quot;_&quot;</span>)(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> clickCountRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = clickActionRdd.map(</span><br><span class="line">      action =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = action.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        (datas(<span class="number">6</span>), <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 统计品类的下单数量：（品类ID，下单数量）</span></span><br><span class="line">    <span class="keyword">val</span> orderActionRdd: <span class="type">RDD</span>[<span class="type">String</span>] = actionRDD.filter(_.split(<span class="string">&quot;_&quot;</span>)(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> orderCountRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = orderActionRdd.flatMap(</span><br><span class="line">      action =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = action.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> cidStr: <span class="type">String</span> = datas(<span class="number">8</span>)</span><br><span class="line">        <span class="keyword">val</span> cids: <span class="type">Array</span>[<span class="type">String</span>] = cidStr.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        cids.map((_, <span class="number">1</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 统计品类的支付数量：（品类ID，支付数量）</span></span><br><span class="line">    <span class="keyword">val</span> payActionRdd: <span class="type">RDD</span>[<span class="type">String</span>] = actionRDD.filter(_.split(<span class="string">&quot;_&quot;</span>)(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> payCountRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = payActionRdd.flatMap(</span><br><span class="line">      action =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> datas: <span class="type">Array</span>[<span class="type">String</span>] = action.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        <span class="keyword">val</span> cidStr: <span class="type">String</span> = datas(<span class="number">10</span>)</span><br><span class="line">        <span class="keyword">val</span> cids: <span class="type">Array</span>[<span class="type">String</span>] = cidStr.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">        cids.map((_, <span class="number">1</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> clickCountFormatRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = clickCountRdd.map(x =&gt; (x._1, (x._2, <span class="number">0</span>, <span class="number">0</span>)))</span><br><span class="line">    <span class="keyword">val</span> orderCountFormatRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = orderCountRdd.map(x =&gt; (x._1, (<span class="number">0</span>, x._2, <span class="number">0</span>)))</span><br><span class="line">    <span class="keyword">val</span> payCountFormatRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = payCountRdd.map(x =&gt; (x._1, (<span class="number">0</span>, <span class="number">0</span>, x._2)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5. 将品类进行排序</span></span><br><span class="line">    <span class="comment">//    元组排序：先比较第一个，再比较第二个，再比较第三，以此类推</span></span><br><span class="line">    <span class="comment">//    （品类ID,（点击数量，下单数量，支付数量））</span></span><br><span class="line">    clickCountFormatRdd.union(orderCountFormatRdd).union(payCountFormatRdd).reduceByKey(</span><br><span class="line">      (v1, v2) =&gt; &#123;</span><br><span class="line">        (v1._1 + v2._1, v1._2 + v2._2, v1._3 + v2._3)</span><br><span class="line">      &#125;</span><br><span class="line">    ).sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="实现三"><a href="#实现三" class="headerlink" title="实现三"></a>实现三</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_HotCategoryTop10</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Case&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 读取原始日志数据</span></span><br><span class="line">    <span class="keyword">val</span> actionRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data\\user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 将数据转换结构</span></span><br><span class="line">    <span class="comment">// 点击：（品类ID，（1，0，0））</span></span><br><span class="line">    <span class="comment">// 下单：（品类ID，（0，1，0））</span></span><br><span class="line">    <span class="comment">// 支付：（品类ID，（0，0，1））</span></span><br><span class="line">    <span class="keyword">val</span> flatmapRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = actionRDD.flatMap(</span><br><span class="line">      line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> (data(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &#123;</span><br><span class="line">          <span class="type">List</span>((data(<span class="number">6</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)))</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (data(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">          data(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>).map((_, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)))</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (data(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">          data(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>).map((_, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="type">Nil</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 将相同的品类ID数据进行分组聚合</span></span><br><span class="line">    <span class="keyword">val</span> analysisRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = flatmapRdd.reduceByKey(</span><br><span class="line">      (v1, v2) =&gt; &#123;</span><br><span class="line">        (v1._1 + v2._1, v1._2 + v2._2, v1._3 + v2._3)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 将统计数据排序并打印</span></span><br><span class="line">    analysisRdd.sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="实现四"><a href="#实现四" class="headerlink" title="实现四"></a>实现四</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark04_HotCategoryTop10</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Case&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. 读取原始日志数据</span></span><br><span class="line">    <span class="keyword">val</span> actionRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data\\user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 声明累加器</span></span><br><span class="line">    <span class="keyword">val</span> accumulator = <span class="keyword">new</span> <span class="type">HotCategoryAccumulator</span></span><br><span class="line">    sparkContext.register(accumulator, <span class="string">&quot;hotCategoryAccumulator&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 遍历累加</span></span><br><span class="line">    actionRDD.foreach(</span><br><span class="line">      line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> (data(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &#123;</span><br><span class="line">          accumulator.add(data(<span class="number">6</span>), <span class="string">&quot;click&quot;</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (data(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">          data(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>).foreach(</span><br><span class="line">            accumulator.add(_, <span class="string">&quot;order&quot;</span>)</span><br><span class="line">          )</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (data(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">          data(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>).foreach(</span><br><span class="line">            accumulator.add(_, <span class="string">&quot;pay&quot;</span>)</span><br><span class="line">          )</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. 使用累加器</span></span><br><span class="line">    <span class="keyword">val</span> accVal: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>] = accumulator.value</span><br><span class="line">    <span class="keyword">val</span> list: <span class="type">List</span>[<span class="type">HotCategory</span>] = accVal.map(_._2).toList</span><br><span class="line"></span><br><span class="line">    list.sortWith(</span><br><span class="line">      (left, right) =&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (left.clickCnt &gt; right.clickCnt) &#123;</span><br><span class="line">          <span class="literal">true</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (left.clickCnt == right.clickCnt &amp;&amp; left.orderCnt &gt; right.orderCnt) &#123;</span><br><span class="line">          <span class="literal">true</span></span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (left.clickCnt == right.clickCnt &amp;&amp; left.orderCnt == right.orderCnt &amp;&amp; left.payCnt &gt; right.payCnt) &#123;</span><br><span class="line">          <span class="literal">true</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ).take(<span class="number">10</span>).foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">HotCategory</span>(<span class="params">cid: <span class="type">String</span>, var clickCnt: <span class="type">Int</span>, var orderCnt: <span class="type">Int</span>, var payCnt: <span class="type">Int</span></span>)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * 自定义累加器</span></span><br><span class="line"><span class="comment">   * 1. 继承 AccumulatorV2，定义泛型</span></span><br><span class="line"><span class="comment">   * IN：（品类ID，行为类型）</span></span><br><span class="line"><span class="comment">   * OUT：mutable.Map[String, HotCategory]</span></span><br><span class="line"><span class="comment">   * 2. 重写方法</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">HotCategoryAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[(<span class="type">String</span>, <span class="type">String</span>), mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> hcMap = mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">      hcMap.isEmpty</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[(<span class="type">String</span>, <span class="type">String</span>), mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]] = &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">HotCategoryAccumulator</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">      hcMap.clear()</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: (<span class="type">String</span>, <span class="type">String</span>)): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> cid: <span class="type">String</span> = v._1</span><br><span class="line">      <span class="keyword">val</span> actionType: <span class="type">String</span> = v._2</span><br><span class="line">      <span class="keyword">val</span> category: <span class="type">HotCategory</span> = hcMap.getOrElse(cid, <span class="keyword">new</span> <span class="type">HotCategory</span>(cid, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">      <span class="keyword">if</span> (actionType == <span class="string">&quot;click&quot;</span>) &#123;</span><br><span class="line">        category.clickCnt += <span class="number">1</span>;</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (actionType == <span class="string">&quot;order&quot;</span>) &#123;</span><br><span class="line">        category.orderCnt += <span class="number">1</span>;</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (actionType == <span class="string">&quot;pay&quot;</span>) &#123;</span><br><span class="line">        category.payCnt += <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      hcMap.update(cid, category)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[(<span class="type">String</span>, <span class="type">String</span>), mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="keyword">val</span> map1: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>] = <span class="keyword">this</span>.value</span><br><span class="line">      <span class="keyword">val</span> map2: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>] = other.value</span><br><span class="line"></span><br><span class="line">      map2.foreach &#123;</span><br><span class="line">        <span class="keyword">case</span> (cid, hc) =&gt; &#123;</span><br><span class="line">          <span class="keyword">val</span> category: <span class="type">HotCategory</span> = map1.getOrElse(cid, <span class="keyword">new</span> <span class="type">HotCategory</span>(cid, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">          category.clickCnt += hc.clickCnt</span><br><span class="line">          category.orderCnt += hc.orderCnt</span><br><span class="line">          category.payCnt += hc.payCnt</span><br><span class="line">          map1.update(cid, category)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">HotCategory</span>] = hcMap</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="需求二：Top10-热门品类中每个品类的-的-Top10-活跃-Session-统计"><a href="#需求二：Top10-热门品类中每个品类的-的-Top10-活跃-Session-统计" class="headerlink" title="需求二：Top10 热门品类中每个品类的 的 Top10 活跃 Session 统计"></a>需求二：Top10 热门品类中每个品类的 的 Top10 活跃 Session 统计</h4><h5 id="需求说明-1"><a href="#需求说明-1" class="headerlink" title="需求说明"></a>需求说明</h5><p>在需求一的基础上，增加每个品类用户 session 的点击统计</p>
<h5 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark05_HotCategoryAndSessionTop10</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Case&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.读取原始日志数据</span></span><br><span class="line">    <span class="keyword">val</span> actionRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data\\user_visit_action.txt&quot;</span>)</span><br><span class="line">    actionRDD.cache()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 获取热门品类Top10的ID</span></span><br><span class="line">    <span class="keyword">val</span> ids: <span class="type">Array</span>[<span class="type">String</span>] = hotCategoryIDsTop10(actionRDD)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.过滤原始数据保留前10品类的点击数据</span></span><br><span class="line">    <span class="keyword">val</span> filterActionRdd: <span class="type">RDD</span>[<span class="type">String</span>] = actionRDD.filter(</span><br><span class="line">      line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        ids.contains(data(<span class="number">6</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4.根据品类ID和session进行点击量统计</span></span><br><span class="line">    <span class="keyword">val</span> reduceRdd: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = filterActionRdd.map(</span><br><span class="line">      line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        ((data(<span class="number">6</span>), data(<span class="number">2</span>)), <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 5.将统计结果结构转换</span></span><br><span class="line">    <span class="keyword">val</span> mapRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = reduceRdd.map &#123;</span><br><span class="line">      <span class="keyword">case</span> ((cid, sid), sum) =&gt; &#123;</span><br><span class="line">        (cid, (sid, sum))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 6.相同品类分组</span></span><br><span class="line">    <span class="keyword">val</span> groupRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = mapRdd.groupByKey()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 7.将分组后的数据取前10</span></span><br><span class="line">    <span class="keyword">val</span> resultRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = groupRdd.mapValues(</span><br><span class="line">      iter =&gt; &#123;</span><br><span class="line">        iter.toList.sortBy(_._2)(<span class="type">Ordering</span>.<span class="type">Int</span>.reverse).take(<span class="number">10</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    resultRdd.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">hotCategoryIDsTop10</span></span>(actionRDD: <span class="type">RDD</span>[<span class="type">String</span>]) = &#123;</span><br><span class="line">    <span class="keyword">val</span> flatmapRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = actionRDD.flatMap(</span><br><span class="line">      line =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data: <span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> (data(<span class="number">6</span>) != <span class="string">&quot;-1&quot;</span>) &#123;</span><br><span class="line">          <span class="type">List</span>((data(<span class="number">6</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>)))</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (data(<span class="number">8</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">          data(<span class="number">8</span>).split(<span class="string">&quot;,&quot;</span>).map((_, (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)))</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (data(<span class="number">10</span>) != <span class="string">&quot;null&quot;</span>) &#123;</span><br><span class="line">          data(<span class="number">10</span>).split(<span class="string">&quot;,&quot;</span>).map((_, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="type">Nil</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> analysisRdd: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>, <span class="type">Int</span>))] = flatmapRdd.reduceByKey(</span><br><span class="line">      (v1, v2) =&gt; &#123;</span><br><span class="line">        (v1._1 + v2._1, v1._2 + v2._2, v1._3 + v2._3)</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    analysisRdd.sortBy(_._2, <span class="literal">false</span>).take(<span class="number">10</span>).map(_._1)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="需求三：页面单跳转换率统计"><a href="#需求三：页面单跳转换率统计" class="headerlink" title="需求三：页面单跳转换率统计"></a>需求三：页面单跳转换率统计</h4><h5 id="需求说明-2"><a href="#需求说明-2" class="headerlink" title="需求说明"></a>需求说明</h5><p>计算页面单跳转化率，什么是页面单跳转换率，比如一个用户在一次 Session 过程中访问的页面路径 3,5,7,9,10,21，那么页面 3 跳到页面 5 叫一次单跳，7-9 也叫一次单跳，那么单跳转化率就是要统计页面点击的概率。<br>比如：计算 3-5 的单跳转化率，先获取符合条件的 Session 对于页面 3 的访问次数（PV）为 A，然后获取符合条件的 Session 中访问了页面 3 又紧接着访问了页面 5 的次数为 B，那么 B/A 就是 3-5 的页面单跳转化率。</p>
<h5 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h5><p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220527005735.png"></p>
<h5 id="需求实现"><a href="#需求实现" class="headerlink" title="需求实现"></a>需求实现</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark07_PageFlowAnalysis_Filter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;Case&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> actionRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data\\user_visit_action.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> actionDataRdd: <span class="type">RDD</span>[<span class="type">UserVisitAction</span>] = actionRDD.map(</span><br><span class="line">      castToUserVisitAction(_)</span><br><span class="line">    )</span><br><span class="line">    actionDataRdd.cache()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 对指定页面连续跳转进行统计</span></span><br><span class="line">    <span class="comment">// 1-2,2-3,3-4,4-5,5-6,6-7</span></span><br><span class="line">    <span class="keyword">val</span> ids: <span class="type">List</span>[<span class="type">Long</span>] = <span class="type">List</span>[<span class="type">Long</span>](<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>)</span><br><span class="line">    <span class="keyword">val</span> wantFlowIds: <span class="type">List</span>[(<span class="type">Long</span>, <span class="type">Long</span>)] = ids.zip(ids.tail)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 计算分母</span></span><br><span class="line">    <span class="keyword">val</span> pageIdToCountMap: <span class="type">Map</span>[<span class="type">Long</span>, <span class="type">Int</span>] = actionDataRdd.filter(</span><br><span class="line">      action =&gt; &#123;</span><br><span class="line">        ids.init.contains(action.page_id)</span><br><span class="line">      &#125;</span><br><span class="line">    ).map(</span><br><span class="line">      action =&gt; &#123;</span><br><span class="line">        (action.page_id, <span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    ).reduceByKey(_ + _).collect().toMap</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 计算分子</span></span><br><span class="line">    <span class="comment">// 根据session进行分组</span></span><br><span class="line">    <span class="keyword">val</span> sessionRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">UserVisitAction</span>])] = actionDataRdd.groupBy(_.session_id)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分组后，根据访问时间从小到大排序</span></span><br><span class="line">    <span class="keyword">val</span> mvRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">List</span>[((<span class="type">Long</span>, <span class="type">Long</span>), <span class="type">Int</span>)])] = sessionRdd.mapValues(</span><br><span class="line">      iter =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> sortList: <span class="type">List</span>[<span class="type">UserVisitAction</span>] = iter.toList.sortBy(_.action_time)</span><br><span class="line">        <span class="keyword">val</span> flowIds: <span class="type">List</span>[<span class="type">Long</span>] = sortList.map(_.page_id)</span><br><span class="line">        <span class="comment">// Sliding：滑窗</span></span><br><span class="line">        <span class="comment">// Zip：拉链</span></span><br><span class="line">        <span class="keyword">val</span> pageFlowIds: <span class="type">List</span>[(<span class="type">Long</span>, <span class="type">Long</span>)] = flowIds.zip(flowIds.tail)</span><br><span class="line">        <span class="comment">// 将不合法的页面过滤</span></span><br><span class="line">        pageFlowIds.filter(</span><br><span class="line">          tuple =&gt; &#123;</span><br><span class="line">            wantFlowIds.contains(tuple)</span><br><span class="line">          &#125;</span><br><span class="line">        ).map(</span><br><span class="line">          tuple =&gt; &#123;</span><br><span class="line">            (tuple, <span class="number">1</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        )</span><br><span class="line">      &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataRdd: <span class="type">RDD</span>[((<span class="type">Long</span>, <span class="type">Long</span>), <span class="type">Int</span>)] = mvRdd.map(_._2).flatMap(list =&gt; list).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// TODO 计算单调转换率</span></span><br><span class="line">    dataRdd.foreach &#123;</span><br><span class="line">      <span class="keyword">case</span> ((pageId1, pageId2), sum) =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> denominator: <span class="type">Int</span> = pageIdToCountMap.get(pageId1).get</span><br><span class="line">        println(<span class="string">s&quot;页面【<span class="subst">$&#123;pageId1&#125;</span>】到页面【<span class="subst">$&#123;pageId2&#125;</span>】单跳转换率为：&quot;</span> + sum.toDouble / denominator)</span><br><span class="line">        denominator</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">castToUserVisitAction</span></span>(str: <span class="type">String</span>): <span class="type">UserVisitAction</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> data: <span class="type">Array</span>[<span class="type">String</span>] = str.split(<span class="string">&quot;_&quot;</span>)</span><br><span class="line">    <span class="type">UserVisitAction</span>(</span><br><span class="line">      data(<span class="number">0</span>),</span><br><span class="line">      data(<span class="number">1</span>).toLong,</span><br><span class="line">      data(<span class="number">2</span>),</span><br><span class="line">      data(<span class="number">3</span>).toLong,</span><br><span class="line">      data(<span class="number">4</span>),</span><br><span class="line">      data(<span class="number">5</span>),</span><br><span class="line">      data(<span class="number">6</span>).toLong,</span><br><span class="line">      data(<span class="number">7</span>).toLong,</span><br><span class="line">      data(<span class="number">8</span>),</span><br><span class="line">      data(<span class="number">9</span>),</span><br><span class="line">      data(<span class="number">10</span>),</span><br><span class="line">      data(<span class="number">11</span>),</span><br><span class="line">      data(<span class="number">12</span>).toLong</span><br><span class="line">    )</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//用户访问动作表</span></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">UserVisitAction</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="class">                              date: <span class="type">String</span>, //用户点击行为的日期</span></span></span><br><span class="line"><span class="params"><span class="class">                              user_id: <span class="type">Long</span>, //用户的 <span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                              session_id: <span class="type">String</span>, //<span class="type">Session</span> 的 <span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                              page_id: <span class="type">Long</span>, //某个页面的 <span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                              action_time: <span class="type">String</span>, //动作的时间点</span></span></span><br><span class="line"><span class="params"><span class="class">                              search_keyword: <span class="type">String</span>, //用户搜索的关键词</span></span></span><br><span class="line"><span class="params"><span class="class">                              click_category_id: <span class="type">Long</span>, //某一个商品品类的 <span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                              click_product_id: <span class="type">Long</span>, //某一个商品的 <span class="type">ID</span></span></span></span><br><span class="line"><span class="params"><span class="class">                              order_category_ids: <span class="type">String</span>, //一次订单中所有品类的 <span class="type">ID</span> 集合</span></span></span><br><span class="line"><span class="params"><span class="class">                              order_product_ids: <span class="type">String</span>, //一次订单中所有商品的 <span class="type">ID</span> 集合</span></span></span><br><span class="line"><span class="params"><span class="class">                              pay_category_ids: <span class="type">String</span>, //一次支付中所有品类的 <span class="type">ID</span> 集合</span></span></span><br><span class="line"><span class="params"><span class="class">                              pay_product_ids: <span class="type">String</span>, //一次支付中所有商品的 <span class="type">ID</span> 集合</span></span></span><br><span class="line"><span class="params"><span class="class">                              city_id: <span class="type">Long</span> //城市 id</span></span></span><br><span class="line"><span class="params"><span class="class">                            </span>)</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="工程化代码"><a href="#工程化代码" class="headerlink" title="工程化代码"></a>工程化代码</h4><p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220527103550.png" alt="image-20220527103546600"></p>
<h5 id="application"><a href="#application" class="headerlink" title="application"></a>application</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.eitan.bigdata.spark.core.framework.application</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.eitan.bigdata.spark.core.framework.common.<span class="type">TApplication</span></span><br><span class="line"><span class="keyword">import</span> com.eitan.bigdata.spark.core.framework.controller.<span class="type">WordCountController</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountApplication</span> <span class="keyword">extends</span> <span class="title">App</span> <span class="keyword">with</span> <span class="title">TApplication</span> </span>&#123;</span><br><span class="line">  start() &#123;</span><br><span class="line">    <span class="keyword">val</span> controller = <span class="keyword">new</span> <span class="type">WordCountController</span></span><br><span class="line">    controller.dispatch()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="common"><a href="#common" class="headerlink" title="common"></a>common</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.eitan.bigdata.spark.core.framework.common</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.eitan.bigdata.spark.core.framework.util.<span class="type">EnvUtil</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TApplication</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(master: <span class="type">String</span> = <span class="string">&quot;local[*]&quot;</span>, app: <span class="type">String</span> = <span class="string">&quot;Application&quot;</span>)(op: =&gt; <span class="type">Unit</span>) = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(master).setAppName(app)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="type">EnvUtil</span>.put(sc)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      op</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> ex =&gt; println(ex.getMessage)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">    <span class="type">EnvUtil</span>.clear()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.eitan.bigdata.spark.core.framework.common</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TController</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dispatch</span></span>(): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.eitan.bigdata.spark.core.framework.common</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.eitan.bigdata.spark.core.framework.util.<span class="type">EnvUtil</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TDao</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">readFile</span></span>(path: <span class="type">String</span>) = &#123;</span><br><span class="line">    <span class="type">EnvUtil</span>.take().textFile(path)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.eitan.bigdata.spark.core.framework.common</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">trait</span> <span class="title">TService</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataAnalysis</span></span>(): <span class="type">Any</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="controller"><a href="#controller" class="headerlink" title="controller"></a>controller</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.eitan.bigdata.spark.core.framework.controller</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.eitan.bigdata.spark.core.framework.common.<span class="type">TController</span></span><br><span class="line"><span class="keyword">import</span> com.eitan.bigdata.spark.core.framework.service.<span class="type">WordCountService</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 控制层</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCountController</span> <span class="keyword">extends</span> <span class="title">TController</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> wordCountService = <span class="keyword">new</span> <span class="type">WordCountService</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dispatch</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> array = wordCountService.dataAnalysis()</span><br><span class="line">    array.foreach(println)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="dao"><a href="#dao" class="headerlink" title="dao"></a>dao</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.eitan.bigdata.spark.core.framework.dao</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.eitan.bigdata.spark.core.framework.common.<span class="type">TDao</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 持久层</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCountDao</span> <span class="keyword">extends</span> <span class="title">TDao</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="service"><a href="#service" class="headerlink" title="service"></a>service</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.eitan.bigdata.spark.core.framework.service</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.eitan.bigdata.spark.core.framework.common.<span class="type">TService</span></span><br><span class="line"><span class="keyword">import</span> com.eitan.bigdata.spark.core.framework.dao.<span class="type">WordCountDao</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 服务层</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCountService</span> <span class="keyword">extends</span> <span class="title">TService</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> wordCountDao = <span class="keyword">new</span> <span class="type">WordCountDao</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 数据分析</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dataAnalysis</span></span>() = &#123;</span><br><span class="line">    <span class="comment">// 读取文件数据</span></span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = wordCountDao.readFile(<span class="string">&quot;data\\1.txt,data\\2.txt&quot;</span>)</span><br><span class="line">    <span class="comment">// 将文件中的数据进行分词</span></span><br><span class="line">    <span class="keyword">val</span> words: <span class="type">RDD</span>[<span class="type">String</span>] = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordToOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.map(</span><br><span class="line">      word =&gt; (word, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Spark 框架提供了更多功能，可以将分组和聚合使用一个方法实现</span></span><br><span class="line">    <span class="comment">// reduceByKey: 相同Key的数据，可以对value进行reduce聚合</span></span><br><span class="line">    <span class="keyword">val</span> wordToCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将转换结果采集到控制台打印出来</span></span><br><span class="line">    wordToCount.collect()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="util"><a href="#util" class="headerlink" title="util"></a>util</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.eitan.bigdata.spark.core.framework.util</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">EnvUtil</span> </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> scLocal = <span class="keyword">new</span> <span class="type">ThreadLocal</span>[<span class="type">SparkContext</span>]</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">put</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    scLocal.set(sc)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(): <span class="type">SparkContext</span> = &#123;</span><br><span class="line">    scLocal.get()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">clear</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    scLocal.remove()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Eitan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/2022/05/22/Spark%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9ASparkCore/" title="Spark（二）：SparkCore">http://example.com/2022/05/22/Spark（二）：SparkCore/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"><i class="fa fa-tag"></i> spark</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/05/20/Hadoop%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9AHive/" rel="prev" title="Hadoop（四）：Hive">
                  <i class="fa fa-chevron-left"></i> Hadoop（四）：Hive
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/05/28/Spark%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9ASparkSQL/" rel="next" title="Spark（三）：SparkSQL">
                  Spark（三）：SparkSQL <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Eitan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  





</body>
</html>
