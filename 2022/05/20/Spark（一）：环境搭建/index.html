<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;example.com&quot;,&quot;root&quot;:&quot;&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;images&quot;,&quot;scheme&quot;:&quot;Muse&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;right&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:true,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:true,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;},&quot;path&quot;:&quot;&#x2F;search.xml&quot;,&quot;localsearch&quot;:{&quot;enable&quot;:true,&quot;trigger&quot;:&quot;auto&quot;,&quot;top_n_per_article&quot;:1,&quot;unescape&quot;:false,&quot;preload&quot;:false}}</script>
<meta name="description" content="本文为学习笔记，对应视频教程来自尚硅谷大数据Spark教程从入门到精通 Spark快速上手创建Maven项目增加Scala插件Spark 由 Scala 语言开发的，我这里使用的 Scala 编译版本为 2.12.15。请通过官网查看 Spark 和 Scala 对应的版本关系Spark Documentation。 WordCount1234567891011121314151617181920">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark（一）：环境搭建">
<meta property="og:url" content="http://example.com/2022/05/20/Spark%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/index.html">
<meta property="og:site_name" content="Eitan&#39;s Blog">
<meta property="og:description" content="本文为学习笔记，对应视频教程来自尚硅谷大数据Spark教程从入门到精通 Spark快速上手创建Maven项目增加Scala插件Spark 由 Scala 语言开发的，我这里使用的 Scala 编译版本为 2.12.15。请通过官网查看 Spark 和 Scala 对应的版本关系Spark Documentation。 WordCount1234567891011121314151617181920">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220521170721.png">
<meta property="og:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220521172629.png">
<meta property="article:published_time" content="2022-05-20T01:44:41.145Z">
<meta property="article:modified_time" content="2022-05-21T17:52:20.849Z">
<meta property="article:author" content="Eitan">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220521170721.png">


<link rel="canonical" href="http://example.com/2022/05/20/Spark%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;http:&#x2F;&#x2F;example.com&#x2F;2022&#x2F;05&#x2F;20&#x2F;Spark%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA&#x2F;&quot;,&quot;path&quot;:&quot;2022&#x2F;05&#x2F;20&#x2F;Spark（一）：环境搭建&#x2F;&quot;,&quot;title&quot;:&quot;Spark（一）：环境搭建&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>Spark（一）：环境搭建 | Eitan's Blog</title><script src="/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">
    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Eitan's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B"><span class="nav-number">1.</span> <span class="nav-text">Spark快速上手</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%88%9B%E5%BB%BAMaven%E9%A1%B9%E7%9B%AE"><span class="nav-number">1.1.</span> <span class="nav-text">创建Maven项目</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A2%9E%E5%8A%A0Scala%E6%8F%92%E4%BB%B6"><span class="nav-number">1.1.1.</span> <span class="nav-text">增加Scala插件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#WordCount"><span class="nav-number">1.1.2.</span> <span class="nav-text">WordCount</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#log4j-properties"><span class="nav-number">1.1.3.</span> <span class="nav-text">log4j.properties</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Spark%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83"><span class="nav-number">2.</span> <span class="nav-text">Spark运行环境</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Local%E6%A8%A1%E5%BC%8F"><span class="nav-number">2.1.</span> <span class="nav-text">Local模式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%89%E8%A3%85JDK"><span class="nav-number">2.1.1.</span> <span class="nav-text">安装JDK</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%B8%8B%E8%BD%BD%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6%E5%B9%B6%E8%A7%A3%E5%8E%8B"><span class="nav-number">2.1.2.</span> <span class="nav-text">下载压缩文件并解压</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8Local%E7%8E%AF%E5%A2%83"><span class="nav-number">2.1.3.</span> <span class="nav-text">启动Local环境</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7"><span class="nav-number">2.1.4.</span> <span class="nav-text">命令行工具</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8F%90%E4%BA%A4%E5%BA%94%E7%94%A8"><span class="nav-number">2.1.5.</span> <span class="nav-text">提交应用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8F%90%E4%BA%A4%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="nav-number">2.1.6.</span> <span class="nav-text">提交参数说明</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Standalone%E6%A8%A1%E5%BC%8F"><span class="nav-number">2.2.</span> <span class="nav-text">Standalone模式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92"><span class="nav-number">2.2.1.</span> <span class="nav-text">集群规划</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6"><span class="nav-number">2.2.2.</span> <span class="nav-text">解压缩文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">2.2.3.</span> <span class="nav-text">修改配置文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="nav-number">2.2.4.</span> <span class="nav-text">启动集群</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1"><span class="nav-number">2.2.5.</span> <span class="nav-text">配置历史服务</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#hadoop%E9%85%8D%E7%BD%AE"><span class="nav-number">2.2.5.1.</span> <span class="nav-text">hadoop配置</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#spark%E9%85%8D%E7%BD%AE"><span class="nav-number">2.2.5.2.</span> <span class="nav-text">spark配置</span></a></li></ol></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Standalone%E9%85%8D%E7%BD%AE%E9%AB%98%E5%8F%AF%E7%94%A8%EF%BC%88HA%EF%BC%89"><span class="nav-number">2.3.</span> <span class="nav-text">Standalone配置高可用（HA）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92-1"><span class="nav-number">2.3.1.</span> <span class="nav-text">集群规划</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E5%8E%8B%E5%AE%89%E8%A3%85Zookeeper"><span class="nav-number">2.3.2.</span> <span class="nav-text">解压安装Zookeeper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEZookeeper"><span class="nav-number">2.3.3.</span> <span class="nav-text">配置Zookeeper</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Zookeeper%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2%E8%84%9A%E6%9C%AC"><span class="nav-number">2.3.4.</span> <span class="nav-text">Zookeeper集群启动停止脚本</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9-spark-env-sh-%E6%96%87%E4%BB%B6%E6%B7%BB%E5%8A%A0%E5%A6%82%E4%B8%8B%E9%85%8D%E7%BD%AE"><span class="nav-number">2.3.5.</span> <span class="nav-text">修改 spark-env.sh 文件添加如下配置</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4-1"><span class="nav-number">2.3.6.</span> <span class="nav-text">启动集群</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9C%A8-spark151-%E4%B8%8A%E5%90%AF%E5%8A%A8%E5%A4%87%E7%94%A8-Master"><span class="nav-number">2.3.7.</span> <span class="nav-text">在 spark151 上启动备用 Master</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8F%90%E4%BA%A4%E5%BA%94%E7%94%A8%E5%88%B0%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4"><span class="nav-number">2.3.8.</span> <span class="nav-text">提交应用到高可用集群</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%81%9C%E6%AD%A2-spark152-%E7%9A%84-Master-%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E8%BF%9B%E7%A8%8B"><span class="nav-number">2.3.9.</span> <span class="nav-text">停止 spark152 的 Master 资源监控进程</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Spark-On-Yarn%E6%A8%A1%E5%BC%8F"><span class="nav-number">2.4.</span> <span class="nav-text">Spark-On-Yarn模式</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%8E%9F%E7%90%86"><span class="nav-number">2.4.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%A7%A3%E5%8E%8B%E7%BC%A9%E6%96%87%E4%BB%B6-1"><span class="nav-number">2.4.2.</span> <span class="nav-text">解压缩文件</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%85%8D%E7%BD%AEyarn-site-xml"><span class="nav-number">2.4.3.</span> <span class="nav-text">配置yarn-site.xml</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%85%8D%E7%BD%AESpark%E7%9A%84%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%92%8CYarn%E7%9A%84%E6%95%B4%E5%90%88"><span class="nav-number">2.4.4.</span> <span class="nav-text">配置Spark的历史服务器和Yarn的整合</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E6%97%A5%E5%BF%97%E7%BA%A7%E5%88%AB"><span class="nav-number">2.4.5.</span> <span class="nav-text">修改日志级别</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE%E4%BE%9D%E8%B5%96%E7%9A%84-Spark-%E7%9A%84jar%E5%8C%85"><span class="nav-number">2.4.6.</span> <span class="nav-text">配置依赖的 Spark 的jar包</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8%E6%9C%8D%E5%8A%A1"><span class="nav-number">2.4.7.</span> <span class="nav-text">启动服务</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8F%90%E4%BA%A4%E5%BA%94%E7%94%A8-1"><span class="nav-number">2.4.8.</span> <span class="nav-text">提交应用</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%90%AF%E5%8A%A8-Spark-%E7%9A%84%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="nav-number">2.4.9.</span> <span class="nav-text">启动 Spark 的历史服务器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%85%8D%E7%BD%AE-MapReduce-%E7%9A%84%E5%8E%86%E5%8F%B2%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="nav-number">2.4.10.</span> <span class="nav-text">配置 MapReduce 的历史服务器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E9%87%8D%E5%90%AF%E6%89%80%E6%9C%89%E6%9C%8D%E5%8A%A1"><span class="nav-number">2.4.11.</span> <span class="nav-text">重启所有服务</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94"><span class="nav-number">2.5.</span> <span class="nav-text">部署模式对比</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AB%AF%E5%8F%A3%E5%8F%B7"><span class="nav-number">2.6.</span> <span class="nav-text">端口号</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Eitan"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">Eitan</p>
  <div class="site-description" itemprop="description">blog用于记忆，大脑擅长思考</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/eitan-blog" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;eitan-blog" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:eitan_blog@163.com" title="E-Mail → mailto:eitan_blog@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>




        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/yourname" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/20/Spark%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Eitan">
      <meta itemprop="description" content="blog用于记忆，大脑擅长思考">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Eitan's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark（一）：环境搭建
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-20 09:44:41" itemprop="dateCreated datePublished" datetime="2022-05-20T09:44:41+08:00">2022-05-20</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2022-05-22 01:52:20" itemprop="dateModified" datetime="2022-05-22T01:52:20+08:00">2022-05-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a>
        </span>
    </span>

  
      </div>
      <div class="post-meta">
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本文为学习笔记，对应视频教程来自<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV11A411L7CK">尚硅谷大数据Spark教程从入门到精通</a></p>
<h3 id="Spark快速上手"><a href="#Spark快速上手" class="headerlink" title="Spark快速上手"></a>Spark快速上手</h3><h4 id="创建Maven项目"><a href="#创建Maven项目" class="headerlink" title="创建Maven项目"></a>创建Maven项目</h4><h5 id="增加Scala插件"><a href="#增加Scala插件" class="headerlink" title="增加Scala插件"></a>增加Scala插件</h5><p>Spark 由 Scala 语言开发的，我这里使用的 Scala 编译版本为 2.12.15。请通过官网查看 Spark 和 Scala 对应的版本关系<a target="_blank" rel="noopener" href="https://spark.apache.org/docs/latest/">Spark Documentation</a>。</p>
<h5 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h5><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.eitan.bigdata.spark.core.wordcount</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Spark03_WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 创建 Spark 运行配置对象</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local&quot;</span>).setAppName(<span class="string">&quot;WordCount&quot;</span>)</span><br><span class="line">    <span class="comment">// 创建 Spark 上下文环境对象（连接对象）</span></span><br><span class="line">    <span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="comment">// 读取文件数据</span></span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">RDD</span>[<span class="type">String</span>] = sparkContext.textFile(<span class="string">&quot;data\\1.txt,data\\2.txt&quot;</span>)</span><br><span class="line">    <span class="comment">// 将文件中的数据进行分词</span></span><br><span class="line">    <span class="keyword">val</span> words: <span class="type">RDD</span>[<span class="type">String</span>] = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordToOne: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = words.map(</span><br><span class="line">      word =&gt; (word, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Spark 框架提供了更多功能，可以将分组和聚合使用一个方法实现</span></span><br><span class="line">    <span class="comment">// reduceByKey: 相同Key的数据，可以对value进行reduce聚合</span></span><br><span class="line">    <span class="keyword">val</span> wordToCount: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToOne.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将转换结果采集到控制台打印出来</span></span><br><span class="line">    <span class="keyword">val</span> array: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordToCount.collect()</span><br><span class="line">    array.foreach(println)</span><br><span class="line">    <span class="comment">//关闭 Spark 连接</span></span><br><span class="line">    sparkContext.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h5 id="log4j-properties"><a href="#log4j-properties" class="headerlink" title="log4j.properties"></a>log4j.properties</h5><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">log4j.rootCategory</span>=<span class="string">ERROR, console</span></span><br><span class="line"><span class="meta">log4j.appender.console</span>=<span class="string">org.apache.log4j.ConsoleAppender</span></span><br><span class="line"><span class="meta">log4j.appender.console.target</span>=<span class="string">System.err</span></span><br><span class="line"><span class="meta">log4j.appender.console.layout</span>=<span class="string">org.apache.log4j.PatternLayout</span></span><br><span class="line"><span class="meta">log4j.appender.console.layout.ConversionPattern</span>=<span class="string">%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n</span></span><br><span class="line"><span class="comment"># Set the default spark-shell log level to ERROR. When running the spark-shell,the</span></span><br><span class="line"><span class="comment"># log level for this class is used to overwrite the root logger&#x27;s log level, so that</span></span><br><span class="line"><span class="comment"># the user can have different defaults for the shell and regular Spark apps.</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.spark.repl.Main</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="comment"># Settings to quiet third party logs that are too verbose</span></span><br><span class="line"><span class="meta">log4j.logger.org.spark_project.jetty</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.parquet</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="meta">log4j.logger.parquet</span>=<span class="string">ERROR</span></span><br><span class="line"><span class="comment"># SPARK-9183: Settings to avoid annoying messages when looking up nonexistent</span></span><br><span class="line"><span class="comment"># UDFs in SparkSQL with Hive support</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler</span>=<span class="string">FATAL</span></span><br><span class="line"><span class="meta">log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry</span>=<span class="string">ERROR</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>为了不在控制台打印日志。</p>
</blockquote>
<h3 id="Spark运行环境"><a href="#Spark运行环境" class="headerlink" title="Spark运行环境"></a>Spark运行环境</h3><h4 id="Local模式"><a href="#Local模式" class="headerlink" title="Local模式"></a>Local模式</h4><p>所谓的 Local 模式，就是不需要其他任何节点资源就可以在本地执行 Spark 代码的环境，一般用于教学，调试，演示。</p>
<h5 id="安装JDK"><a href="#安装JDK" class="headerlink" title="安装JDK"></a>安装JDK</h5><p>参考文章：<a target="_blank" rel="noopener" href="https://eitan-blog.github.io/2022/05/08/Hadoop%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">Hadoop(一)：集群搭建</a></p>
<h5 id="下载压缩文件并解压"><a href="#下载压缩文件并解压" class="headerlink" title="下载压缩文件并解压"></a>下载压缩文件并解压</h5><p>下载地址：<a target="_blank" rel="noopener" href="https://spark.apache.org/downloads.html">https://spark.apache.org/downloads.html</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 通过 xftp 上传到指定文件夹下并解压</span></span><br><span class="line">[root@CentOS7 software]# tar -zxf spark-3.2.1-bin-hadoop3.2.tgz -C /opt/module/</span><br><span class="line">[eitan@SparkOrigin module]$ mv spark-3.2.1-bin-hadoop3.2/ spark-local-3.2.1</span><br></pre></td></tr></table></figure>

<h5 id="启动Local环境"><a href="#启动Local环境" class="headerlink" title="启动Local环境"></a>启动Local环境</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[eitan@SparkOrigin module]$ ./spark-local-3.2.1/bin/spark-shell</span><br></pre></td></tr></table></figure>

<blockquote>
<p>WebUI监控页面：<a target="_blank" rel="noopener" href="http://192.168.203.150:4040/">http://192.168.203.150:4040/</a></p>
</blockquote>
<h5 id="命令行工具"><a href="#命令行工具" class="headerlink" title="命令行工具"></a>命令行工具</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Using Spark&#x27;s default log4j profile: org/apache/spark/log4j-defaults.properties</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">22/05/20 16:59:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Spark context Web UI available at http://SparkOrigin:4040</span><br><span class="line">Spark context available as &#x27;sc&#x27; (master = local[*], app id = local-1653037162181).</span><br><span class="line">Spark session available as &#x27;spark&#x27;.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &#x27;_/</span><br><span class="line">   /___/ .__/\_,_/_/ /_/\_\   version 3.2.1</span><br><span class="line">      /_/</span><br><span class="line">         </span><br><span class="line">Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_212)</span><br><span class="line">Type in expressions to have them evaluated.</span><br><span class="line">Type :help for more information.</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">scala&gt;</span><span class="bash"> sc.textFile(<span class="string">&quot;spark-local-3.2.1/data/word.txt&quot;</span>).flatMap(_.split(<span class="string">&quot; &quot;</span>)).map((_,1)).reduceByKey(_+_).collect</span></span><br><span class="line">res0: Array[(String, Int)] = Array((Spark,1), (Hello,3), (World,1), (Scala,1))  </span><br></pre></td></tr></table></figure>

<h5 id="提交应用"><a href="#提交应用" class="headerlink" title="提交应用"></a>提交应用</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master local[2] \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.2.1.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li>–class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序</li>
<li>–master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟 CPU 核数量</li>
<li>spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包</li>
<li>数字 10 表示程序的入口参数，用于设定当前应用的任务数量</li>
</ul>
</blockquote>
<h5 id="提交参数说明"><a href="#提交参数说明" class="headerlink" title="提交参数说明"></a>提交参数说明</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class &lt;main-class&gt;</span><br><span class="line">--master &lt;master-url&gt; \</span><br><span class="line">... # other options</span><br><span class="line">&lt;application-jar&gt; \</span><br><span class="line">[application-arguments]</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>参数</th>
<th>解释</th>
<th>可选值举例</th>
</tr>
</thead>
<tbody><tr>
<td>–class</td>
<td>Spark 程序中包含主函数的类</td>
<td></td>
</tr>
<tr>
<td>–master</td>
<td>Spark 程序运行的模式(环境)</td>
<td>模式：local[*]、spark://linux1:7077、Yarn</td>
</tr>
<tr>
<td>–executor-memory 1G</td>
<td>指定每个 executor 可用内存为 1G</td>
<td>符合集群内存配置即可，具体情况具体分析。</td>
</tr>
<tr>
<td>–total-executor-cores 2</td>
<td>指定所有executor使用的cpu核数为 2 个</td>
<td></td>
</tr>
<tr>
<td>–executor-cores</td>
<td>指定每个executor使用的cpu核数</td>
<td></td>
</tr>
<tr>
<td>application-jar</td>
<td>打包好的应用 jar，包含依赖。这个 URL 在集群中全局可见。<br />比如 hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的path 都包含同样的 jar</td>
<td></td>
</tr>
<tr>
<td>application-arguments</td>
<td>传给 main()方法的参数</td>
<td></td>
</tr>
</tbody></table>
<h4 id="Standalone模式"><a href="#Standalone模式" class="headerlink" title="Standalone模式"></a>Standalone模式</h4><p>local 本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用 Spark 自身节点运行的集群模式，也就是我们所谓的独立部署（Standalone）模式。Spark 的 Standalone 模式体现了经典的 master-slave 模式。</p>
<h5 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h5><table>
<thead>
<tr>
<th>spark151</th>
<th>spark152</th>
<th>spark153</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Master</strong><br />Worker</td>
<td>Worker</td>
<td>Worker</td>
</tr>
</tbody></table>
<h5 id="解压缩文件"><a href="#解压缩文件" class="headerlink" title="解压缩文件"></a>解压缩文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 ~]$ cd /opt/software/</span><br><span class="line">[eitan@spark151 software]$ tar -zxf spark-3.2.1-bin-hadoop3.2.tgz -C /opt/module/</span><br><span class="line">[eitan@spark151 software]$ cd /opt/module/</span><br><span class="line">[eitan@spark151 module]$ mv spark-3.2.1-bin-hadoop3.2/ spark-standalone-3.2.1</span><br></pre></td></tr></table></figure>

<h5 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.进入解压缩后路径的 conf 目录，复制 workers.template 文件名为 workers</span></span><br><span class="line">[eitan@spark151 module]$ cd spark-standalone-3.2.1/conf/</span><br><span class="line">[eitan@spark151 conf]$ cp workers.template workers</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.修改 workers 文件，添加 worker 节点</span></span><br><span class="line">[eitan@spark151 conf]$ vim workers</span><br><span class="line"><span class="meta">#</span><span class="bash"> A Spark Worker will be started on each of the machines listed below.</span></span><br><span class="line">spark151</span><br><span class="line">spark152</span><br><span class="line">spark153</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.复制 spark-env.sh.template 文件名为 spark-env.sh</span></span><br><span class="line">[eitan@spark151 conf]$ cp spark-env.sh.template spark-env.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.修改 spark-env.sh 文件，添加 JAVA_HOME 环境变量和集群对应的 master 节点</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">SPARK_MASTER_HOST=spark151</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 5.分发 spark-standalone-3.2.1 目录</span></span><br><span class="line">[eitan@spark151 ~]$ xsync /opt/module/spark-standalone-3.2.1/</span><br></pre></td></tr></table></figure>

<h5 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.执行脚本命令</span></span><br><span class="line">[eitan@spark151 spark-standalone-3.2.1]$ ./sbin/start-all.sh </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.查看三台服务器运行进程</span></span><br><span class="line">[eitan@spark151 spark-standalone-3.2.1]$ xcall jps</span><br><span class="line">================ spark151 ================</span><br><span class="line">8089 Worker</span><br><span class="line">8014 Master</span><br><span class="line">8142 Jps</span><br><span class="line">================ spark152 ================</span><br><span class="line">7958 Worker</span><br><span class="line">8007 Jps</span><br><span class="line">================ spark153 ================</span><br><span class="line">7944 Worker</span><br><span class="line">7993 Jps</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.查看 Master 资源监控 Web UI 界面: http://192.168.203.151:8080/</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4. 提交应用</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://spark151:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.2.1.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>

<h5 id="配置历史服务"><a href="#配置历史服务" class="headerlink" title="配置历史服务"></a>配置历史服务</h5><p>首先要有Hadoop的集群环境，请参考<a target="_blank" rel="noopener" href="https://eitan-blog.github.io/2022/05/08/Hadoop%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/">Hadoop(一)：集群搭建</a></p>
<h6 id="hadoop配置"><a href="#hadoop配置" class="headerlink" title="hadoop配置"></a>hadoop配置</h6><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- core-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://spark152:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-3.3.2/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置HDFS网页登录使用的静态用户为eitan --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>eitan<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- hdfs-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark152:9870<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="comment">&lt;!-- 2nn web端访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark153:9868<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 hadoop]$ cat workers </span><br><span class="line">spark151</span><br><span class="line">spark152</span><br><span class="line">spark153</span><br></pre></td></tr></table></figure>

<h6 id="spark配置"><a href="#spark配置" class="headerlink" title="spark配置"></a>spark配置</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.复制 spark-defaults.conf.template 文件名为 spark-defaults.conf</span></span><br><span class="line">[eitan@spark151 spark-standalone-3.2.1]$ cp conf/spark-defaults.conf.template conf/spark-defaults.conf</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.修改 spark-default.conf 文件，配置日志存储路径</span></span><br><span class="line">[eitan@spark151 spark-standalone-3.2.1]$ vim conf/spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled          true    </span><br><span class="line">spark.eventLog.dir              hdfs://spark152:8020/directory</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.需要启动 hadoop 集群，HDFS 上的 directory 目录需要提前存在</span></span><br><span class="line">[eitan@spark151 hadoop-3.3.2]$ ./sbin/start-dfs.sh</span><br><span class="line">[eitan@spark151 ~]$ hadoop fs -mkdir /directory</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.修改 spark-env.sh 文件, 添加日志配置</span></span><br><span class="line">[eitan@spark151 spark-standalone-3.2.1]$ vim conf/spark-env.sh</span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;</span><br><span class="line">-Dspark.history.ui.port=18080 </span><br><span class="line">-Dspark.history.fs.logDirectory=hdfs://spark152:8020/directory </span><br><span class="line">-Dspark.history.retainedApplications=30&quot;</span><br><span class="line"><span class="meta">#</span><span class="bash"> 参数 1 含义：WEB UI 访问的端口号为 18080</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 参数 2 含义：指定历史服务器日志存储路径</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 参数 3 含义：指定保存 Application 历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 5.重新启动集群和历史服务</span></span><br><span class="line">[eitan@spark151 ~]$ xsync /opt/module/spark-standalone-3.2.1/conf/</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 6.重新启动集群和历史服务</span></span><br><span class="line">[eitan@spark151 spark-standalone-3.2.1]$ ./sbin/start-all.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 7.重新执行任务</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://spark151:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.2.1.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>

<h4 id="Standalone配置高可用（HA）"><a href="#Standalone配置高可用（HA）" class="headerlink" title="Standalone配置高可用（HA）"></a>Standalone配置高可用（HA）</h4><h5 id="集群规划-1"><a href="#集群规划-1" class="headerlink" title="集群规划"></a>集群规划</h5><table>
<thead>
<tr>
<th>spark151</th>
<th>spark152</th>
<th>spark153</th>
</tr>
</thead>
<tbody><tr>
<td>Master<br />Zookeeper<br />Worker</td>
<td>Zookeeper<br />Worker</td>
<td>Master<br />Zookeeper<br />Worker</td>
</tr>
</tbody></table>
<h5 id="解压安装Zookeeper"><a href="#解压安装Zookeeper" class="headerlink" title="解压安装Zookeeper"></a>解压安装Zookeeper</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 software]$ tar -zxf apache-zookeeper-3.7.1-bin.tar.gz -C /opt/module/</span><br><span class="line">[eitan@spark151 module]$ mv apache-zookeeper-3.7.1-bin/ apache-zookeeper-3.7.1/</span><br></pre></td></tr></table></figure>

<h5 id="配置Zookeeper"><a href="#配置Zookeeper" class="headerlink" title="配置Zookeeper"></a>配置Zookeeper</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.在/opt/module/apache-zookeeper-3.7.1/这个目录下创建 zkData</span></span><br><span class="line">[eitan@spark151 apache-zookeeper-3.7.1]$ mkdir zkData</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.在/opt/module/apache-zookeeper-3.7.1/zkData 目录下创建一个 myid 的文件</span></span><br><span class="line">[eitan@spark151 apache-zookeeper-3.7.1]$ vim zkData/myid</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在文件中添加与 server 对应的编号（注意：上下不要有空行，左右不要有空格）</span></span><br><span class="line">1</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.拷贝配置好的 zookeeper 到其他机器上并分别在 spark152、spark153 上修改 myid 文件中内容为 2、3</span></span><br><span class="line">[eitan@spark151 module]$ xsync apache-zookeeper-3.7.1/</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.复制/opt/module/apache-zookeeper-3.7.1/conf 这个目录下的 zoo_sample.cfg 为 zoo.cfg</span></span><br><span class="line">[eitan@spark151 apache-zookeeper-3.7.1]$ cp conf/zoo_sample.cfg conf/zoo.cfg</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 5.修改 zoo.cfg 文件</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改</span></span><br><span class="line">dataDir=/opt/module/apache-zookeeper-3.7.1/zkData</span><br><span class="line"><span class="meta">#</span><span class="bash"> 新增</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cluster</span></span><br><span class="line">server.1=spark151:2888:3888</span><br><span class="line">server.2=spark152:2888:3888</span><br><span class="line">server.3=spark153:2888:3888</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 6.同步 zoo.cfg 配置文件</span></span><br><span class="line">[eitan@spark151 apache-zookeeper-3.7.1]$ xsync conf/zoo.cfg</span><br></pre></td></tr></table></figure>

<blockquote>
<p>配置参数解读：</p>
<p>​    server.A=B:C:D</p>
<ul>
<li><strong>A</strong> 是一个数字，表示这个是第几号服务器。集群模式下配置一个文件 myid，这个文件在 dataDir 目录下，这个文件里面有一个数据就是 A 的值，Zookeeper 启动时读取此文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是哪个server；</li>
<li><strong>B</strong> 是这个服务器的地址；</li>
<li><strong>C</strong> 是这个服务器 Follower 与集群中的 Leader 服务器交换信息的端口；</li>
<li><strong>D</strong> 是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。</li>
</ul>
</blockquote>
<h5 id="Zookeeper集群启动停止脚本"><a href="#Zookeeper集群启动停止脚本" class="headerlink" title="Zookeeper集群启动停止脚本"></a>Zookeeper集群启动停止脚本</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.编写脚本</span></span><br><span class="line">[eitan@spark151 ~]$ vim bin/zk.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">case $1 in</span><br><span class="line">&quot;start&quot;)&#123;</span><br><span class="line">    for host in spark151 spark152 spark153</span><br><span class="line">    do</span><br><span class="line">        echo ---------- zookeeper $host 启动 ------------</span><br><span class="line">        ssh $host &quot;/opt/module/apache-zookeeper-3.7.1/bin/zkServer.sh start&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;stop&quot;)&#123;</span><br><span class="line">    for host in spark151 spark152 spark153</span><br><span class="line">    do</span><br><span class="line">        echo ---------- zookeeper $host 停止 ------------</span><br><span class="line">        ssh $host &quot;/opt/module/apache-zookeeper-3.7.1/bin/zkServer.sh stop&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">&quot;status&quot;)&#123;</span><br><span class="line">    for host in spark151 spark152 spark153</span><br><span class="line">    do</span><br><span class="line">        echo ---------- zookeeper $host 状态 ------------</span><br><span class="line">        ssh $host &quot;/opt/module/apache-zookeeper-3.7.1/bin/zkServer.sh status&quot;</span><br><span class="line">    done</span><br><span class="line">&#125;;;</span><br><span class="line">esac</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.增加脚本执行权限</span></span><br><span class="line">[eitan@spark151 ~]$ chmod u+x bin/zk.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.通过脚本启动集群</span></span><br><span class="line">[eitan@spark151 ~]$ zk.sh start</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 4.通过脚本查看集群状态</span></span><br><span class="line">[eitan@spark151 ~]$ zk.sh status</span><br><span class="line">---------- zookeeper spark151 状态 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/apache-zookeeper-3.7.1/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost. Client SSL: false.</span><br><span class="line">Mode: follower</span><br><span class="line">---------- zookeeper spark152 状态 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/apache-zookeeper-3.7.1/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost. Client SSL: false.</span><br><span class="line">Mode: leader</span><br><span class="line">---------- zookeeper spark153 状态 ------------</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /opt/module/apache-zookeeper-3.7.1/bin/../conf/zoo.cfg</span><br><span class="line">Client port found: 2181. Client address: localhost. Client SSL: false.</span><br><span class="line">Mode: follower</span><br></pre></td></tr></table></figure>

<h5 id="修改-spark-env-sh-文件添加如下配置"><a href="#修改-spark-env-sh-文件添加如下配置" class="headerlink" title="修改 spark-env.sh 文件添加如下配置"></a>修改 spark-env.sh 文件添加如下配置</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 ~]$ vim /opt/module/spark-standalone-3.2.1/conf/spark-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 注释以下内容</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_MASTER_HOST=spark151</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_MASTER_PORT=7077</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 添加如下内容</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Master 监控页面默认访问端口为 8080，但是可能会和 Zookeeper 冲突，所以改成 8989</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8989</span><br><span class="line"></span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS=&quot;</span><br><span class="line">-Dspark.deploy.recoveryMode=ZOOKEEPER </span><br><span class="line">-Dspark.deploy.zookeeper.url=spark151,spark152,spark153</span><br><span class="line">-Dspark.deploy.zookeeper.dir=/spark&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 分发配置文件</span></span><br><span class="line">[eitan@spark151 ~]$ xsync /opt/module/spark-standalone-3.2.1/conf/</span><br></pre></td></tr></table></figure>

<h5 id="启动集群-1"><a href="#启动集群-1" class="headerlink" title="启动集群"></a>启动集群</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.在 spark152 上运行脚本启动集群</span></span><br><span class="line">[eitan@spark152 apache-zookeeper-3.7.1]$ /opt/module/spark-standalone-3.2.1/sbin/start-all.sh</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.发现 Master 在 spark152 上</span></span><br><span class="line">[eitan@spark152 apache-zookeeper-3.7.1]$ xcall jps</span><br><span class="line">================ spark151 ================</span><br><span class="line">20006 Worker</span><br><span class="line">17798 DataNode</span><br><span class="line">19752 QuorumPeerMain</span><br><span class="line">20106 Jps</span><br><span class="line">================ spark152 ================</span><br><span class="line">14306 Master</span><br><span class="line">13443 NameNode</span><br><span class="line">14499 Jps</span><br><span class="line">14103 QuorumPeerMain</span><br><span class="line">13531 DataNode</span><br><span class="line">14399 Worker</span><br><span class="line">================ spark153 ================</span><br><span class="line">14371 QuorumPeerMain</span><br><span class="line">14709 Worker</span><br><span class="line">14789 Jps</span><br><span class="line">13560 SecondaryNameNode</span><br><span class="line">13450 DataNode</span><br></pre></td></tr></table></figure>

<h5 id="在-spark151-上启动备用-Master"><a href="#在-spark151-上启动备用-Master" class="headerlink" title="在 spark151 上启动备用 Master"></a>在 spark151 上启动备用 Master</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 ~]$ /opt/module/spark-standalone-3.2.1/sbin/start-master.sh</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220521170721.png" alt="image-20220521170716726"></p>
<h5 id="提交应用到高可用集群"><a href="#提交应用到高可用集群" class="headerlink" title="提交应用到高可用集群"></a>提交应用到高可用集群</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://spark151:7077,spark152:7077 \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.2.1.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>

<h5 id="停止-spark152-的-Master-资源监控进程"><a href="#停止-spark152-的-Master-资源监控进程" class="headerlink" title="停止 spark152 的 Master 资源监控进程"></a>停止 spark152 的 Master 资源监控进程</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark152 ~]$ jps</span><br><span class="line">14306 Master</span><br><span class="line">13443 NameNode</span><br><span class="line">14644 Jps</span><br><span class="line">14103 QuorumPeerMain</span><br><span class="line">13531 DataNode</span><br><span class="line">14399 Worker</span><br><span class="line">[eitan@spark152 ~]$ kill -9 14306</span><br></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/xyq-material/image/master/blog/20220521172629.png" alt="image-20220521172625793"></p>
<h4 id="Spark-On-Yarn模式"><a href="#Spark-On-Yarn模式" class="headerlink" title="Spark-On-Yarn模式"></a>Spark-On-Yarn模式</h4><p>独立部署（Standalone）模式由 Spark 自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark 主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。</p>
<h5 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h5><ol>
<li>SparkOnYarn 的本质是把 Spark 任务的 class 字节码文件打成 jar 包，上传到 Yarn 集群的 JVM 中运行；</li>
<li>Spark 集群的相关角色（Master，Worker）也会在 Yarn 的 JVM 中运行；</li>
<li>SparkOnYarn需要：<ol>
<li>修改一些配置，使支持 SparkOnYarn</li>
<li>Spark 程序打成的 jar 包，如示例中的 jar 包 spark-examples_2.12-3.2.1.jar，也可以使用我们自己开发的程序达成的 jar 包</li>
<li>Spark 任务提交工具：bin/spark-submit</li>
<li>Spark 本身依赖的 jars：提交任务时会被上传到 Yarn/HDFS，可手动提前上传</li>
</ol>
</li>
<li>SparkOnYarn 不需要 Spark 集群，只需要单机版 spark 即可；</li>
<li>SparkOnYarn 根据 Driver 运行在哪里分为两种模式：client 模式和 cluster 模式。</li>
</ol>
<h5 id="解压缩文件-1"><a href="#解压缩文件-1" class="headerlink" title="解压缩文件"></a>解压缩文件</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 software]$ tar -zxf spark-3.2.1-bin-hadoop3.2.tgz -C /opt/module/</span><br><span class="line">[eitan@spark151 software]$ cd /opt/module/</span><br><span class="line">[eitan@spark151 module]$ mv spark-3.2.1-bin-hadoop3.2/ spark-yarn-3.2.1</span><br></pre></td></tr></table></figure>

<h5 id="配置yarn-site-xml"><a href="#配置yarn-site-xml" class="headerlink" title="配置yarn-site.xml"></a>配置yarn-site.xml</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 module]$ vim hadoop-3.3.2/etc/hadoop/yarn-site.xml </span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 配置yarn主节点的位置 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark151<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn集群的内存分配方案 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>20480<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启日志聚合功能 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置聚合日志在hdfs上的保存时间 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置yarn历史服务器地址 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://spark153:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 关闭yarn内存检查 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 分发给其他节点</span></span><br><span class="line">[eitan@spark151 module]$ xsync hadoop-3.3.2/etc/hadoop/yarn-site.xml </span><br></pre></td></tr></table></figure>

<h5 id="配置Spark的历史服务器和Yarn的整合"><a href="#配置Spark的历史服务器和Yarn的整合" class="headerlink" title="配置Spark的历史服务器和Yarn的整合"></a>配置Spark的历史服务器和Yarn的整合</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.修改spark-defaults.conf</span></span><br><span class="line">[eitan@spark151 module]$ cp .conf/spark-defaults.conf.template  ./conf/spark-defaults.conf</span><br><span class="line">[eitan@spark151 spark-yarn-3.2.1]$ vim conf/spark-defaults.conf</span><br><span class="line">spark.eventLog.enabled                  true</span><br><span class="line">spark.eventLog.dir                      hdfs://spark152:8020/directory</span><br><span class="line">spark.yarn.historyServer.address        spark151:18080</span><br><span class="line">spark.history.ui.port                   18080</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.修改spark-env.sh</span></span><br><span class="line">[eitan@spark151 spark-yarn-3.2.1]$ cp conf/spark-env.sh.template conf/spark-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> spark-standalone</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_212</span><br><span class="line">YARN_CONF_DIR=/opt/module/hadoop-3.3.2/etc/hadoop</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> history-server</span></span><br><span class="line">export SPARK_HISTORY_OPTS=&quot;</span><br><span class="line">-Dspark.history.ui.port=18080 </span><br><span class="line">-Dspark.history.fs.logDirectory=hdfs://spark152:8020/directory </span><br><span class="line">-Dspark.history.retainedApplications=30&quot;</span><br></pre></td></tr></table></figure>

<h5 id="修改日志级别"><a href="#修改日志级别" class="headerlink" title="修改日志级别"></a>修改日志级别</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 spark-yarn-3.2.1]$ cp conf/log4j.properties.template conf/log4j.properties</span><br><span class="line">[eitan@spark151 spark-yarn-3.2.1]$ vim conf/log4j.properties</span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改日志级别为 WARN</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Set everything to be logged to the console</span></span><br><span class="line">log4j.rootCategory=WARN, console</span><br></pre></td></tr></table></figure>

<h5 id="配置依赖的-Spark-的jar包"><a href="#配置依赖的-Spark-的jar包" class="headerlink" title="配置依赖的 Spark 的jar包"></a>配置依赖的 Spark 的jar包</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 1.在HDFS上创建存储spark相关jar包的目录</span></span><br><span class="line">[eitan@spark151 ~]$ hadoop fs -mkdir -p /spark/jars</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2.上传<span class="variable">$SPARK_HOME</span>/jars所有jar包到HDFS</span></span><br><span class="line">[eitan@spark151 ~]$ hadoop fs -put /opt/module/spark-yarn-3.2.1/jars/* /spark/jars</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3.修改spark-defaults.conf</span></span><br><span class="line">[eitan@spark151 spark-yarn-3.2.1]$ vim conf/spark-defaults.conf</span><br><span class="line"><span class="meta">#</span><span class="bash"> 预上传所需要的jar包</span></span><br><span class="line">spark.yarn.jars                         hdfs://spark152:8020/spark/jars/*</span><br></pre></td></tr></table></figure>

<h5 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 ~]$ /opt/module/hadoop-3.3.2/sbin/start-dfs.sh</span><br><span class="line">[eitan@spark151 ~]$ /opt/module/hadoop-3.3.2/sbin/start-yarn.sh</span><br></pre></td></tr></table></figure>

<h5 id="提交应用-1"><a href="#提交应用-1" class="headerlink" title="提交应用"></a>提交应用</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">./examples/jars/spark-examples_2.12-3.2.1.jar \</span><br><span class="line">10</span><br></pre></td></tr></table></figure>

<blockquote>
<p>进入 <a target="_blank" rel="noopener" href="http://192.168.203.151:8088/">http://192.168.203.151:8088/</a> 后无法点入每个 Application 的 History</p>
</blockquote>
<h5 id="启动-Spark-的历史服务器"><a href="#启动-Spark-的历史服务器" class="headerlink" title="启动 Spark 的历史服务器"></a>启动 Spark 的历史服务器</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 spark-yarn-3.2.1]$ sbin/start-history-server.sh</span><br></pre></td></tr></table></figure>

<blockquote>
<p>开启 Spark 的历史服务器，可以进入每个 Application 的 History，但是却看不了对应任务的 stdout 和 stderr</p>
</blockquote>
<h5 id="配置-MapReduce-的历史服务器"><a href="#配置-MapReduce-的历史服务器" class="headerlink" title="配置 MapReduce 的历史服务器"></a>配置 MapReduce 的历史服务器</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 hadoop-3.3.2]$ vim ./etc/hadoop/mapred-site.xml</span><br><span class="line">    &lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 历史服务器端地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;spark153:10020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- 历史服务器web端地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;spark153:19888&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h5 id="重启所有服务"><a href="#重启所有服务" class="headerlink" title="重启所有服务"></a>重启所有服务</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[eitan@spark151 module]$ ./hadoop-3.3.2/sbin/start-dfs.sh</span><br><span class="line">[eitan@spark151 module]$ ./hadoop-3.3.2/sbin/start-yarn.sh </span><br><span class="line">[eitan@spark151 module]$ ./spark-yarn-3.2.1/sbin/start-history-server.sh</span><br><span class="line">[eitan@spark153 ~]$ mapred --daemon start historyserver</span><br></pre></td></tr></table></figure>

<blockquote>
<p>所有功能均可正常使用</p>
</blockquote>
<h4 id="部署模式对比"><a href="#部署模式对比" class="headerlink" title="部署模式对比"></a>部署模式对比</h4><table>
<thead>
<tr>
<th>模式</th>
<th>Spark 安装机器数</th>
<th>需启动的进程</th>
<th>所属者</th>
<th>应用场景</th>
</tr>
</thead>
<tbody><tr>
<td>Local</td>
<td>1</td>
<td>无</td>
<td>Spark</td>
<td>测试</td>
</tr>
<tr>
<td>Standalone</td>
<td>3</td>
<td>Master及Worker</td>
<td>Spark</td>
<td>单独部署</td>
</tr>
<tr>
<td>Yarn</td>
<td>1</td>
<td>Yarn及HDFS</td>
<td>Hadoop</td>
<td>混合部署</td>
</tr>
</tbody></table>
<h4 id="端口号"><a href="#端口号" class="headerlink" title="端口号"></a>端口号</h4><ul>
<li>Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算）</li>
<li>Spark Master 内部通信服务端口号：7077</li>
<li>Standalone 模式下，Spark Master Web 端口号：8080（资源）</li>
<li>Spark 历史服务器端口号：18080</li>
<li>Hadoop YARN 任务运行情况查看端口号：8088</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Eitan
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="http://example.com/2022/05/20/Spark%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="Spark（一）：环境搭建">http://example.com/2022/05/20/Spark（一）：环境搭建/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"><i class="fa fa-tag"></i> spark</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2022/05/11/Hadoop%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9AHive/" rel="prev" title="Hadoop（三）：Hive">
                  <i class="fa fa-chevron-left"></i> Hadoop（三）：Hive
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022/05/20/Hadoop%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9AHive/" rel="next" title="Hadoop（四）：Hive">
                  Hadoop（四）：Hive <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Eitan</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>






  





</body>
</html>
